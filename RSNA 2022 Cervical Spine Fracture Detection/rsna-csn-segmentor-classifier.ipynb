{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e475ea25",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-05T15:32:25.113813Z",
     "iopub.status.busy": "2022-11-05T15:32:25.112932Z",
     "iopub.status.idle": "2022-11-05T15:34:14.185745Z",
     "shell.execute_reply": "2022-11-05T15:34:14.184575Z"
    },
    "papermill": {
     "duration": 109.081564,
     "end_time": "2022-11-05T15:34:14.188341",
     "exception": false,
     "start_time": "2022-11-05T15:32:25.106777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rsna-2022-whl/pydicom-2.3.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/rsna-2022-whl/pylibjpeg-1.4.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/rsna-2022-whl/python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pylibjpeg==1.4.0) (1.21.6)\r\n",
      "pydicom is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Installing collected packages: python-gdcm, pylibjpeg\r\n",
      "Successfully installed pylibjpeg-1.4.0 python-gdcm-3.0.15\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/rsna-2022-whl/torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/rsna-2022-whl/torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.12.1) (4.3.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1) (2.28.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1) (9.1.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.1) (1.21.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1) (2022.6.15.2)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1) (1.26.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.1) (2.1.0)\r\n",
      "Installing collected packages: torch, torchvision\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.11.0\r\n",
      "    Uninstalling torch-1.11.0:\r\n",
      "      Successfully uninstalled torch-1.11.0\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.12.0\r\n",
      "    Uninstalling torchvision-0.12.0:\r\n",
      "      Successfully uninstalled torchvision-0.12.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "allennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\r\n",
      "allennlp 2.10.0 requires torch<1.12.0,>=1.10.0, but you have torch 1.12.1 which is incompatible.\r\n",
      "allennlp 2.10.0 requires torchvision<0.13.0,>=0.8.1, but you have torchvision 0.13.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed torch-1.12.1 torchvision-0.13.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rsna-2022-whl/{pydicom-2.3.0-py3-none-any.whl,pylibjpeg-1.4.0-py3-none-any.whl,python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl}\n",
    "!pip install /kaggle/input/rsna-2022-whl/{torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl,torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe73cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:34:14.201845Z",
     "iopub.status.busy": "2022-11-05T15:34:14.200320Z",
     "iopub.status.idle": "2022-11-05T15:35:35.441454Z",
     "shell.execute_reply": "2022-11-05T15:35:35.440236Z"
    },
    "papermill": {
     "duration": 81.259951,
     "end_time": "2022-11-05T15:35:35.454200",
     "exception": false,
     "start_time": "2022-11-05T15:34:14.194249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rsna-weights/timm-0.5.4-py3-none-any.whl\r\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm==0.5.4) (1.12.1)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.5.4) (0.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.5.4) (4.3.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.5.4) (2.28.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.5.4) (1.21.6)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.5.4) (9.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.5.4) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.5.4) (2022.6.15.2)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.5.4) (2.1.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.5.4) (1.26.12)\r\n",
      "Installing collected packages: timm\r\n",
      "Successfully installed timm-0.5.4\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/rsna-weights/tifffile-2022.8.8-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tifffile==2022.8.8) (1.21.6)\r\n",
      "\u001b[31mERROR: Package 'tifffile' requires a different Python: 3.7.12 not in '>=3.8'\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/rsna-weights/einops-0.5.0-py3-none-any.whl\r\n",
      "Installing collected packages: einops\r\n",
      "Successfully installed einops-0.5.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rsna-weights/timm-0.5.4-py3-none-any.whl\n",
    "!pip install /kaggle/input/rsna-weights/tifffile-2022.8.8-py3-none-any.whl\n",
    "!pip install /kaggle/input/rsna-weights/einops-0.5.0-py3-none-any.whl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b002cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:35:35.501581Z",
     "iopub.status.busy": "2022-11-05T15:35:35.500438Z",
     "iopub.status.idle": "2022-11-05T15:38:57.695538Z",
     "shell.execute_reply": "2022-11-05T15:38:57.694369Z"
    },
    "papermill": {
     "duration": 202.223156,
     "end_time": "2022-11-05T15:38:57.697925",
     "exception": false,
     "start_time": "2022-11-05T15:35:35.474769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/mmdetection-2-17-offline/mmcv_full-1.3.14-cp37-cp37m-linux_x86_64.whl\r\n",
      "Installing collected packages: mmcv-full\r\n",
      "Successfully installed mmcv-full-1.3.14\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl\r\n",
      "Installing collected packages: pycocotools\r\n",
      "Successfully installed pycocotools-2.0.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/terminaltables-3.1.0-py3-none-any.whl\r\n",
      "Installing collected packages: terminaltables\r\n",
      "Successfully installed terminaltables-3.1.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/pytest_runner-5.3.1-py3-none-any.whl\r\n",
      "Installing collected packages: pytest-runner\r\n",
      "Successfully installed pytest-runner-5.3.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/mmpycocotools-12.0.3-cp37-cp37m-linux_x86_64.whl\r\n",
      "Installing collected packages: mmpycocotools\r\n",
      "Successfully installed mmpycocotools-12.0.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/terminal-0.4.0-py3-none-any.whl\r\n",
      "Installing collected packages: terminal\r\n",
      "Successfully installed terminal-0.4.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/mmdet-2.17.0-py3-none-any.whl\r\n",
      "Installing collected packages: mmdet\r\n",
      "Successfully installed mmdet-2.17.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/addict-2.4.0-py3-none-any.whl\r\n",
      "Installing collected packages: addict\r\n",
      "Successfully installed addict-2.4.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/mmdetection-2-17-offline/yapf-0.31.0-py2.py3-none-any.whl\r\n",
      "Installing collected packages: yapf\r\n",
      "  Attempting uninstall: yapf\r\n",
      "    Found existing installation: yapf 0.32.0\r\n",
      "    Uninstalling yapf-0.32.0:\r\n",
      "      Successfully uninstalled yapf-0.32.0\r\n",
      "Successfully installed yapf-0.31.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('/kaggle/input/mmdetection-2-17-offline')\n",
    "\n",
    "!pip install /kaggle/input/mmdetection-2-17-offline/mmcv_full-1.3.14-cp37-cp37m-linux_x86_64.whl --no-deps\n",
    "!pip install /kaggle/input/mmdetection-2-17-offline/pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl --no-deps\n",
    "!pip install /kaggle/input/mmdetection-2-17-offline/terminaltables-3.1.0-py3-none-any.whl --no-deps\n",
    "!pip install /kaggle/input/mmdetection-2-17-offline/pytest_runner-5.3.1-py3-none-any.whl --no-deps\n",
    "!pip install /kaggle/input/mmdetection-2-17-offline/mmpycocotools-12.0.3-cp37-cp37m-linux_x86_64.whl --no-deps\n",
    "!pip install /kaggle/input/mmdetection-2-17-offline/terminal-0.4.0-py3-none-any.whl --no-deps\n",
    "!pip install /kaggle/input/mmdetection-2-17-offline/mmdet-2.17.0-py3-none-any.whl --no-deps\n",
    "!pip install /kaggle/input/mmdetection-2-17-offline/addict-2.4.0-py3-none-any.whl --no-deps\n",
    "!pip install /kaggle/input/mmdetection-2-17-offline/yapf-0.31.0-py2.py3-none-any.whl --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b8633c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:38:57.714856Z",
     "iopub.status.busy": "2022-11-05T15:38:57.713854Z",
     "iopub.status.idle": "2022-11-05T15:38:57.719279Z",
     "shell.execute_reply": "2022-11-05T15:38:57.718403Z"
    },
    "papermill": {
     "duration": 0.015666,
     "end_time": "2022-11-05T15:38:57.721284",
     "exception": false,
     "start_time": "2022-11-05T15:38:57.705618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/rsnazoopublic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158db3b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:38:57.736651Z",
     "iopub.status.busy": "2022-11-05T15:38:57.735784Z",
     "iopub.status.idle": "2022-11-05T15:39:00.447418Z",
     "shell.execute_reply": "2022-11-05T15:39:00.446460Z"
    },
    "papermill": {
     "duration": 2.721929,
     "end_time": "2022-11-05T15:39:00.449858",
     "exception": false,
     "start_time": "2022-11-05T15:38:57.727929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "\n",
    "import albumentations\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import tifffile\n",
    "import torch\n",
    "import torch.hub\n",
    "from albumentations import ReplayCompose\n",
    "from skimage import measure\n",
    "from torch.functional import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3156f2ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:39:00.467420Z",
     "iopub.status.busy": "2022-11-05T15:39:00.465735Z",
     "iopub.status.idle": "2022-11-05T15:39:00.474542Z",
     "shell.execute_reply": "2022-11-05T15:39:00.473604Z"
    },
    "papermill": {
     "duration": 0.019306,
     "end_time": "2022-11-05T15:39:00.476490",
     "exception": false,
     "start_time": "2022-11-05T15:39:00.457184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class BatchSlice:\n",
    "    i_from: int\n",
    "    i_to: int\n",
    "    i_start: int\n",
    "\n",
    "\n",
    "def get_slices(batch: Tensor, dim=1, window: int = 16, overlap: int = 8) -> List[BatchSlice]:\n",
    "    num_imgs = batch.size(dim)\n",
    "    if num_imgs <= window:\n",
    "        return [BatchSlice(0, num_imgs, 0)]\n",
    "    stride = window - overlap\n",
    "    result = []\n",
    "    current_idx = 0\n",
    "    while True:\n",
    "        next_idx = current_idx + window\n",
    "\n",
    "        if next_idx >= num_imgs:\n",
    "            current_idx = num_imgs - window\n",
    "            offset = overlap // 2 if current_idx > 0 else 0\n",
    "            next_idx = num_imgs\n",
    "            result.append(BatchSlice(current_idx, next_idx, offset))\n",
    "            break\n",
    "        else:\n",
    "            offset = overlap // 2 if current_idx > 0 else 0\n",
    "            result.append(BatchSlice(current_idx, next_idx, offset))\n",
    "        current_idx += stride\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f95b02c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:39:00.491631Z",
     "iopub.status.busy": "2022-11-05T15:39:00.491370Z",
     "iopub.status.idle": "2022-11-05T15:39:00.519672Z",
     "shell.execute_reply": "2022-11-05T15:39:00.518519Z"
    },
    "papermill": {
     "duration": 0.038517,
     "end_time": "2022-11-05T15:39:00.521778",
     "exception": false,
     "start_time": "2022-11-05T15:39:00.483261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_scan(scan_dir: str, size=512, fix_monochrome: bool = True) -> np.ndarray:\n",
    "    num_files = len(os.listdir(scan_dir))\n",
    "    images = []\n",
    "    offset = 0\n",
    "    first = None\n",
    "    last = None\n",
    "    files = []\n",
    "    for i in range(num_files):\n",
    "        dpath = os.path.join(scan_dir, f\"{i + offset}.dcm\")\n",
    "        if i == 0:\n",
    "            while not os.path.exists(dpath):\n",
    "                offset += 1\n",
    "                dpath = os.path.join(scan_dir, f\"{i + offset}.dcm\")\n",
    "        files.append(dpath)\n",
    "\n",
    "    for dpath in files[::2]:\n",
    "        ds = pydicom.dcmread(dpath)\n",
    "        if not first:\n",
    "            first = ds\n",
    "        last = ds\n",
    "\n",
    "        data = ds.pixel_array\n",
    "        data = cv2.resize(data, (size, size))\n",
    "        if fix_monochrome and ds.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "            data = np.amax(data) - data\n",
    "        images.append(data)\n",
    "\n",
    "    if first and last:\n",
    "        if last.ImagePositionPatient[2] > first.ImagePositionPatient[2]:\n",
    "            images = images[::-1]\n",
    "    return np.array(images)\n",
    "\n",
    "class DatasetSeg(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_dir: str,\n",
    "            cases: List[str],\n",
    "    ):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.cases = cases\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        cube_id = self.cases[i]\n",
    "        image_cube = combine_scan(os.path.join(self.dataset_dir, cube_id), size=256)\n",
    "        image_mean = image_cube.mean()\n",
    "        image_std = image_cube.std()\n",
    "        h = image_cube.shape[0]\n",
    "\n",
    "        images = image_cube\n",
    "        if h % 32 > 0:\n",
    "            tmp = np.zeros(((h // 32 + 1) * 32, 256, 256))\n",
    "            tmp[:h] = images\n",
    "            images = tmp\n",
    "        images = (images - image_mean) / image_std\n",
    "        images = np.expand_dims(images, 0)\n",
    "        sample = {}\n",
    "        sample['image'] = torch.from_numpy(images).float()\n",
    "        sample['cube_id'] = cube_id\n",
    "        sample['h'] = h\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cases)\n",
    "\n",
    "\n",
    "crop_augs =  albumentations.ReplayCompose([\n",
    "            albumentations.LongestMaxSize(256),\n",
    "            albumentations.PadIfNeeded(256, 256, border_mode=cv2.BORDER_CONSTANT),\n",
    "        ])\n",
    "\n",
    "class DatasetCrops(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_dir: str,\n",
    "            cases: List[str],\n",
    "            transforms=crop_augs,\n",
    "            slice_size=40,\n",
    "    ):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.transforms = transforms\n",
    "        self.slice_size = slice_size\n",
    "        self.cases = cases\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        cube_id = self.cases[i]\n",
    "        mask_cube = tifffile.imread(os.path.join(\"seg_preds\", f\"{cube_id}.tif\"))\n",
    "        image_cube = combine_scan(os.path.join(self.dataset_dir, cube_id) ,size=512)\n",
    "        boxes = {}\n",
    "        for rprop in measure.regionprops(mask_cube):\n",
    "            boxes[rprop.label] = rprop.bbox, rprop.area\n",
    "\n",
    "        image_mean = image_cube.mean()\n",
    "        image_std = image_cube.std()\n",
    "        slice_size = self.slice_size\n",
    "        all_images = []\n",
    "        labels = np.zeros((8,))\n",
    "        for li in range(1, 8):\n",
    "            if li not in boxes:\n",
    "                all_images.append(np.zeros((3, self.slice_size, 256, 256)))\n",
    "            else:\n",
    "                bbox, area = boxes[li]\n",
    "                z1, z2 = bbox[0], bbox[3]\n",
    "                y1, y2 = max(bbox[1] - 16, 0), min(bbox[4] + 16, 256)\n",
    "                x1, x2 = max(bbox[2] - 16, 0), min(bbox[5] + 16, 256)\n",
    "                # if z2 - z1 < slice_size:\n",
    "                #     z1 = random.randint(max(z2 - slice_size, 0), z1)\n",
    "                #     z2 = z1 + slice_size\n",
    "                # todo: verify\n",
    "                if z2 - z1 < slice_size:\n",
    "                    diff = (slice_size - z2 + z1) // 2\n",
    "                    z1 = max(0, z1 - diff)\n",
    "                    z2 = z1 + slice_size\n",
    "                images = image_cube[z1:z2, y1 * 2:y2 * 2, x1 * 2:x2 * 2].copy()\n",
    "                masks = mask_cube[z1:z2, y1:y2, x1:x2].copy()\n",
    "                slice_size = self.slice_size\n",
    "\n",
    "                replay = None\n",
    "                image_crops = []\n",
    "                mask_crops = []\n",
    "                for i in range(images.shape[0]):\n",
    "                    image = images[i]\n",
    "                    mask = masks[i]\n",
    "                    h, w, = mask.shape\n",
    "                    mask = cv2.resize(mask, (w * 2, h * 2), interpolation=cv2.INTER_NEAREST)\n",
    "                    if replay is None:\n",
    "                        sample = self.transforms(image=image, mask=mask)\n",
    "                        replay = sample[\"replay\"]\n",
    "                    else:\n",
    "                        sample = ReplayCompose.replay(replay, image=image, mask=mask)\n",
    "                    image_ = sample[\"image\"]\n",
    "                    image_crops.append(image_)\n",
    "                    mask_crops.append(sample[\"mask\"])\n",
    "                images = np.array(image_crops).astype(np.float32)\n",
    "                masks = np.array(mask_crops).astype(np.float32)\n",
    "                images = np.expand_dims(images, -1)\n",
    "                masks = np.expand_dims(masks, -1)\n",
    "                images = (images - image_mean) / image_std\n",
    "\n",
    "                images = np.concatenate([images, images, masks], axis=-1)\n",
    "                h = images.shape[0]\n",
    "                if h > slice_size:\n",
    "                    images = images[: slice_size]\n",
    "                    all_images.append(np.moveaxis(images, -1, 0))\n",
    "                    images = images[-slice_size:]\n",
    "                    all_images.append(np.moveaxis(images, -1, 0))\n",
    "                else:\n",
    "                    if h != slice_size:\n",
    "                        tmp = np.zeros((slice_size, *images.shape[1:]))\n",
    "                        tmp[:h] = images\n",
    "                        images = tmp\n",
    "                    all_images.append(np.moveaxis(images, -1, 0))\n",
    "\n",
    "        sample = {}\n",
    "        sample['image'] = torch.from_numpy(np.array(all_images)).float()\n",
    "        sample['label'] = torch.from_numpy(labels).float()\n",
    "        sample['cube_id'] = cube_id\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1698a269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:39:00.536711Z",
     "iopub.status.busy": "2022-11-05T15:39:00.536449Z",
     "iopub.status.idle": "2022-11-05T15:39:00.545472Z",
     "shell.execute_reply": "2022-11-05T15:39:00.544535Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.018534,
     "end_time": "2022-11-05T15:39:00.547452",
     "exception": false,
     "start_time": "2022-11-05T15:39:00.528918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_checkpoint(model, checkpoint_path, strict=False, verbose=True):\n",
    "    if verbose:\n",
    "        print(\"=> loading checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        state_dict = {re.sub(\"^module.\", \"\", k): w for k, w in state_dict.items()}\n",
    "        orig_state_dict = model.state_dict()\n",
    "        mismatched_keys = []\n",
    "        for k, v in state_dict.items():\n",
    "            ori_size = orig_state_dict[k].size() if k in orig_state_dict else None\n",
    "            if v.size() != ori_size:\n",
    "                if verbose:\n",
    "                    print(\"SKIPPING!!! Shape of {} changed from {} to {}\".format(k, v.size(), ori_size))\n",
    "                mismatched_keys.append(k)\n",
    "        for k in mismatched_keys:\n",
    "            del state_dict[k]\n",
    "        model.load_state_dict(state_dict, strict=strict)\n",
    "        del state_dict\n",
    "        del orig_state_dict\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(checkpoint_path, checkpoint['epoch']))\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    del checkpoint\n",
    "\n",
    "\n",
    "def load_model(conf: Dict, checkpoint: str):\n",
    "    model = conf[\"network\"](**conf[\"encoder_params\"])\n",
    "    model = model.cuda()\n",
    "    load_checkpoint(model, checkpoint)\n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a5aa22d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:39:00.562581Z",
     "iopub.status.busy": "2022-11-05T15:39:00.561655Z",
     "iopub.status.idle": "2022-11-05T15:39:00.573234Z",
     "shell.execute_reply": "2022-11-05T15:39:00.572410Z"
    },
    "papermill": {
     "duration": 0.021114,
     "end_time": "2022-11-05T15:39:00.575208",
     "exception": false,
     "start_time": "2022-11-05T15:39:00.554094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_segmentations(models: List[torch.nn.Module], test_dataset_dir: str, out_dir: str,  cases: List[str]):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    test_dataset = DatasetSeg(dataset_dir=test_dataset_dir, cases=cases)\n",
    "    sampler = None\n",
    "    oof_loader = DataLoader(\n",
    "        test_dataset, batch_size=1, sampler=sampler, shuffle=False, num_workers=1, pin_memory=False\n",
    "    )\n",
    "    pred_dir = out_dir\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "    for sample in tqdm(oof_loader):\n",
    "        image = sample[\"image\"]\n",
    "        h = int(sample[\"h\"][0])\n",
    "        cube_id = sample[\"cube_id\"][0]\n",
    "        imgs = image.cpu().float()\n",
    "        case_preds = np.zeros((imgs.shape[2], 256, 256), dtype=np.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            slices = get_slices(imgs, dim=2, window=256, overlap=128)\n",
    "            for slice in slices:\n",
    "                batch = imgs[:, :, slice.i_from:slice.i_to].cuda().float()\n",
    "                with torch.cuda.amp.autocast(enabled=True):\n",
    "                    preds = None\n",
    "                    for model in models:\n",
    "                        if preds is None:\n",
    "                            preds = torch.softmax(model(batch)[\"mask\"], dim=1)[0]\n",
    "                        else:\n",
    "                            preds += torch.softmax(model(batch)[\"mask\"], dim=1)[0]\n",
    "                    preds = torch.argmax(preds, dim=0)\n",
    "                preds = preds.cpu().numpy()\n",
    "\n",
    "                for pred_idx in range(slice.i_start, preds.shape[0]):\n",
    "                    idx = slice.i_from + pred_idx\n",
    "                    y_pred = preds[pred_idx]\n",
    "                    case_preds[idx] = y_pred[:, :]\n",
    "                torch.cuda.empty_cache()\n",
    "        case_preds = np.array(case_preds)[:h]\n",
    "        case_preds = case_preds.astype(np.uint8)\n",
    "        tifffile.imwrite(os.path.join(pred_dir, f\"{cube_id}.tif\"), case_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ad3a7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:39:00.590193Z",
     "iopub.status.busy": "2022-11-05T15:39:00.589416Z",
     "iopub.status.idle": "2022-11-05T15:39:00.599860Z",
     "shell.execute_reply": "2022-11-05T15:39:00.599007Z"
    },
    "papermill": {
     "duration": 0.019895,
     "end_time": "2022-11-05T15:39:00.601796",
     "exception": false,
     "start_time": "2022-11-05T15:39:00.581901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def predict_classification(models: List[nn.Module], test_dataset_dir: str, cases: List, output_list: List):\n",
    "    test_dataset = DatasetCrops(dataset_dir=test_dataset_dir, cases=cases)\n",
    "    dataloader = DataLoader(\n",
    "        test_dataset, batch_size=1, sampler=None, shuffle=False, num_workers=1, pin_memory=False\n",
    "    )\n",
    "    \n",
    "    def predict_model(model):\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(imgs)):\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = model(imgs[i:i + 1])[\"cls\"][0]\n",
    "                pred_slice = torch.sigmoid(output.float()).cpu().numpy().astype(np.float32)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = model(torch.flip(imgs[i:i + 1], dims=(-1,)))[\"cls\"][0]\n",
    "                pred_slice += torch.sigmoid(output.float()).cpu().numpy().astype(np.float32)\n",
    "                pred_slice /= 2\n",
    "                preds.append(pred_slice)\n",
    "        preds = np.max(np.array(preds), axis=0)\n",
    "        preds[np.isnan(preds)] = 0.01\n",
    "        return preds\n",
    "        \n",
    "    for sample in tqdm(dataloader):\n",
    "        imgs = sample[\"image\"].cuda().float()[0]\n",
    "        cube_id = sample[\"cube_id\"][0]\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            for model in models:\n",
    "                preds.append(predict_model(model))\n",
    "            preds = np.average(np.array(preds), axis=0)\n",
    "            preds = np.clip(preds, 0.01, 0.99)\n",
    "            output_list.append([cube_id, preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45738830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:39:00.616712Z",
     "iopub.status.busy": "2022-11-05T15:39:00.615880Z",
     "iopub.status.idle": "2022-11-05T15:39:13.727848Z",
     "shell.execute_reply": "2022-11-05T15:39:13.726801Z"
    },
    "papermill": {
     "duration": 13.121723,
     "end_time": "2022-11-05T15:39:13.730209",
     "exception": false,
     "start_time": "2022-11-05T15:39:00.608486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '/kaggle/input/rsna-weights/256_ResNet3dCSN2P1D_r50ir_0_dice'\n",
      "=> loaded checkpoint '/kaggle/input/rsna-weights/256_ResNet3dCSN2P1D_r50ir_0_dice' (epoch 23)\n"
     ]
    }
   ],
   "source": [
    "import zoo\n",
    "config_seg  = {\n",
    "  \"network\": zoo.ResNet3dCSN2P1D,\n",
    "  \"encoder_params\": {\n",
    "    \"encoder\": \"r50ir\"\n",
    "  }\n",
    "}\n",
    "test_dataset_dir = \"/kaggle/input/rsna-2022-cervical-spine-fracture-detection/test_images/\"\n",
    "cases = os.listdir(test_dataset_dir)\n",
    "seg_model = load_model(config_seg, \"/kaggle/input/rsna-weights/256_ResNet3dCSN2P1D_r50ir_0_dice\")\n",
    "ds = DatasetSeg(test_dataset_dir, cases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea4a1fd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:39:13.746340Z",
     "iopub.status.busy": "2022-11-05T15:39:13.745576Z",
     "iopub.status.idle": "2022-11-05T15:39:27.051369Z",
     "shell.execute_reply": "2022-11-05T15:39:27.049465Z"
    },
    "papermill": {
     "duration": 13.31623,
     "end_time": "2022-11-05T15:39:27.053694",
     "exception": false,
     "start_time": "2022-11-05T15:39:13.737464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:13<00:00,  4.42s/it]\n"
     ]
    }
   ],
   "source": [
    "preds_dir = \"/kaggle/working/seg_preds\"\n",
    "process_segmentations([seg_model], test_dataset_dir,out_dir=preds_dir, cases=cases )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "823b7860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:39:27.072776Z",
     "iopub.status.busy": "2022-11-05T15:39:27.070901Z",
     "iopub.status.idle": "2022-11-05T15:39:51.931262Z",
     "shell.execute_reply": "2022-11-05T15:39:51.929964Z"
    },
    "papermill": {
     "duration": 24.871818,
     "end_time": "2022-11-05T15:39:51.933384",
     "exception": false,
     "start_time": "2022-11-05T15:39:27.061566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '/kaggle/input/rsna-weights/swa_5_best_full_ClassifierResNet3dCSN2P1D_r152ir_0.pth'\n",
      "=> loaded checkpoint '/kaggle/input/rsna-weights/swa_5_best_full_ClassifierResNet3dCSN2P1D_r152ir_0.pth' (epoch 0)\n",
      "=> loading checkpoint '/kaggle/input/rsna-weights/swa_5_best_full_ClassifierResNet3dCSN2P1D_r152ir_1.pth'\n",
      "=> loaded checkpoint '/kaggle/input/rsna-weights/swa_5_best_full_ClassifierResNet3dCSN2P1D_r152ir_1.pth' (epoch 0)\n",
      "=> loading checkpoint '/kaggle/input/rsna-weights/swa_5_best_full_ClassifierResNet3dCSN2P1D_r152ir_2.pth'\n",
      "=> loaded checkpoint '/kaggle/input/rsna-weights/swa_5_best_full_ClassifierResNet3dCSN2P1D_r152ir_2.pth' (epoch 0)\n",
      "=> loading checkpoint '/kaggle/input/rsna-weights/swa_5_best_full_ClassifierResNet3dCSN2P1D_r152ir_3.pth'\n",
      "=> loaded checkpoint '/kaggle/input/rsna-weights/swa_5_best_full_ClassifierResNet3dCSN2P1D_r152ir_3.pth' (epoch 0)\n",
      "=> loading checkpoint '/kaggle/input/rsna-weights/swa_3_best_full_frozen_ClassifierResNet3dCSN2P1D_r152ip_0.pth'\n",
      "=> loaded checkpoint '/kaggle/input/rsna-weights/swa_3_best_full_frozen_ClassifierResNet3dCSN2P1D_r152ip_0.pth' (epoch 0)\n",
      "=> loading checkpoint '/kaggle/input/rsna-weights/swa_3_best_full_frozen_ClassifierResNet3dCSN2P1D_r152ip_1.pth'\n",
      "=> loaded checkpoint '/kaggle/input/rsna-weights/swa_3_best_full_frozen_ClassifierResNet3dCSN2P1D_r152ip_1.pth' (epoch 0)\n",
      "=> loading checkpoint '/kaggle/input/rsna-weights/swa_3_best_full_frozen_ClassifierResNet3dCSN2P1D_r152ip_2.pth'\n",
      "=> loaded checkpoint '/kaggle/input/rsna-weights/swa_3_best_full_frozen_ClassifierResNet3dCSN2P1D_r152ip_2.pth' (epoch 0)\n",
      "=> loading checkpoint '/kaggle/input/rsna-weights/swa_3_best_full_frozen_ClassifierResNet3dCSN2P1D_r152ip_3.pth'\n",
      "=> loaded checkpoint '/kaggle/input/rsna-weights/swa_3_best_full_frozen_ClassifierResNet3dCSN2P1D_r152ip_3.pth' (epoch 0)\n"
     ]
    }
   ],
   "source": [
    "config_cls = {\n",
    "  \"network\": zoo.ClassifierResNet3dCSN2P1D,\n",
    "  \"encoder_params\": {\n",
    "    \"encoder\": \"r152ir\",\n",
    "    \"num_classes\": 8,\n",
    "    \"pool\": \"max\"\n",
    "  }\n",
    "}\n",
    "cls_models = [\n",
    "    load_model(config_cls, \"/kaggle/input/rsna-weights/swa_5_best_full_ClassifierResNet3dCSN2P1D_r152ir_0.pth\"),\n",
    "    load_model(config_cls, \"/kaggle/input/rsna-weights/swa_5_best_full_ClassifierResNet3dCSN2P1D_r152ir_1.pth\"),\n",
    "    load_model(config_cls, \"/kaggle/input/rsna-weights/swa_5_best_full_ClassifierResNet3dCSN2P1D_r152ir_2.pth\"),\n",
    "    load_model(config_cls, \"/kaggle/input/rsna-weights/swa_5_best_full_ClassifierResNet3dCSN2P1D_r152ir_3.pth\"),\n",
    "\n",
    "]\n",
    "\n",
    "config_cls = {\n",
    "  \"network\": zoo.ClassifierResNet3dCSN2P1D,\n",
    "  \"encoder_params\": {\n",
    "    \"encoder\": \"r152ip\",\n",
    "    \"num_classes\": 8,\n",
    "    \"pool\": \"max\"\n",
    "  }\n",
    "}\n",
    "\n",
    "cls_models += [\n",
    "    load_model(config_cls, \"/kaggle/input/rsna-weights/swa_3_best_full_frozen_ClassifierResNet3dCSN2P1D_r152ip_0.pth\"),\n",
    "    load_model(config_cls, \"/kaggle/input/rsna-weights/swa_3_best_full_frozen_ClassifierResNet3dCSN2P1D_r152ip_1.pth\"),\n",
    "    load_model(config_cls, \"/kaggle/input/rsna-weights/swa_3_best_full_frozen_ClassifierResNet3dCSN2P1D_r152ip_2.pth\"),\n",
    "    load_model(config_cls, \"/kaggle/input/rsna-weights/swa_3_best_full_frozen_ClassifierResNet3dCSN2P1D_r152ip_3.pth\"),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a49d2921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:39:51.952429Z",
     "iopub.status.busy": "2022-11-05T15:39:51.952054Z",
     "iopub.status.idle": "2022-11-05T15:40:27.510263Z",
     "shell.execute_reply": "2022-11-05T15:40:27.508226Z"
    },
    "papermill": {
     "duration": 35.57056,
     "end_time": "2022-11-05T15:40:27.512421",
     "exception": false,
     "start_time": "2022-11-05T15:39:51.941861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:35<00:00, 11.83s/it]\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "\n",
    "predict_classification(cls_models, test_dataset_dir, cases, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7ca6747",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:40:27.530704Z",
     "iopub.status.busy": "2022-11-05T15:40:27.530394Z",
     "iopub.status.idle": "2022-11-05T15:40:27.595030Z",
     "shell.execute_reply": "2022-11-05T15:40:27.593792Z"
    },
    "papermill": {
     "duration": 0.076949,
     "end_time": "2022-11-05T15:40:27.597873",
     "exception": false,
     "start_time": "2022-11-05T15:40:27.520924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>fractured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [row_id, fractured]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sub_df = pd.read_csv('/kaggle/input/rsna-2022-cervical-spine-fracture-detection/test.csv')\n",
    "\n",
    "del sub_df['prediction_type']\n",
    "del sub_df['StudyInstanceUID']\n",
    "\n",
    "data = []\n",
    "for cube_id, preds in output:\n",
    "    data.append([f\"{cube_id}_patient_overall\", preds[0]])\n",
    "    for i in range(1, 8):\n",
    "        data.append([f\"{cube_id}_C{i}\", preds[i]])\n",
    "pred_df = pd.DataFrame(data, columns=[\"row_id\", \"fractured\"])\n",
    "sub_df = sub_df.merge(pred_df, on=['row_id'])\n",
    "sub_df.to_csv('submission.csv',index=False)\n",
    "sub_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86f1a5fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-05T15:40:27.617178Z",
     "iopub.status.busy": "2022-11-05T15:40:27.616370Z",
     "iopub.status.idle": "2022-11-05T15:40:27.627297Z",
     "shell.execute_reply": "2022-11-05T15:40:27.626428Z"
    },
    "papermill": {
     "duration": 0.022906,
     "end_time": "2022-11-05T15:40:27.629337",
     "exception": false,
     "start_time": "2022-11-05T15:40:27.606431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('/kaggle/working/seg_preds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 491.401987,
   "end_time": "2022-11-05T15:40:29.061437",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-05T15:32:17.659450",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
