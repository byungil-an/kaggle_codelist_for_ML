{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc  \nimport os  \nimport time  \nimport warnings \nfrom itertools import combinations  \nfrom warnings import simplefilter \nimport joblib  \nimport lightgbm as lgb  \nimport numpy as np  \nimport pandas as pd  \nfrom sklearn.metrics import mean_absolute_error \nfrom sklearn.model_selection import KFold, TimeSeriesSplit  \nimport polars as pl\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n\nis_offline = False \nLGB = True\nNN = False\nis_train = True  \nis_infer = True \nmax_lookback = np.nan \nsplit_day = 435  \n\ndef weighted_average(a):\n    w = []\n    n = len(a)\n    for j in range(1, n + 1):\n        j = 2 if j == 1 else j\n        w.append(1 / (2**(n + 1 - j)))\n    return w\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# modified code for group gaps; source\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups // n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n            \n            \n            if self.verbose > 0:\n                    pass\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]\n\ndef reduce_mem_usage(df, verbose=0):\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n               \n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n    return df\n\ndf = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\ndf = df.dropna(subset=[\"target\"])\ndf.reset_index(drop=True, inplace=True)\ndf_shape = df.shape\n\n# outlier_days = [33,\n#  48,\n#  58,\n#  83,\n#  86,\n#  96,\n#  119,\n#  122,\n#  158,\n#  193,\n#  196,\n#  208,\n#  221,\n#  225,\n#  283,\n#  328,\n#  334,\n#  346,\n#  404,\n#  407,\n#  438,\n#  453,\n#  458,\n#  470,\n#  474]\n\n# outlier_days = [33,\n#  35,\n#  83,\n#  86,\n#  96,\n#  119,\n#  122,\n#  158,\n#  193,\n#  196,\n#  208,\n#  221,\n#  225,\n#  283,\n#  322,\n#  328,\n#  334,\n#  346,\n#  404,\n#  407,\n#  458,\n#  470,\n#  474]\n\n# df = df.query('date_id not in @outlier_days').reset_index(drop=True)\n\n\n\n\nfrom numba import njit, prange\n\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            \n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features\n\ndef calculate_triplet_imbalance_numba(price, df):\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n    return features\n\n\ndef dfrank(newdf): # 添加基础排名因子\n#     target = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\", \"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n#     target = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n#     target = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n#     target=[\"volume\",\"mid_price\",\"liquidity_imbalance\",\"matched_imbalance\",\"size_imbalance\"]\n    target=[\"stock_weights\",\"weighted_wap\",'wap_momentum',\"imbalance_momentum\",\"price_spread\",\"spread_intensity\",'price_pressure','market_urgency','depth_pressure','spread_depth_ratio','mid_price_movement','micro_price','relative_spread']\n    \n    columns=[column for column in newdf.columns if (column in target)]\n    \n#     columns=[column for column in newdf.columns if ((column in target)or\n#                                                     (\"_imb\" in column)\n#                                                    )]\n\n    for column in columns:\n        # 从小到大排名【测试下双排名有效果是因为加上了na_option='bottom'的处理机制还是因为实现的双排名方案】\n        newdf=pd.concat([newdf,(newdf[str(column)].rank(method=\"max\", ascending=False,na_option='bottom')/len(newdf)).rename(f\"{str(column)}_rank\")], axis=1) # 从大到小排序\n        # 从大到小排名\n        newdf=pd.concat([newdf,(newdf[str(column)].rank(method=\"max\", ascending=True,na_option='bottom')/len(newdf)).rename(f\"{str(column)}_rerank\")], axis=1) # 从大到小排序\n    return newdf\n\n\ndef imbalance_features(df):\n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n    \n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n\n#     df = df.groupby(['date_id','seconds_in_bucket']).apply(dfrank) # 计算排名因子【之前得分最好的方案没有这个因子】#【换到这里减少内存占用】\n\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n    \n    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n   \n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    \n    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n    \n    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n    \n    df = df.groupby(['date_id','seconds_in_bucket']).apply(dfrank) # 计算排名因子【之前得分最好的方案没有这个因子】#【换到这里能够用更多因子】\n    \n    # Calculate various statistical aggregation features\n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n        \n\n    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n        for window in [1,3,5,10]:\n            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n    \n    # Calculate diff features for specific columns\n    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'weighted_wap','price_spread']:\n        for window in [1,3,5,10]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n    \n    #V4 feature\n    for window in [3,5,10]:\n        df[f'price_change_diff_{window}'] = df[f'bid_price_diff_{window}'] - df[f'ask_price_diff_{window}']\n        df[f'size_change_diff_{window}'] = df[f'bid_size_diff_{window}'] - df[f'ask_size_diff_{window}']\n\n    #V5 - rolling diff\n    # Convert from pandas to Polars\n    pl_df = pl.from_pandas(df)\n\n    #Define the windows and columns for which you want to calculate the rolling statistics\n    windows = [3, 5, 10]\n    columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n\n    # prepare the operations for each column and window\n    group = [\"stock_id\"]\n    expressions = []\n\n    # Loop over each window and column to create the rolling mean and std expressions\n    for window in windows:\n        for col in columns:\n            rolling_mean_expr = (\n                pl.col(f\"{col}_diff_{window}\")\n                .rolling_mean(window)\n                .over(group)\n                .alias(f'rolling_diff_{col}_{window}')\n            )\n\n            rolling_std_expr = (\n                pl.col(f\"{col}_diff_{window}\")\n                .rolling_std(window)\n                .over(group)\n                .alias(f'rolling_std_diff_{col}_{window}')\n            )\n\n            expressions.append(rolling_mean_expr)\n            expressions.append(rolling_std_expr)\n\n    # Run the operations using Polars' lazy API\n    lazy_df = pl_df.lazy().with_columns(expressions)\n\n    # Execute the lazy expressions and overwrite the pl_df variable\n    pl_df = lazy_df.collect()\n\n    # Convert back to pandas if necessary\n    df = pl_df.to_pandas()\n    gc.collect()\n    \n    df['mid_price*volume'] = df['mid_price_movement'] * df['volume']\n    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n    \n    for col in df.columns:\n        df[col] = df[col].replace([np.inf, -np.inf], 0)\n\n    return df\n\ndef other_features(df):\n    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n    df['time_to_market_close'] = 540 - df['seconds_in_bucket']\n    \n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n\n    return df\n\ndef generate_all_features(df):\n    # Select relevant columns for feature generation\n    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n    df = df[cols]\n    \n    # Generate imbalance features\n    df = imbalance_features(df)\n    gc.collect() \n    df = other_features(df)\n    gc.collect()  \n    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n    \n    return df[feature_name]\n\n\nweights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n]\nweights = {int(k):v for k,v in enumerate(weights)}\n\n\nif is_offline:\n    \n    df_train = df[df[\"date_id\"] <= split_day]\n    df_valid = df[df[\"date_id\"] > split_day]\n    print(\"Offline mode\")\n    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n    \nelse:\n    df_train = df\n    print(\"Online mode\")\n\n\nif is_train:\n    global_stock_id_feats = {\n        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n    }\n    if is_offline:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Train Feats Finished.\")\n        df_valid_feats = generate_all_features(df_valid)\n        print(\"Build Valid Feats Finished.\")\n        df_valid_feats = reduce_mem_usage(df_valid_feats)\n    else:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Online Train Feats Finished.\")\n\n    df_train_feats = reduce_mem_usage(df_train_feats)\n    \n# corr = df_train_feats.corr()\n# # Drop highly correlated features (37->30)\n# columns = np.full((corr.shape[0],), True, dtype=bool)\n# for i in range(corr.shape[0]):\n#     for j in range(i+1, corr.shape[0]):\n#         if corr.iloc[i,j] >= 0.99:\n#             if columns[j]:\n#                 columns[j] = False\n\n# feature_columns = df_train_feats.columns[columns].values\n# drop_columns = df_train_feats.columns[columns == False].values\n# print(feature_columns)\n# print('-'*73)\n# print(drop_columns)\n\n# gc.collect()\n\nif LGB:\n    import numpy as np\n    import lightgbm as lgb\n    \n    lgb_params = {\n        \"objective\": \"mae\",\n        \"n_estimators\": 6000,\n        \"num_leaves\": 256,\n        \"subsample\": 0.6,\n        \"colsample_bytree\": 0.8,\n        \"learning_rate\": 0.00871,\n        'max_depth': 11,\n        \"n_jobs\": 4,\n        \"device\": \"gpu\",\n        \"verbosity\": -1,\n        \"importance_type\": \"gain\",\n        \"reg_alpha\": 0.1,\n        \"reg_lambda\": 3.25\n    }\n\n    feature_columns = list(df_train_feats.columns)\n    print(f\"Features = {len(feature_columns)}\")\n    #print(f\"Feature length = {len(feature_columns)}\")\n\n    num_folds = 5\n    fold_size = 480 // num_folds\n    gap = 5\n\n    models = []\n    models_cbt = []\n    scores = []\n\n    model_save_path = 'modelitos_para_despues' \n    if not os.path.exists(model_save_path):\n        os.makedirs(model_save_path)\n\n    date_ids = df_train['date_id'].values\n\n    for i in range(num_folds):\n        start = i * fold_size\n        end = start + fold_size\n        if i < num_folds - 1:  # No need to purge after the last fold\n            purged_start = end - 2\n            purged_end = end + gap + 2\n            train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n        else:\n            train_indices = (date_ids >= start) & (date_ids < end)\n\n        test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n        \n        gc.collect()\n        \n        df_fold_train = df_train_feats[train_indices]\n        df_fold_train_target = df_train['target'][train_indices]\n        df_fold_valid = df_train_feats[test_indices]\n        df_fold_valid_target = df_train['target'][test_indices]\n\n        print(f\"Fold {i+1} Model Training\")\n\n        # Train a LightGBM model for the current fold\n        lgb_model = lgb.LGBMRegressor(**lgb_params)\n        lgb_model.fit(\n            df_fold_train[feature_columns],\n            df_fold_train_target,\n            eval_set=[(df_fold_valid[feature_columns], df_fold_valid_target)],\n            callbacks=[\n                lgb.callback.early_stopping(stopping_rounds=100),\n                lgb.callback.log_evaluation(period=100),\n            ],\n        )\n        \n#         cbt_model = cbt.CatBoostRegressor(objective='MAE', iterations=5000,bagging_temperature=0.5,\n#                                 colsample_bylevel = 0.7,learning_rate = 0.065,\n#                                 od_wait = 25,max_depth = 7,l2_leaf_reg = 1.5,\n#                                 min_data_in_leaf = 1000,random_strength=0.65,\n#                                 verbose=0,use_best_model=True,task_type='CPU')\n#         cbt_model.fit(\n#             df_fold_train[feature_columns],\n#             df_fold_train_target,\n#             eval_set=[(df_fold_valid[feature_columns], df_fold_valid_target)]\n#         )\n        \n#         models_cbt.append(cbt_model)\n\n        models.append(lgb_model)\n        # Save the model to a file\n        model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n        lgb_model.booster_.save_model(model_filename)\n        print(f\"Model for fold {i+1} saved to {model_filename}\")\n\n        # Evaluate model performance on the validation set\n        #------------LGB--------------#\n        fold_predictions = lgb_model.predict(df_fold_valid[feature_columns])\n        fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n        scores.append(fold_score)\n        print(f\":LGB Fold {i+1} MAE: {fold_score}\")\n        #------------CBT--------------#\n#         fold_predictions = cbt_model.predict(df_fold_valid[feature_columns])\n#         fold_score_cbt = mean_absolute_error(fold_predictions, df_fold_valid_target)\n#         scores.append(fold_score_cbt)\n#         print(f\"CBT Fold {i+1} MAE: {fold_score_cbt}\")\n\n        # Free up memory by deleting fold specific variables\n        del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n        gc.collect()\n\n    # Calculate the average best iteration from all regular folds\n    average_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n\n    # Update the lgb_params with the average best iteration\n    final_model_params = lgb_params.copy()\n\n    # final_model_params['n_estimators'] = average_best_iteration\n    # print(f\"Training final model with average best iteration: {average_best_iteration}\")\n\n    # Train the final model on the entire dataset\n    num_model = 1\n\n    for i in range(num_model):\n        final_model = lgb.LGBMRegressor(**final_model_params)\n        final_model.fit(\n            df_train_feats[feature_columns],\n            df_train['target'],\n            callbacks=[\n                lgb.callback.log_evaluation(period=100),\n            ],\n        )\n        # Append the final model to the list of models\n        models.append(final_model)\n\n\n\ndef create_mlp(num_continuous_features, num_categorical_features, embedding_dims, num_labels, hidden_units, dropout_rates, learning_rate,l2_strength=0.01):\n\n    # Numerical variables input\n    input_continuous = tf.keras.layers.Input(shape=(num_continuous_features,))\n\n    # Categorical variables input\n    input_categorical = [tf.keras.layers.Input(shape=(1,))\n                         for _ in range(len(num_categorical_features))]\n\n    # Embedding layer for categorical variables\n    embeddings = [tf.keras.layers.Embedding(input_dim=num_categorical_features[i],\n                                            output_dim=embedding_dims[i])(input_cat)\n                  for i, input_cat in enumerate(input_categorical)]\n    flat_embeddings = [tf.keras.layers.Flatten()(embed) for embed in embeddings]\n\n    # concat numerical and categorical\n    concat_input = tf.keras.layers.concatenate([input_continuous] + flat_embeddings)\n\n    # MLP\n    x = tf.keras.layers.BatchNormalization()(concat_input)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i],kernel_regularizer=l2(0.01),kernel_initializer='he_normal')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.ReLU()(x)\n        #x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)\n\n    #No activation\n    out = tf.keras.layers.Dense(num_labels,kernel_regularizer=l2(0.01),kernel_initializer='he_normal')(x)\n\n    model = tf.keras.models.Model(inputs=[input_continuous] + input_categorical, outputs=out)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                  loss='mean_absolute_error',\n                  metrics=['mean_absolute_error'])\n    return model\n\nif NN:\n    import numpy as np\n    from sklearn.metrics import mean_absolute_error\n    import gc\n    from sklearn.model_selection import KFold\n    import tensorflow as tf\n    import tensorflow.keras.backend as K\n    import tensorflow.keras.layers as layers\n    from tensorflow.keras.regularizers import l2\n    from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n    \n    df_train_feats = df_train_feats.groupby('stock_id').apply(lambda group: group.fillna(method='ffill')).fillna(0)\n    \n    categorical_features = [\"stock_id\"]\n    numerical_features = [column for column in list(df_train_feats) if column not in categorical_features]\n    num_categorical_features = [len(df_train_feats[col].unique()) for col in categorical_features]\n\n    nn_models = []\n\n    batch_size = 64\n    hidden_units = [128,128]\n    dropout_rates = [0.1,0.1,0.1]\n    learning_rate = 1e-5\n    embedding_dims = [20]\n\n    directory = '/kaggle/working/NN_Models/'\n    if not os.path.exists(directory):\n        os.mkdir(directory)\n\n    pred = np.zeros(len(df_train['target']))\n    scores = []\n    gkf = PurgedGroupTimeSeriesSplit(n_splits = 5, group_gap = 5)\n\n\n    for fold, (tr, te) in enumerate(gkf.split(df_train_feats,df_train['target'],df_train['date_id'])):\n\n        ckp_path = os.path.join(directory, f'nn_Fold_{fold+1}.h5')\n\n        X_tr_continuous = df_train_feats.iloc[tr][numerical_features].values\n        X_val_continuous = df_train_feats.iloc[te][numerical_features].values\n\n        X_tr_categorical = df_train_feats.iloc[tr][categorical_features].values\n        X_val_categorical = df_train_feats.iloc[te][categorical_features].values\n\n        y_tr, y_val = df_train['target'].iloc[tr].values, df_train['target'].iloc[te].values\n\n        print(\"X_train_numerical shape:\",X_tr_continuous.shape)\n        print(\"X_train_categorical shape:\",X_tr_categorical.shape)\n        print(\"Y_train shape:\",y_tr.shape)\n        print(\"X_test_numerical shape:\",X_val_continuous.shape)\n        print(\"X_test_categorical shape:\",X_val_categorical.shape)\n        print(\"Y_test shape:\",y_val.shape)\n\n        print(f\"Creating Model - Fold{fold}\")\n        model = create_mlp(len(numerical_features), num_categorical_features, embedding_dims, 1, hidden_units, dropout_rates, learning_rate)\n\n        rlr = ReduceLROnPlateau(monitor='val_mean_absolute_error', factor=0.1, patience=3, verbose=0, min_delta=1e-4, mode='min')\n        ckp = ModelCheckpoint(ckp_path, monitor='val_mean_absolute_error', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n        es = EarlyStopping(monitor='val_mean_absolute_error', min_delta=1e-4, patience=10, mode='min', restore_best_weights=True, verbose=0)\n\n        print(f\"Fitting Model - Fold{fold}\")\n        model.fit((X_tr_continuous,X_tr_categorical), y_tr,\n                  validation_data=([X_val_continuous,X_val_categorical], y_val),\n                  epochs=200, batch_size=batch_size,callbacks=[ckp,es,rlr])\n\n        output = model.predict((X_val_continuous,X_val_categorical), batch_size=batch_size * 4)\n\n        pred[te] += model.predict((X_val_continuous,X_val_categorical), batch_size=batch_size * 4).ravel()\n\n        score = mean_absolute_error(y_val, pred[te])\n        scores.append(score)\n        print(f'Fold {fold} MAE:\\t', score)\n\n        # Finetune 3 epochs on validation set with small learning rate\n        print(f\"Finetuning Model - Fold{fold}\")\n        model = create_mlp(len(numerical_features), num_categorical_features, embedding_dims, 1, hidden_units, dropout_rates, learning_rate / 100)\n        model.load_weights(ckp_path)\n        model.fit((X_val_continuous,X_val_categorical), y_val, epochs=5, batch_size=batch_size, verbose=0)\n        model.save_weights(ckp_path)\n        nn_models.append(model)\n\n        K.clear_session()\n        del model\n        gc.collect()\n\n    print(\"Average NN CV Scores:\",np.mean(scores))\n\n\ndef zero_sum(prices, volumes):\n    std_error = np.sqrt(volumes)\n    step = np.sum(prices) / np.sum(std_error)\n    out = prices - std_error * step\n    return out\n\nif is_infer:\n    import optiver2023\n    env = optiver2023.make_env()\n    iter_test = env.iter_test()\n    counter = 0\n    y_min, y_max = -64, 64\n    qps, predictions = [], []\n    cache = pd.DataFrame()\n\n    # Weights for each fold model\n    if LGB:\n        lgb_model_weights = weighted_average(models)\n        #cbt_model_weights = weighted_average(models_cbt)\n    \n    for (test, revealed_targets, sample_prediction) in iter_test:\n        now_time = time.time()\n        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n        if counter > 0:\n            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n        feat = generate_all_features(cache)[-len(test):]\n        print(f\"Feat Shape is: {feat.shape}\")\n\n        # Generate predictions for each model and calculate the weighted average\n        if LGB:\n            lgb_predictions = np.zeros(len(test))\n            for model, weight in zip(models, lgb_model_weights):\n                lgb_predictions += weight * model.predict(feat[feature_columns])\n\n        predictions = lgb_predictions\n        \n        #Using mean predictions rather than zero sum\n        final_predictions = predictions - np.mean(predictions)\n        clipped_predictions = np.clip(final_predictions, y_min, y_max)\n        sample_prediction['target'] = clipped_predictions\n        env.predict(sample_prediction)\n        counter += 1\n        qps.append(time.time() - now_time)\n        if counter % 10 == 0:\n            print(counter, 'qps:', np.mean(qps))\n\n    time_cost = 1.146 * np.mean(qps)\n    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n\n\n\n\n\n\n# import gc\n# import os\n# import time\n# import warnings\n# from itertools import combinations\n# from warnings import simplefilter\n\n# import joblib\n# import numpy as np\n# import pandas as pd\n# from sklearn.metrics import mean_absolute_error\n# from sklearn.model_selection import KFold, TimeSeriesSplit\n\n# is_offline = False # 设置是否离线\n# is_train = True # 设置是否进训练模型\n# is_infer = True \n# max_lookback = np.nan\n# split_day = 435 #设置离线观察周期\n\n# warnings.filterwarnings(\"ignore\") # 忽略警告\n# simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning) # 指定忽略哪些类型的警告\n# df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\") # 读取数据\n# df = df.dropna(subset=[\"target\"]) # 删除预测目标列上的空值\n# df.reset_index(drop=True, inplace=True) # 重置索引\n# df.shape # 使用shape属性打印出DataFrame的形状（行数和列数）\n\n\n# def reduce_mem_usage(df, verbose=0): # 优化性能，对不同数据格式的数据进行优化【不过针对浮点数没有专门优化】\n#     start_mem = df.memory_usage().sum() / 1024**2\n#     for col in df.columns:\n#         col_type = df[col].dtype\n#         if col_type != object:\n#             c_min = df[col].min()\n#             c_max = df[col].max()\n#             if str(col_type)[:3] == \"int\":\n#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n#                     df[col] = df[col].astype(np.int8)\n#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n#                     df[col] = df[col].astype(np.int16)\n#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n#                     df[col] = df[col].astype(np.int32)\n#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n#                     df[col] = df[col].astype(np.int64)\n#             else:\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float32)\n#                 elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n#                     df[col] = df[col].astype(np.float32)\n#                 else:\n#                     df[col] = df[col].astype(np.float32)\n#     if verbose:\n#         logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n#         end_mem = df.memory_usage().sum() / 1024**2\n#         logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n#         decrease = 100 * (start_mem - end_mem) / start_mem\n#         logger.info(f\"Decreased by {decrease:.2f}%\")\n#     return df\n\n\n# from numba import njit, prange\n# @njit(parallel=True) # @njit(parallel=True)表示使用Numba的Just-In-Time (JIT)编译器对被装饰的函数进行编译，其中parallel=True表示允许使用并行化技术来进一步加速函数。\n# # 具体而言，@njit表示该函数将被Numba JIT编译器编译，使其在执行时能够以本地机器码而非Python解释器的形式运行，从而提高函数的执行速度。paralell=True表示在编译时启用自动并行化功能，允许Numba将循环等操作并行化，以更充分地利用计算机的多核心处理能力，进一步提高函数的运行效率。需要注意的是，由于并行化技术的特殊性质，该装饰器只能用于某些特定类型的函数，并不适用于所有情况。\n# # @njit(parallel=True)只会应用于紧随其下的函数。只要该行下面还有其他函数定义，它们就不会被自动加速。如果您想将多个函数编译加速，您需要对每个函数单独应用@njit(parallel=True)装饰器，或将多个函数定义在同一个文件中并在文件顶部应用一次装饰器。\n# # 需要注意的是，只有符合Numba支持的特定数据类型和操作的函数才能被成功编译加速。对某些Python函数应用装饰器可能会导致编译错误或意外的结果，因此在使用Numba进行加速时需要仔细测试和验证。\n# def compute_triplet_imbalance(df_values, comb_indices):\n#     num_rows = df_values.shape[0]\n#     num_combinations = len(comb_indices)\n#     imbalance_features = np.empty((num_rows, num_combinations))\n#     for i in prange(num_combinations):\n#         a, b, c = comb_indices[i]\n#         for j in range(num_rows):\n#             max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n#             min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n#             mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n#             if mid_val == min_val:  # Prevent division by zero\n#                 imbalance_features[j, i] = np.nan\n#             else:\n#                 imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n#     return imbalance_features\n# def calculate_triplet_imbalance_numba(price, df):\n#     # Convert DataFrame to numpy array for Numba compatibility\n#     df_values = df[price].values\n#     comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n#     # Calculate the triplet imbalance\n#     features_array = compute_triplet_imbalance(df_values, comb_indices)\n#     # Create a DataFrame from the results\n#     columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n#     features = pd.DataFrame(features_array, columns=columns)\n#     return features\n# # generate imbalance features\n# def imbalance_features(df):\n#     df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n#     df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n#     df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n#     df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n#     df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n#     # V1\n#     prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n#     sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n#     for c in combinations(prices, 2):\n#         df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n#     for c in combinations(sizes, 2): # 加上这个5.3444\n#         df[f\"{c[0]}/{c[1]}\"] = df.eval(f\"({c[0]})/({c[1]})\")\n#     for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n#         # 分别计算prices和sizes三元高低点\n#         triplet_feature = calculate_triplet_imbalance_numba(c, df)\n#         df[triplet_feature.columns] = triplet_feature.values\n#     # V2\n#     df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n#     df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n#     df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n#     df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n#     df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n#     df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n#     df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n#     df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n#     df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n#     df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n#     df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n#     df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n#     df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n#     for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n#         df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n#         df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n#     # V3\n#     for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n#         for window in [1, 2, 3, 5, 10]:\n#             df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n#             df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n#     for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size',\n#                 'wap', 'near_price', 'far_price']:\n#         for window in [1, 2, 3, 5, 10]:\n#             df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n#     return df.replace([np.inf, -np.inf], 0)\n\n# def dfrank(newdf): # 添加基础排名因子\n#     columns=[column for column in newdf.columns if (\n#             ('target' not in column) # 这个是预测目标，不能排名\n#         and ('date_id' not in column) # 这个在回测时不显示，不能使用\n#         and ('time_id' not in column) # 这个在回测时不显示，不能使用\n#         and ('row_id' not in column)\n#         and ('stock_id' not in column)\n#         and ('seconds_in_bucket' not in column)\n#         and ('currently_scored' not in column)\n#         )]\n#     for column in columns:\n#         # 从小到大排名【测试下双排名有效果是因为加上了na_option='bottom'的处理机制还是因为实现的双排名方案】\n#         newdf=pd.concat([newdf,(newdf[str(column)].rank(method=\"max\", ascending=False,na_option='bottom')/len(newdf)).rename(f\"{str(column)}_rank\")], axis=1) # 从大到小排序\n#         # 从大到小排名\n#         newdf=pd.concat([newdf,(newdf[str(column)].rank(method=\"max\", ascending=True,na_option='bottom')/len(newdf)).rename(f\"{str(column)}_rerank\")], axis=1) # 从大到小排序\n#     return newdf\n\n\n# # generate all features\n# def generate_all_features(df):\n#     cols = [c for c in df.columns if c not in [\"row_id\",\"time_id\",\"target\",\"currently_scored\"]]\n#     df = df[cols]\n#     # 计算基础排名因子\n#     df = df.groupby(['date_id','seconds_in_bucket']).apply(dfrank) # 计算排名因子\n#     df = df.reset_index(drop=True) # 重置索引并且不使其生成新的列【提交之前尽量对df的格式进行还原,需要注意这俩索引的值在分组之后是保留了的，如果不去掉容易造成资源竞争】\n#     # 计算其他特征\n#     df = imbalance_features(df)\n#     # 计算时序相关特征（将无序的时序有序化）\n#     df[\"dow\"] = df[\"date_id\"] % 5 # 计算星期几，将date_id列中的值对5取模，得到星期几的信息，并将结果存储在新的dow列中。\n#     df[\"dom\"] = df[\"date_id\"] % 20 # 计算一个月中的哪一天，将date_id列中的值对20取模，得到一个月中的哪一天，并将结果存储在新的dom列中。\n#     df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60 # 计算每分钟内的秒数，将seconds_in_bucket列中的值对60取模，得到每分钟内的秒数，并将结果存储在新的seconds列中。\n#     df[\"minute\"] = df[\"seconds_in_bucket\"] // 60 # 计算分钟数，将seconds_in_bucket列中的值除以60取整，得到分钟数，并将结果存储在新的minute列中。\n#     for key, value in global_stock_id_feats.items(): # 对于global_stock_id_feats字典中的每个键值对，\n#         df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict()) # 将df[\"stock_id\"]列中的值通过映射到value的字典中的对应值，并将结果存储在以\"global_\" + key命名的新列中。  \n#     gc.collect() # 使用gc.collect()进行垃圾回收，释放内存空间。触发缓存回收功能【把空余数据（没有被占用的数据）删除（简单讲就是生成变量的过程数据，如果说我把变量指针删掉之后变成无主数据，这个gc.collect()才能够起作用）】可以在gc之前打印一下内存，gc之后打印一下内存看看效果，到底多久才把想要删除的文件删掉了\n#     feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n#     return df[feature_name]\n\n\n# weights = [\n#     0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n#     0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n#     0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n#     0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n#     0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n#     0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n#     0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n#     0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n#     0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n#     0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n#     0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n#     0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n#     0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n#     0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n#     0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n#     0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n#     0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n# ]\n# weights = {int(k):v for k,v in enumerate(weights)}\n\n# if is_offline: # 如果离线则截取split_day之后数据进行训练\n#     df_train = df[df[\"date_id\"] <= split_day]\n#     df_valid = df[df[\"date_id\"] > split_day]\n#     print(\"Offline mode\")\n#     print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n# else:\n#     df_train = df\n# print(\"Online mode\")\n    \n# if is_train:\n#     global_stock_id_feats = {\n#         \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n#         \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n#         \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n#         \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n#         \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n#         \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n#     }\n#     if is_offline:\n#         df_train_feats = generate_all_features(df_train) # 对训练集计算特征\n#         print(\"Build Train Feats Finished.\")\n#         df_valid_feats = generate_all_features(df_valid) # 对测试集计算特征\n#         print(\"Build Valid Feats Finished.\")\n#         df_valid_feats = reduce_mem_usage(df_valid_feats) # 性能优化\n#     else:\n#         df_train_feats = generate_all_features(df_train) # 对训练集计算特征\n#         print(\"Build Online Train Feats Finished.\")\n        \n# df_train_feats = reduce_mem_usage(df_train_feats) # 性能优化\n\n\n# import lightgbm as lgb\n# if is_train:\n#     models = [] # 多模型预测【所有模型装进列表】\n#     # 添加第一个LGBM模型\n#     feature_name = list(df_train_feats.columns)\n#     lgb_params = {\n#         \"objective\" : \"mae\",\n#         \"n_estimators\" : 3000,\n#         \"num_leaves\" : 128,\n#         \"subsample\" : 0.6,\n#         \"colsample_bytree\" : 0.6,\n#         \"learning_rate\" : 0.05,\n#         \"n_jobs\" : 4,\n#         \"device\" : \"gpu\",\n#         \"verbosity\": -1,\n#         \"importance_type\" : \"gain\",\n#     }\n#     print(f\"Feature length = {len(feature_name)}\")\n#     # 截取训练时间为split_day - 45\n#     offline_split = df_train['date_id']>(split_day - 45)\n#     df_offline_train = df_train_feats[~offline_split]\n#     df_offline_valid = df_train_feats[offline_split]\n#     df_offline_train_target = df_train['target'][~offline_split]\n#     df_offline_valid_target = df_train['target'][offline_split]\n#     print(\"Valid Model Trainning.\")\n#     lgb_model = lgb.LGBMRegressor(**lgb_params)\n#     lgb_model.fit(\n#         df_offline_train[feature_name],\n#         df_offline_train_target,\n#         eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n#         callbacks=[\n#             lgb.callback.early_stopping(stopping_rounds=100),\n#             lgb.callback.log_evaluation(period=100),\n#         ],\n#     )\n#     del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target # 通过del语句删除（释放）不再需要的变量【其实是删除（释放）了指向内容的指针对象】\n#     gc.collect() # 使用gc.collect()进行垃圾回收，释放内存空间。触发缓存回收功能【把空余数据（没有被占用的数据）删除（简单讲就是生成变量的过程数据，如果说我把变量指针删掉之后变成无主数据，这个gc.collect()才能够起作用）】可以在gc之前打印一下内存，gc之后打印一下内存看看效果，到底多久才把想要删除的文件删掉了\n#     # infer\n#     df_train_target = df_train[\"target\"]\n#     # 将训练数据集中的目标数据存储在df_train_target中。然后，创建一个新的模型对象infer_lgb_model，并使用整个训练数据集进行训练。\n#     print(\"Infer Model Trainning.\")\n#     infer_params = lgb_params.copy()\n#     infer_params[\"n_estimators\"] = int(1.2 * lgb_model.best_iteration_)\n#     # 为了增加模型的泛化能力，将n_estimators参数设置为原始模型最佳迭代次数的1.2倍。【适度过拟合】\n#     infer_lgb_model = lgb.LGBMRegressor(**infer_params)\n#     infer_lgb_model.fit(df_train_feats[feature_name], df_train_target)\n#     models.append(infer_lgb_model)\n\n    \n# #     # 添加第二个树模型\n# #     import catboost as cbt\n# #     cbt_model = cbt.CatBoostRegressor(objective=\"MAE\",iterations=3000,learning_rate=0.03,depth=6,l2_leaf_reg=3) # Iterations（迭代次数）,Learning Rate（学习率）,Depth（树最大深度）,L2 Regularization（L2正则化）\n# #     cbt_model.fit(df_train_feats[feature_name], df_train_target)\n# #     models.append(cbt_model)\n    \n    \n#     if is_offline: # 如果is_offline为True，则进行离线预测。首先，将验证集的目标数据存储在df_valid_target中。然后，使用infer_lgb_model.predict()函数对验证集的特征数据进行预测，并计算预测结果与真实值之间的平均绝对误差（MAE）作为离线评估指标。\n#         # offline predictions\n#         df_valid_target = df_valid[\"target\"]\n#         offline_predictions = infer_lgb_model.predict(df_valid_feats[feature_name])\n#         offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n#         print(f\"Offline Score {np.round(offline_score, 4)}\")\n    \n        \n# def zero_sum(prices,volumes):\n#     std_error = np.sqrt(volumes) # 首先，通过 np.sqrt(volumes) 计算交易量数据 volumes 的标准误差，将结果保存在 std_error 中。\n#     step = np.sum(prices)/np.sum(std_error) # 计算步长 step，通过将价格数据 prices 的总和除以标准误差的总和得到。这样做是为了将价格数据与交易量之间相对的尺度关系纳入考虑。\n#     out = prices-std_error*step # 将价格数据 prices 减去标准误差乘以步长，得到标准化后的输出结果，将其保存在 out 中。\n#     return out\n\n\n# # 这段代码是一个基于LightGBM模型的推断过程。\n# if is_infer:\n#     import optiver2023\n#     env = optiver2023.make_env() # 通过optiver2023.make_env()函数创建一个Optiver交易环境对象env。\n#     iter_test = env.iter_test() # 使用env.iter_test()函数生成测试数据集的迭代器iter_test。\n#     counter = 0 # counter表示当前处于第几个测试数据，\n#     y_min, y_max = -64, 64 # 设置预测当中的极值【将所有数据限制到这个区间，超过的则变更为临界值而非原始值】\n#     qps, predictions = [],[] # qps和predictions是用于记录每秒处理次数和预测结果的列表。\n#     cache = pd.DataFrame() # cache是用于存储历史数据的DataFrame对象。\n#     for (test, revealed_targets, sample_prediction) in iter_test:\n#         now_time = time.time()\n#         cache = pd.concat([cache, test], ignore_index=True, axis=0) # 在迭代器中，将每个测试数据添加到cache中。然后\n#         if counter > 0: # 当预测开始时只保留每个股票的最后21个数据\n#             cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n#         feat = generate_all_features(cache)[-len(test):] # 通过generate_all_features()函数从缓存中生成特征数据。\n#         lgb_prediction = np.mean([model.predict(feat) for model in models], axis=0) # 多模型预测【所有模型都放到一个列表当中了】\n#         lgb_prediction = zero_sum(lgb_prediction, test['ask_price'] + test['bid_price']) # 对预测数据进行缩放【有比较大的提升】\n#         lgb_prediction = lgb_prediction - np.mean(lgb_prediction) # 预测结果标准化，即使用lgb_prediction减去其平均值。\n#         clipped_predictions = np.clip(lgb_prediction, y_min, y_max) # 为了避免预测结果超出规定的范围，使用np.clip()函数将预测结果限制在y_min和y_max之间。最后将预测结果存储在sample_prediction中，并使用env.predict()函数提交预测结果。counter加1，记录每秒处理次数，若counter被10整除，则打印出平均每秒处理次数。\n#         sample_prediction['target'] = clipped_predictions\n#         env.predict(sample_prediction)\n#         counter += 1\n#         qps.append(time.time() - now_time)\n#         if counter % 10 == 0: # 每计算十轮打印一次当前轮次耗时\n#             print(counter, 'qps:', np.mean(qps))\n#     time_cost = 1.146 * np.mean(qps) # 最后，估计代码执行时间。1.146是一个常数系数，用于根据平均每秒处理次数估计总运行时间。\n#     print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-12T06:24:44.310183Z","iopub.execute_input":"2023-12-12T06:24:44.310555Z"},"trusted":true},"execution_count":null,"outputs":[]}]}