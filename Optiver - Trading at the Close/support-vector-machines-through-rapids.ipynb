{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d1f3e3",
   "metadata": {
    "papermill": {
     "duration": 0.008127,
     "end_time": "2023-11-27T16:27:29.189240",
     "exception": false,
     "start_time": "2023-11-27T16:27:29.181113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "üöÄ Introduction üåü\n",
    "\n",
    "Hello Kaggle Enthusiasts! üß†üí°\n",
    "\n",
    "In the vast world of machine learning, most of us are familiar with the usual suspects: LightGBM, XGBoost, CatBoost, and a variety of neural networks like LSTM. However, this notebook is about to take a different turn. üîÑ We're diving into the world of Support Vector Regressions (SVR). üåê\n",
    "\n",
    "üîç What is SVR? ü§î\n",
    "\n",
    "SVR, a member of the Support Vector Machine (SVM) family, is essentially a regression method. In simple terms, while classic SVM is used for classification problems, SVR is used for regression problems. Imagine SVM as a clever algorithm that finds the best line (or hyperplane in higher dimensions) to separate two classes. SVR, on the other hand, uses a similar approach to fit the best line within a predefined error threshold for a set of data points. This makes it particularly useful in forecasting and predicting continuous values. üìà\n",
    "\n",
    "What sets SVR apart is its ability to manage non-linear relationships by employing kernels. I will argue that a well implemented SVR with RBF kernel can compete with gradient boosting tools any day!. Kernels transform the data into a higher dimension where it's possible to find a linear separation (or closest to it). üåê‚öôÔ∏è\n",
    "\n",
    "üîÑ SVR vs. SVC: The Connection and Difference üîÑ\n",
    "\n",
    "While SVR focuses on regression, its cousin, Support Vector Classification (SVC), deals with classification tasks. Both share the same foundation of maximizing margin and minimizing classification/regression error, but they differ in their ultimate goals. SVC aims to find the best separating hyperplane to classify data points, whereas SVR tries to fit the best line within a certain tolerance to the data points. Despite these differences, their underlying principles and mathematics are closely related. ü§ù\n",
    "\n",
    "‚è±Ô∏è The SVR Calibration Challenge ‚è±Ô∏è\n",
    "\n",
    "A critical point to note about SVR is its calibration time. SVR can be slow to tune and calibrate due to its complexity, especially when dealing with large datasets. Hence, it's a wise strategy to perform this calibration step ahead of time and save the calibrated model for future use. üîÑüíæ\n",
    "\n",
    "üìö This Notebook's Journey üìö\n",
    "\n",
    "In this notebook, we begin by guiding you through the offline installation of RAPIDS, an open-source suite of libraries for executing end-to-end data science pipelines entirely on GPUs, which significantly speeds up the data science process. üöÄ I run the installation with the help of a supporting notebook: https://www.kaggle.com/code/verracodeguacas/rapids-installer feel free to use that notebook to learn how to install stuff offline for this and other kaggle competitions.\n",
    "\n",
    "Next, we delve into training the SVR model. Although I've commented out the actual training code, it's there for you to experiment with and calibrate your models. The calibration can be time-consuming, so I've provided a link to a pre-trained SVR model: SVR Models Dataset https://www.kaggle.com/datasets/verracodeguacas/svr-models. üìàüíª\n",
    "\n",
    "Finally, we'll explore how to use this trained SVR model for predictions and demonstrate how to ensemble it with other popular models like LightGBM. This approach allows us to leverage the strengths of different models for better performance. ü§ñüí•\n",
    "\n",
    "Get ready to embark on an exciting journey into the world of SVR, a less-traveled path with promising potential in machine learning! üåüüõ§Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518e6bb",
   "metadata": {
    "papermill": {
     "duration": 0.00736,
     "end_time": "2023-11-27T16:27:29.204155",
     "exception": false,
     "start_time": "2023-11-27T16:27:29.196795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RAPIDS offline installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f6fcd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:27:29.220873Z",
     "iopub.status.busy": "2023-11-27T16:27:29.220211Z",
     "iopub.status.idle": "2023-11-27T16:29:12.759743Z",
     "shell.execute_reply": "2023-11-27T16:29:12.758591Z"
    },
    "papermill": {
     "duration": 103.552348,
     "end_time": "2023-11-27T16:29:12.763855",
     "exception": false,
     "start_time": "2023-11-27T16:27:29.211507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: 'cupy,'\u001b[0m\u001b[31m\r\n",
      "\u001b[0mLooking in links: /kaggle/input/rapids-installer\r\n",
      "Processing /kaggle/input/rapids-installer/cudf_cu11-23.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/rapids-installer/cuml_cu11-23.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from cudf-cu11) (4.2.4)\r\n",
      "Requirement already satisfied: cubinlinker-cu11 in /opt/conda/lib/python3.10/site-packages (from cudf-cu11) (0.3.0.post1)\r\n",
      "Requirement already satisfied: cuda-python<12.0a0,>=11.7.1 in /opt/conda/lib/python3.10/site-packages (from cudf-cu11) (11.8.3)\r\n",
      "Processing /kaggle/input/rapids-installer/cupy_cuda11x-12.2.0-cp310-cp310-manylinux2014_x86_64.whl (from cudf-cu11)\r\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from cudf-cu11) (2023.10.0)\r\n",
      "Requirement already satisfied: numba<0.58,>=0.57 in /opt/conda/lib/python3.10/site-packages (from cudf-cu11) (0.57.1)\r\n",
      "Requirement already satisfied: numpy<1.25,>=1.21 in /opt/conda/lib/python3.10/site-packages (from cudf-cu11) (1.24.3)\r\n",
      "Requirement already satisfied: nvtx>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from cudf-cu11) (0.2.8)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from cudf-cu11) (21.3)\r\n",
      "Processing /kaggle/input/rapids-installer/pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from cudf-cu11)\r\n",
      "Processing /kaggle/input/rapids-installer/protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (from cudf-cu11)\r\n",
      "Requirement already satisfied: ptxcompiler-cu11 in /opt/conda/lib/python3.10/site-packages (from cudf-cu11) (0.7.0.post1)\r\n",
      "Processing /kaggle/input/rapids-installer/pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from cudf-cu11)\r\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from cudf-cu11) (13.5.2)\r\n",
      "Processing /kaggle/input/rapids-installer/rmm_cu11-23.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from cudf-cu11)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from cudf-cu11) (4.5.0)\r\n",
      "Processing /kaggle/input/rapids-installer/dask_cuda-23.10.0-py3-none-any.whl (from cuml-cu11)\r\n",
      "Processing /kaggle/input/rapids-installer/dask_cudf_cu11-23.10.2-py3-none-any.whl (from cuml-cu11)\r\n",
      "Processing /kaggle/input/rapids-installer/dask-2023.9.2-py3-none-any.whl (from cuml-cu11)\r\n",
      "Processing /kaggle/input/rapids-installer/distributed-2023.9.2-py3-none-any.whl (from cuml-cu11)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from cuml-cu11) (1.3.2)\r\n",
      "Processing /kaggle/input/rapids-installer/raft_dask_cu11-23.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from cuml-cu11)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from cuml-cu11) (1.11.3)\r\n",
      "Processing /kaggle/input/rapids-installer/treelite-3.9.1-py3-none-manylinux2014_x86_64.whl (from cuml-cu11)\r\n",
      "Processing /kaggle/input/rapids-installer/treelite_runtime-3.9.1-py3-none-manylinux2014_x86_64.whl (from cuml-cu11)\r\n",
      "Requirement already satisfied: click>=8.0 in /opt/conda/lib/python3.10/site-packages (from dask==2023.9.2->cuml-cu11) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from dask==2023.9.2->cuml-cu11) (2.2.1)\r\n",
      "Requirement already satisfied: partd>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from dask==2023.9.2->cuml-cu11) (1.4.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from dask==2023.9.2->cuml-cu11) (6.0.1)\r\n",
      "Requirement already satisfied: toolz>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from dask==2023.9.2->cuml-cu11) (0.12.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.10/site-packages (from dask==2023.9.2->cuml-cu11) (6.8.0)\r\n",
      "Requirement already satisfied: pynvml<11.5,>=11.0.0 in /opt/conda/lib/python3.10/site-packages (from dask-cuda==23.10.*->cuml-cu11) (11.4.1)\r\n",
      "Requirement already satisfied: zict>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from dask-cuda==23.10.*->cuml-cu11) (3.0.0)\r\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /opt/conda/lib/python3.10/site-packages (from distributed==2023.9.2->cuml-cu11) (3.1.2)\r\n",
      "Requirement already satisfied: locket>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from distributed==2023.9.2->cuml-cu11) (1.0.0)\r\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from distributed==2023.9.2->cuml-cu11) (1.0.5)\r\n",
      "Requirement already satisfied: psutil>=5.7.2 in /opt/conda/lib/python3.10/site-packages (from distributed==2023.9.2->cuml-cu11) (5.9.3)\r\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /opt/conda/lib/python3.10/site-packages (from distributed==2023.9.2->cuml-cu11) (2.4.0)\r\n",
      "Requirement already satisfied: tblib>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from distributed==2023.9.2->cuml-cu11) (2.0.0)\r\n",
      "Requirement already satisfied: tornado>=6.0.4 in /opt/conda/lib/python3.10/site-packages (from distributed==2023.9.2->cuml-cu11) (6.3.3)\r\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from distributed==2023.9.2->cuml-cu11) (1.26.15)\r\n",
      "Processing /kaggle/input/rapids-installer/pylibraft_cu11-23.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from raft-dask-cu11==23.10.*->cuml-cu11)\r\n",
      "Processing /kaggle/input/rapids-installer/ucx_py_cu11-0.34.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from raft-dask-cu11==23.10.*->cuml-cu11)\r\n",
      "Requirement already satisfied: fastrlock>=0.5 in /opt/conda/lib/python3.10/site-packages (from cupy-cuda11x>=12.0.0->cudf-cu11) (0.8.2)\r\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba<0.58,>=0.57->cudf-cu11) (0.40.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->cudf-cu11) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11) (2023.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->cudf-cu11) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->cudf-cu11) (2.16.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask==2023.9.2->cuml-cu11) (3.16.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.10.3->distributed==2023.9.2->cuml-cu11) (2.1.3)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->cudf-cu11) (0.1.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<1.6.0dev0,>=1.3->cudf-cu11) (1.16.0)\r\n",
      "Installing collected packages: ucx-py-cu11, pyarrow, protobuf, cupy-cuda11x, treelite-runtime, treelite, rmm-cu11, pandas, dask, pylibraft-cu11, distributed, cudf-cu11, dask-cudf-cu11, dask-cuda, raft-dask-cu11, cuml-cu11\r\n",
      "  Attempting uninstall: pyarrow\r\n",
      "    Found existing installation: pyarrow 11.0.0\r\n",
      "    Uninstalling pyarrow-11.0.0:\r\n",
      "      Successfully uninstalled pyarrow-11.0.0\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 3.20.3\r\n",
      "    Uninstalling protobuf-3.20.3:\r\n",
      "      Successfully uninstalled protobuf-3.20.3\r\n",
      "  Attempting uninstall: treelite-runtime\r\n",
      "    Found existing installation: treelite-runtime 3.2.0\r\n",
      "    Uninstalling treelite-runtime-3.2.0:\r\n",
      "      Successfully uninstalled treelite-runtime-3.2.0\r\n",
      "  Attempting uninstall: treelite\r\n",
      "    Found existing installation: treelite 3.2.0\r\n",
      "    Uninstalling treelite-3.2.0:\r\n",
      "      Successfully uninstalled treelite-3.2.0\r\n",
      "  Attempting uninstall: pandas\r\n",
      "    Found existing installation: pandas 2.0.3\r\n",
      "    Uninstalling pandas-2.0.3:\r\n",
      "      Successfully uninstalled pandas-2.0.3\r\n",
      "  Attempting uninstall: dask\r\n",
      "    Found existing installation: dask 2023.11.0\r\n",
      "    Uninstalling dask-2023.11.0:\r\n",
      "      Successfully uninstalled dask-2023.11.0\r\n",
      "  Attempting uninstall: distributed\r\n",
      "    Found existing installation: distributed 2023.11.0\r\n",
      "    Uninstalling distributed-2023.11.0:\r\n",
      "      Successfully uninstalled distributed-2023.11.0\r\n",
      "  Attempting uninstall: dask-cuda\r\n",
      "    Found existing installation: dask-cuda 23.8.0\r\n",
      "    Uninstalling dask-cuda-23.8.0:\r\n",
      "      Successfully uninstalled dask-cuda-23.8.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.1 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 12.0.1 which is incompatible.\r\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.10.0 which is incompatible.\r\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\r\n",
      "cudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 12.0.1 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.2 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask-cuda==23.8.*, but you have dask-cuda 23.10.0 which is incompatible.\r\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.9.2 which is incompatible.\r\n",
      "cuml 23.8.0 requires treelite==3.2.0, but you have treelite 3.9.1 which is incompatible.\r\n",
      "cuml 23.8.0 requires treelite-runtime==3.2.0, but you have treelite-runtime 3.9.1 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.2 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.9.2 which is incompatible.\r\n",
      "fitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 1.5.3 which is incompatible.\r\n",
      "google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\r\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\r\n",
      "google-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.1 which is incompatible.\r\n",
      "google-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.1 which is incompatible.\r\n",
      "google-cloud-pubsub 2.18.3 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\r\n",
      "google-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.1 which is incompatible.\r\n",
      "kfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "kfp 2.0.1 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.1 which is incompatible.\r\n",
      "kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.1 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.3 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.2 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask-cuda==23.8.*, but you have dask-cuda 23.10.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.9.2 which is incompatible.\r\n",
      "tensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.1 which is incompatible.\r\n",
      "tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.1 which is incompatible.\r\n",
      "tensorflowjs 4.13.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\r\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed cudf-cu11-23.10.2 cuml-cu11-23.10.0 cupy-cuda11x-12.2.0 dask-2023.9.2 dask-cuda-23.10.0 dask-cudf-cu11-23.10.2 distributed-2023.9.2 pandas-1.5.3 protobuf-4.21.12 pyarrow-12.0.1 pylibraft-cu11-23.10.0 raft-dask-cu11-23.10.0 rmm-cu11-23.10.0 treelite-3.9.1 treelite-runtime-3.9.1 ucx-py-cu11-0.34.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall cupy, cupy-cuda11x\n",
    "!pip install --no-index --find-links /kaggle/input/rapids-installer cudf-cu11 cuml-cu11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ebac4",
   "metadata": {
    "papermill": {
     "duration": 0.014158,
     "end_time": "2023-11-27T16:29:12.794313",
     "exception": false,
     "start_time": "2023-11-27T16:29:12.780155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ignore this warning (let me know if you can fix it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb87e04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:12.823435Z",
     "iopub.status.busy": "2023-11-27T16:29:12.823061Z",
     "iopub.status.idle": "2023-11-27T16:29:19.004985Z",
     "shell.execute_reply": "2023-11-27T16:29:19.004038Z"
    },
    "papermill": {
     "duration": 6.199596,
     "end_time": "2023-11-27T16:29:19.007372",
     "exception": false,
     "start_time": "2023-11-27T16:29:12.807776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/cupy/_environment.py:447: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy, cupy-cuda11x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n"
     ]
    }
   ],
   "source": [
    "import cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c71ae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:19.032210Z",
     "iopub.status.busy": "2023-11-27T16:29:19.031682Z",
     "iopub.status.idle": "2023-11-27T16:29:19.038213Z",
     "shell.execute_reply": "2023-11-27T16:29:19.037343Z"
    },
    "papermill": {
     "duration": 0.020862,
     "end_time": "2023-11-27T16:29:19.040157",
     "exception": false,
     "start_time": "2023-11-27T16:29:19.019295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23.10.00'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuml.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd1639",
   "metadata": {
    "papermill": {
     "duration": 0.011511,
     "end_time": "2023-11-27T16:29:19.063470",
     "exception": false,
     "start_time": "2023-11-27T16:29:19.051959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "SVR Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba31415a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:19.089108Z",
     "iopub.status.busy": "2023-11-27T16:29:19.088778Z",
     "iopub.status.idle": "2023-11-27T16:29:19.095200Z",
     "shell.execute_reply": "2023-11-27T16:29:19.094405Z"
    },
    "papermill": {
     "duration": 0.02143,
     "end_time": "2023-11-27T16:29:19.097008",
     "exception": false,
     "start_time": "2023-11-27T16:29:19.075578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import polars as pl  # Replacing pandas with Polars\n",
    "from cuml.svm import SVR  # Importing CUML's SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#simplefilter(action=\"ignore\", category=pl.errors.PerformanceWarning)  # Updated for Polars\n",
    "is_offline = False\n",
    "is_train = True\n",
    "is_infer = True\n",
    "max_lookback = np.nan\n",
    "split_day = 435"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5b82f",
   "metadata": {
    "papermill": {
     "duration": 0.011383,
     "end_time": "2023-11-27T16:29:19.120004",
     "exception": false,
     "start_time": "2023-11-27T16:29:19.108621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "LGBM Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb7909d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:19.144371Z",
     "iopub.status.busy": "2023-11-27T16:29:19.144063Z",
     "iopub.status.idle": "2023-11-27T16:29:19.150597Z",
     "shell.execute_reply": "2023-11-27T16:29:19.149780Z"
    },
    "papermill": {
     "duration": 0.020907,
     "end_time": "2023-11-27T16:29:19.152540",
     "exception": false,
     "start_time": "2023-11-27T16:29:19.131633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc  # Garbage collection for memory management\n",
    "import os  # Operating system-related functions\n",
    "import time  # Time-related functions\n",
    "import warnings  # Handling warnings\n",
    "from itertools import combinations  # For creating combinations of elements\n",
    "from warnings import simplefilter  # Simplifying warning handling\n",
    "\n",
    "# üì¶ Importing machine learning libraries\n",
    "import joblib  # For saving and loading models\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n",
    "\n",
    "# ü§ê Disable warnings to keep the code clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# üìä Define flags and variables\n",
    "is_offline = False  # Flag for online/offline mode\n",
    "is_train = True  # Flag for training mode\n",
    "is_infer = True  # Flag for inference mode\n",
    "max_lookback = np.nan  # Maximum lookback (not specified)\n",
    "split_day = 435  # Split day for time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b77652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:19.177328Z",
     "iopub.status.busy": "2023-11-27T16:29:19.177051Z",
     "iopub.status.idle": "2023-11-27T16:29:38.943882Z",
     "shell.execute_reply": "2023-11-27T16:29:38.943047Z"
    },
    "papermill": {
     "duration": 19.781872,
     "end_time": "2023-11-27T16:29:38.946279",
     "exception": false,
     "start_time": "2023-11-27T16:29:19.164407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_shape = df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dbba4ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:38.971582Z",
     "iopub.status.busy": "2023-11-27T16:29:38.971270Z",
     "iopub.status.idle": "2023-11-27T16:29:38.983583Z",
     "shell.execute_reply": "2023-11-27T16:29:38.982725Z"
    },
    "papermill": {
     "duration": 0.026996,
     "end_time": "2023-11-27T16:29:38.985490",
     "exception": false,
     "start_time": "2023-11-27T16:29:38.958494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "               \n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "220198d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:39.009777Z",
     "iopub.status.busy": "2023-11-27T16:29:39.009526Z",
     "iopub.status.idle": "2023-11-27T16:29:39.102562Z",
     "shell.execute_reply": "2023-11-27T16:29:39.101861Z"
    },
    "papermill": {
     "duration": 0.107478,
     "end_time": "2023-11-27T16:29:39.104580",
     "exception": false,
     "start_time": "2023-11-27T16:29:38.997102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def rolling_average(arr, window):\n",
    "    \"\"\"\n",
    "    Calculate the rolling average for a 1D numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "    arr (numpy.ndarray): Input array to calculate the rolling average.\n",
    "    window (int): The number of elements to consider for the moving average.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Array containing the rolling average values.\n",
    "    \"\"\"\n",
    "    n = len(arr)\n",
    "    result = np.empty(n)\n",
    "    result[:window] = np.nan  # Padding with NaN for elements where the window is not full\n",
    "    cumsum = np.cumsum(arr)\n",
    "\n",
    "    for i in range(window, n):\n",
    "        result[i] = (cumsum[i] - cumsum[i - window]) / window\n",
    "\n",
    "    return result\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_rolling_averages(df_values, window_sizes):\n",
    "    \"\"\"\n",
    "    Calculate the rolling averages for multiple window sizes in parallel.\n",
    "    \n",
    "    Parameters:\n",
    "    df_values (numpy.ndarray): 2D array of values to calculate the rolling averages.\n",
    "    window_sizes (List[int]): List of window sizes for the rolling averages.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: A 3D array containing the rolling averages for each window size.\n",
    "    \"\"\"\n",
    "    num_rows, num_features = df_values.shape\n",
    "    num_windows = len(window_sizes)\n",
    "    rolling_features = np.empty((num_rows, num_features, num_windows))\n",
    "\n",
    "    for feature_idx in prange(num_features):\n",
    "        for window_idx, window in enumerate(window_sizes):\n",
    "            rolling_features[:, feature_idx, window_idx] = rolling_average(df_values[:, feature_idx], window)\n",
    "\n",
    "    return rolling_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402ea77c",
   "metadata": {
    "papermill": {
     "duration": 0.011456,
     "end_time": "2023-11-27T16:29:39.127847",
     "exception": false,
     "start_time": "2023-11-27T16:29:39.116391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "SVR features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdf3e2eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:39.152445Z",
     "iopub.status.busy": "2023-11-27T16:29:39.152022Z",
     "iopub.status.idle": "2023-11-27T16:29:39.168838Z",
     "shell.execute_reply": "2023-11-27T16:29:39.168066Z"
    },
    "papermill": {
     "duration": 0.031439,
     "end_time": "2023-11-27T16:29:39.170764",
     "exception": false,
     "start_time": "2023-11-27T16:29:39.139325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imbalance_features(df):\n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "   \n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_all_features(df):\n",
    "    # Select relevant columns for feature generation\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features(df)\n",
    "    df = other_features(df)\n",
    "    gc.collect()  \n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28004b87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:39.194996Z",
     "iopub.status.busy": "2023-11-27T16:29:39.194486Z",
     "iopub.status.idle": "2023-11-27T16:29:39.205946Z",
     "shell.execute_reply": "2023-11-27T16:29:39.205211Z"
    },
    "papermill": {
     "duration": 0.025524,
     "end_time": "2023-11-27T16:29:39.207816",
     "exception": false,
     "start_time": "2023-11-27T16:29:39.182292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca43675f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:39.231722Z",
     "iopub.status.busy": "2023-11-27T16:29:39.231488Z",
     "iopub.status.idle": "2023-11-27T16:29:39.236522Z",
     "shell.execute_reply": "2023-11-27T16:29:39.235787Z"
    },
    "papermill": {
     "duration": 0.01909,
     "end_time": "2023-11-27T16:29:39.238379",
     "exception": false,
     "start_time": "2023-11-27T16:29:39.219289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online mode\n"
     ]
    }
   ],
   "source": [
    "if is_offline:\n",
    "    \n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "else:\n",
    "    df_train = df\n",
    "    print(\"Online mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3fbdc9",
   "metadata": {
    "papermill": {
     "duration": 0.011348,
     "end_time": "2023-11-27T16:29:39.261300",
     "exception": false,
     "start_time": "2023-11-27T16:29:39.249952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Uncomment the lines below if you want to train other folds of the SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f355266d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:39.285311Z",
     "iopub.status.busy": "2023-11-27T16:29:39.285070Z",
     "iopub.status.idle": "2023-11-27T16:29:40.955079Z",
     "shell.execute_reply": "2023-11-27T16:29:40.954277Z"
    },
    "papermill": {
     "duration": 1.684595,
     "end_time": "2023-11-27T16:29:40.957417",
     "exception": false,
     "start_time": "2023-11-27T16:29:39.272822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "#     if is_offline:\n",
    "#         df_train_feats = generate_all_features(df_train)\n",
    "#         print(\"Build Train Feats Finished.\")\n",
    "#         df_valid_feats = generate_all_features(df_valid)\n",
    "#         print(\"Build Valid Feats Finished.\")\n",
    "#         df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "#     else:\n",
    "#         df_train_feats = generate_all_features(df_train)\n",
    "#         print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "#     df_train_feats = reduce_mem_usage(df_train_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e205a96",
   "metadata": {
    "papermill": {
     "duration": 0.011355,
     "end_time": "2023-11-27T16:29:40.981262",
     "exception": false,
     "start_time": "2023-11-27T16:29:40.969907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Commented below you will find the SVR training loops. Each fold takes approx. 10 hours to train, so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05d15ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:41.006147Z",
     "iopub.status.busy": "2023-11-27T16:29:41.005307Z",
     "iopub.status.idle": "2023-11-27T16:29:41.017007Z",
     "shell.execute_reply": "2023-11-27T16:29:41.016192Z"
    },
    "papermill": {
     "duration": 0.026111,
     "end_time": "2023-11-27T16:29:41.018830",
     "exception": false,
     "start_time": "2023-11-27T16:29:40.992719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cuml.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "import gc\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# # Define SVR parameters (adjust these based on your dataset)\n",
    "# svr_params = {\n",
    "#     \"kernel\": \"rbf\",  # Example kernel type\n",
    "#     \"C\": 1,           # Regularization parameter\n",
    "#     \"gamma\": 'scale', # Kernel coefficient\n",
    "#     # ... other SVR parameters as needed ...\n",
    "# }\n",
    "\n",
    "# feature_name = list(df_train_feats.columns)\n",
    "# print(f\"Feature length = {len(feature_name)}\")\n",
    "\n",
    "# num_folds = 5\n",
    "# fold_size = 480 // num_folds\n",
    "# gap = 5\n",
    "\n",
    "# models = []\n",
    "# scores = []\n",
    "\n",
    "# model_save_path = 'modelitos_para_despues'\n",
    "# if not os.path.exists(model_save_path):\n",
    "#     os.makedirs(model_save_path)\n",
    "\n",
    "# date_ids = df_train['date_id'].values\n",
    "\n",
    "# for i in range(num_folds):\n",
    "#     start = i * fold_size\n",
    "#     end = start + fold_size\n",
    "#     if i < num_folds - 1:  # No need to purge after the last fold\n",
    "#         purged_start = end - 2\n",
    "#         purged_end = end + gap + 2\n",
    "#         train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "#     else:\n",
    "#         train_indices = (date_ids >= start) & (date_ids < end)\n",
    "    \n",
    "#     test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "    \n",
    "#     df_fold_train = df_train_feats[train_indices]\n",
    "#     df_fold_train_target = df_train['target'][train_indices]\n",
    "#     df_fold_valid = df_train_feats[test_indices]\n",
    "#     df_fold_valid_target = df_train['target'][test_indices]\n",
    "\n",
    "#     # Impute NaN values\n",
    "#     df_fold_train_imputed = imputer.fit_transform(df_fold_train[feature_name])\n",
    "#     df_fold_valid_imputed = imputer.transform(df_fold_valid[feature_name])\n",
    "    \n",
    "#     print(f\"Fold {i+1} Model Training\")\n",
    "    \n",
    "#     # Train an SVR model for the current fold\n",
    "#     svr_model = SVR(**svr_params)\n",
    "#     svr_model.fit(df_fold_train_imputed, df_fold_train_target)\n",
    "\n",
    "#     models.append(svr_model)\n",
    "    \n",
    "#     # Save the model to a file using joblib\n",
    "#     model_filename = os.path.join(model_save_path, f'svr_model_fold_{i+1}.joblib')\n",
    "#     joblib.dump(svr_model, model_filename)\n",
    "#     print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "\n",
    "#     # Evaluate model performance on the validation set\n",
    "#     fold_predictions = svr_model.predict(df_fold_valid_imputed)\n",
    "#     fold_score = mean_absolute_error(df_fold_valid_target, fold_predictions)\n",
    "#     scores.append(fold_score)\n",
    "#     print(f\"Fold {i+1} MAE: {fold_score}\")\n",
    "\n",
    "#     # Free up memory\n",
    "#     del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target, df_fold_valid_imputed, df_fold_train_imputed\n",
    "#     gc.collect()\n",
    "\n",
    "# print(f\"Training final model\")\n",
    "\n",
    "# # Train the final model on the entire dataset\n",
    "# final_model = SVR(**svr_params)\n",
    "# final_model.fit(df_train_feats[feature_name], df_train['target'])\n",
    "\n",
    "# # Save the final model to a file\n",
    "# final_model_filename = os.path.join(model_save_path, 'final_svr_model.joblib')\n",
    "# joblib.dump(final_model, final_model_filename)\n",
    "# print(f\"Final model saved to {final_model_filename}\")\n",
    "\n",
    "# print(f\"Average MAE across all folds: {np.mean(scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab4ab64",
   "metadata": {
    "papermill": {
     "duration": 0.011241,
     "end_time": "2023-11-27T16:29:41.041618",
     "exception": false,
     "start_time": "2023-11-27T16:29:41.030377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load the sample SVR model provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bcb8fa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:29:41.065864Z",
     "iopub.status.busy": "2023-11-27T16:29:41.065594Z",
     "iopub.status.idle": "2023-11-27T16:30:07.626135Z",
     "shell.execute_reply": "2023-11-27T16:30:07.625158Z"
    },
    "papermill": {
     "duration": 26.575228,
     "end_time": "2023-11-27T16:30:07.628526",
     "exception": false,
     "start_time": "2023-11-27T16:29:41.053298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load the model from the file\n",
    "final_model = joblib.load(\"/kaggle/input/svr-models/svr_model_fold_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f4b1397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:30:07.663132Z",
     "iopub.status.busy": "2023-11-27T16:30:07.662404Z",
     "iopub.status.idle": "2023-11-27T16:30:07.672370Z",
     "shell.execute_reply": "2023-11-27T16:30:07.670787Z"
    },
    "papermill": {
     "duration": 0.031625,
     "end_time": "2023-11-27T16:30:07.676634",
     "exception": false,
     "start_time": "2023-11-27T16:30:07.645009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [final_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8db47",
   "metadata": {
    "papermill": {
     "duration": 0.01728,
     "end_time": "2023-11-27T16:30:07.707791",
     "exception": false,
     "start_time": "2023-11-27T16:30:07.690511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load other models that you may want to use for an ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4d3d266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:30:07.737392Z",
     "iopub.status.busy": "2023-11-27T16:30:07.736410Z",
     "iopub.status.idle": "2023-11-27T16:31:06.752462Z",
     "shell.execute_reply": "2023-11-27T16:31:06.751448Z"
    },
    "papermill": {
     "duration": 59.030873,
     "end_time": "2023-11-27T16:31:06.754635",
     "exception": false,
     "start_time": "2023-11-27T16:30:07.723762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for fold 1 loaded from /kaggle/input/ensemble-of-models/results/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/ensemble-of-models/results/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/ensemble-of-models/results/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/ensemble-of-models/results/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/ensemble-of-models/results/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/ensemble-of-models/results/modelitos_para_despues/doblez-conjunto.txt\n",
      "Model for fold 1 loaded from /kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues/doblez-conjunto.txt\n",
      "Model for fold 1 loaded from /kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues/doblez-conjunto.txt\n",
      "Model for fold 1 loaded from /kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues/doblez-conjunto.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lightgbm as lgb\n",
    "\n",
    "def load_models_from_folder(model_save_path, num_folds=5):\n",
    "    loaded_models = []\n",
    "\n",
    "    # Load each fold model\n",
    "    for i in range(1, num_folds + 1):\n",
    "        model_filename = os.path.join(model_save_path, f'doblez_{i}.txt')\n",
    "        if os.path.exists(model_filename):\n",
    "            loaded_model = lgb.Booster(model_file=model_filename)\n",
    "            loaded_models.append(loaded_model)\n",
    "            print(f\"Model for fold {i} loaded from {model_filename}\")\n",
    "        else:\n",
    "            print(f\"Model file {model_filename} not found.\")\n",
    "\n",
    "    # Load the final model\n",
    "    final_model_filename = os.path.join(model_save_path, 'doblez-conjunto.txt')\n",
    "    if os.path.exists(final_model_filename):\n",
    "        final_model = lgb.Booster(model_file=final_model_filename)\n",
    "        loaded_models.append(final_model)\n",
    "        print(f\"Final model loaded from {final_model_filename}\")\n",
    "    else:\n",
    "        print(f\"Final model file {final_model_filename} not found.\")\n",
    "    \n",
    "    return loaded_models\n",
    "\n",
    "# Assuming you have a list of folders from which to load the models\n",
    "folders = [\n",
    "#     '/kaggle/input/lightgbm-models/modelitos_para_despues',\n",
    "    '/kaggle/input/ensemble-of-models/results/modelitos_para_despues',\n",
    "    '/kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues',\n",
    "    '/kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues',\n",
    "    '/kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues',\n",
    "#      '/kaggle/input/ensemble-of-models/results (4)/modelitos_para_despues',\n",
    "#      '/kaggle/input/ensemble-of-models/results (5)/modelitos_para_despues',\n",
    "#     '/kaggle/input/ensemble-of-models/results (6)/modelitos_para_despues',\n",
    "#     '/kaggle/input/ensemble-of-models/results (7)/modelitos_para_despues',\n",
    "]\n",
    "num_folds = 5\n",
    "all_loaded_models = []\n",
    "for folder in folders:\n",
    "    all_loaded_models.extend(load_models_from_folder(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13011b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T16:31:06.783910Z",
     "iopub.status.busy": "2023-11-27T16:31:06.783414Z",
     "iopub.status.idle": "2023-11-27T16:36:50.797723Z",
     "shell.execute_reply": "2023-11-27T16:36:50.796546Z"
    },
    "papermill": {
     "duration": 344.031199,
     "end_time": "2023-11-27T16:36:50.799892",
     "exception": false,
     "start_time": "2023-11-27T16:31:06.768693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "10 qps: 2.516120433807373\n",
      "20 qps: 2.2827959299087524\n",
      "30 qps: 2.1962295055389403\n",
      "40 qps: 2.1692974507808684\n",
      "50 qps: 2.140265426635742\n",
      "60 qps: 2.1205986698468524\n",
      "70 qps: 2.1110734122140067\n",
      "80 qps: 2.10722231566906\n",
      "90 qps: 2.1005428552627565\n",
      "100 qps: 2.099291501045227\n",
      "110 qps: 2.0939115806059405\n",
      "120 qps: 2.0881338357925414\n",
      "130 qps: 2.0882353415855994\n",
      "140 qps: 2.0862694450787136\n",
      "150 qps: 2.0813777303695677\n",
      "160 qps: 2.0815048426389695\n",
      "The code will take approximately 2.3823 hours to reason about\n"
     ]
    }
   ],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "\n",
    "if is_infer:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 0\n",
    "    y_min, y_max = -64, 64\n",
    "    qps, predictions = [], []\n",
    "    cache = pd.DataFrame()\n",
    "\n",
    "    # Weights for each fold model\n",
    "    model_weights_lgbm = [1 / len(all_loaded_models)] * len(all_loaded_models)\n",
    "    model_weights = [1 / len(models)] * len(models)\n",
    "    \n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        try:\n",
    "            test = test.drop('currently_scored', axis = 1)\n",
    "            now_time = time.time()\n",
    "            cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "            if counter > 0:\n",
    "                cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "            feat = generate_all_features(cache)[-len(test):]\n",
    "            pred = np.zeros(len(test))\n",
    "            # Supportive models\n",
    "            for model, weight in zip(all_loaded_models, model_weights_lgbm):\n",
    "                pred += weight * model.predict(feat)\n",
    "            # Generate predictions for each model and calculate the weighted average\n",
    "            svr_predictions = np.zeros(len(test))\n",
    "            for model, weight in zip(models, model_weights):\n",
    "                svr_predictions += weight * model.predict(feat.fillna(0.0))\n",
    "                \n",
    "            svr_predictions = [pred[i] if np.isnan(x) else x for i, x in enumerate(svr_predictions)]\n",
    "            \n",
    "            svr_predictions = (len(models) * svr_predictions + len(all_loaded_models) * pred) / (len(models) + len(all_loaded_models))\n",
    "            \n",
    "            svr_predictions = zero_sum(svr_predictions, test['bid_size'] + test['ask_size'])\n",
    "            clipped_predictions = np.clip(svr_predictions, y_min, y_max)\n",
    "            # Replace NaN values with zeros\n",
    "            sample_prediction['target'] = clipped_predictions\n",
    "            env.predict(sample_prediction)\n",
    "        except:\n",
    "            sample_prediction['target'] = 0.0\n",
    "            env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0d0587",
   "metadata": {
    "papermill": {
     "duration": 0.014975,
     "end_time": "2023-11-27T16:36:50.830293",
     "exception": false,
     "start_time": "2023-11-27T16:36:50.815318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 3986474,
     "sourceId": 6941738,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4018611,
     "sourceId": 6991591,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4037788,
     "sourceId": 7021664,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 151409841,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 568.722217,
   "end_time": "2023-11-27T16:36:54.639909",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-27T16:27:25.917692",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
