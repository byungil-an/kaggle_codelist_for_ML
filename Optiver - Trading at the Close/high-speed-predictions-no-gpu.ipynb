{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d23b1e5",
   "metadata": {
    "papermill": {
     "duration": 0.006286,
     "end_time": "2023-11-19T13:10:04.160292",
     "exception": false,
     "start_time": "2023-11-19T13:10:04.154006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Welcome to our comprehensive notebook, where we delve into the powerful world of ensemble learning using LightGBM (Light Gradient Boosting Machine). LightGBM, known for its efficiency and speed, is a gradient boosting framework that has gained popularity in the machine learning community, especially for large datasets.\n",
    "\n",
    "In this notebook, we have designed an ensemble approach with LightGBM models. Ensemble learning is a technique that combines the predictions from multiple machine learning algorithms to make more accurate predictions than any individual model. This approach is particularly effective in improving the performance of models on complex datasets, as it capitalizes on the strengths of each individual model while mitigating their weaknesses.\n",
    "\n",
    "Our focus here is to demonstrate how an ensemble of LightGBM models can be effectively employed to achieve superior predictive performance. This is especially beneficial for scenarios where using GPU resources is not feasible or preferred. We take advantage of LightGBM's ability to handle large datasets with ease, even on CPU, making it an ideal choice for environments with hardware constraints.\n",
    "\n",
    "Of course you can run on GPU if you wish. \n",
    "\n",
    "I'm also sharing the dataset with you if you want to ensemble with your own models: https://www.kaggle.com/datasets/verracodeguacas/ensemble-of-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f9395c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-19T13:10:04.176621Z",
     "iopub.status.busy": "2023-11-19T13:10:04.175819Z",
     "iopub.status.idle": "2023-11-19T13:10:05.824500Z",
     "shell.execute_reply": "2023-11-19T13:10:05.823337Z"
    },
    "papermill": {
     "duration": 1.659854,
     "end_time": "2023-11-19T13:10:05.828236",
     "exception": false,
     "start_time": "2023-11-19T13:10:04.168382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc  # Garbage collection for memory management\n",
    "import os  # Operating system-related functions\n",
    "import time  # Time-related functions\n",
    "import warnings  # Handling warnings\n",
    "from itertools import combinations  # For creating combinations of elements\n",
    "from warnings import simplefilter  # Simplifying warning handling\n",
    "\n",
    "# üì¶ Importing machine learning libraries\n",
    "import joblib  # For saving and loading models\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n",
    "\n",
    "# ü§ê Disable warnings to keep the code clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# üìä Define flags and variables\n",
    "is_offline = False  # Flag for online/offline mode\n",
    "is_train = True  # Flag for training mode\n",
    "is_infer = True  # Flag for inference mode\n",
    "max_lookback = np.nan  # Maximum lookback (not specified)\n",
    "split_day = 435  # Split day for time series data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a218aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:10:05.841463Z",
     "iopub.status.busy": "2023-11-19T13:10:05.840921Z",
     "iopub.status.idle": "2023-11-19T13:10:28.858409Z",
     "shell.execute_reply": "2023-11-19T13:10:28.857124Z"
    },
    "papermill": {
     "duration": 23.027722,
     "end_time": "2023-11-19T13:10:28.861844",
     "exception": false,
     "start_time": "2023-11-19T13:10:05.834122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# üìÇ Read the dataset from a CSV file using Pandas\n",
    "df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n",
    "\n",
    "# üßπ Remove rows with missing values in the \"target\" column\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "\n",
    "# üîÅ Reset the index of the DataFrame and apply the changes in place\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# üìè Get the shape of the DataFrame (number of rows and columns)\n",
    "df_shape = df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f0d40d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:10:28.875454Z",
     "iopub.status.busy": "2023-11-19T13:10:28.874372Z",
     "iopub.status.idle": "2023-11-19T13:10:30.339605Z",
     "shell.execute_reply": "2023-11-19T13:10:30.338282Z"
    },
    "papermill": {
     "duration": 1.474877,
     "end_time": "2023-11-19T13:10:30.342549",
     "exception": false,
     "start_time": "2023-11-19T13:10:28.867672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# üèéÔ∏è Import Numba for just-in-time (JIT) compilation and parallel processing\n",
    "from numba import njit, prange\n",
    "\n",
    "# üìä Function to compute triplet imbalance in parallel using Numba\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    # üîÅ Loop through all combinations of triplets\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        \n",
    "        # üîÅ Loop through rows of the DataFrame\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            # üö´ Prevent division by zero\n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "# üìà Function to calculate triplet imbalance for given price data and a DataFrame\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance using the Numba-optimized function\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def rolling_average(arr, window):\n",
    "    \"\"\"\n",
    "    Calculate the rolling average for a 1D numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "    arr (numpy.ndarray): Input array to calculate the rolling average.\n",
    "    window (int): The number of elements to consider for the moving average.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Array containing the rolling average values.\n",
    "    \"\"\"\n",
    "    n = len(arr)\n",
    "    result = np.empty(n)\n",
    "    result[:window] = np.nan  # Padding with NaN for elements where the window is not full\n",
    "    cumsum = np.cumsum(arr)\n",
    "\n",
    "    for i in range(window, n):\n",
    "        result[i] = (cumsum[i] - cumsum[i - window]) / window\n",
    "\n",
    "    return result\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_rolling_averages(df_values, window_sizes):\n",
    "    \"\"\"\n",
    "    Calculate the rolling averages for multiple window sizes in parallel.\n",
    "    \n",
    "    Parameters:\n",
    "    df_values (numpy.ndarray): 2D array of values to calculate the rolling averages.\n",
    "    window_sizes (List[int]): List of window sizes for the rolling averages.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: A 3D array containing the rolling averages for each window size.\n",
    "    \"\"\"\n",
    "    num_rows, num_features = df_values.shape\n",
    "    num_windows = len(window_sizes)\n",
    "    rolling_features = np.empty((num_rows, num_features, num_windows))\n",
    "\n",
    "    for feature_idx in prange(num_features):\n",
    "        for window_idx, window in enumerate(window_sizes):\n",
    "            rolling_features[:, feature_idx, window_idx] = rolling_average(df_values[:, feature_idx], window)\n",
    "\n",
    "    return rolling_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c7fc1",
   "metadata": {
    "papermill": {
     "duration": 0.005022,
     "end_time": "2023-11-19T13:10:30.353263",
     "exception": false,
     "start_time": "2023-11-19T13:10:30.348241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üìä Feature Generation Functions üìä\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab9b89",
   "metadata": {
    "papermill": {
     "duration": 0.005225,
     "end_time": "2023-11-19T13:10:30.363686",
     "exception": false,
     "start_time": "2023-11-19T13:10:30.358461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "\n",
    "\n",
    "1. `imbalance_features(df)`:\n",
    "   - This function takes a DataFrame `df` as input.\n",
    "   - It calculates various features related to price and size data using Pandas' `eval` function, creating new columns in the DataFrame for each feature.\n",
    "   - It then creates pairwise price imbalance features for combinations of price columns.\n",
    "   - Next, it calculates triplet imbalance features using the Numba-optimized function `calculate_triplet_imbalance_numba`.\n",
    "   - It calculates the rolling features.\n",
    "   - Finally, it calculates additional features, including momentum, spread, intensity, pressure, market urgency, and depth pressure.\n",
    "   - It also calculates statistical aggregation features (mean, standard deviation, skewness, kurtosis) for both price and size columns.\n",
    "   - Shifted, return, and diff features are generated for specific columns.\n",
    "   - Infinite values in the DataFrame are replaced with 0.\n",
    "\n",
    "2. `other_features(df)`:\n",
    "   - This function adds time-related and stock-related features to the DataFrame.\n",
    "   - It calculates the day of the week, seconds, and minutes from the \"date_id\" and \"seconds_in_bucket\" columns.\n",
    "   - It maps global features from a predefined dictionary to the DataFrame based on the \"stock_id.\"\n",
    "\n",
    "3. `generate_all_features(df)`:\n",
    "   - This function combines the features generated by the `imbalance_features` and `other_features` functions.\n",
    "   - It selects the relevant columns for feature generation, applies the `imbalance_features` function, adds time and stock-related features using the `other_features` function, and then performs garbage collection to free up memory.\n",
    "   - The function returns a DataFrame containing the generated features, excluding certain columns like \"row_id,\" \"target,\" \"time_id,\" and \"date_id.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58cadf2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:10:30.376393Z",
     "iopub.status.busy": "2023-11-19T13:10:30.375902Z",
     "iopub.status.idle": "2023-11-19T13:10:30.383311Z",
     "shell.execute_reply": "2023-11-19T13:10:30.382167Z"
    },
    "papermill": {
     "duration": 0.016957,
     "end_time": "2023-11-19T13:10:30.385827",
     "exception": false,
     "start_time": "2023-11-19T13:10:30.368870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online mode\n"
     ]
    }
   ],
   "source": [
    "# Check if the code is running in offline or online mode\n",
    "if is_offline:\n",
    "    # In offline mode, split the data into training and validation sets based on the split_day\n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    \n",
    "    # Display a message indicating offline mode and the shapes of the training and validation sets\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "else:\n",
    "    # In online mode, use the entire dataset for training\n",
    "    df_train = df\n",
    "    \n",
    "    # Display a message indicating online mode\n",
    "    print(\"Online mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429ccb97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:10:30.398702Z",
     "iopub.status.busy": "2023-11-19T13:10:30.398310Z",
     "iopub.status.idle": "2023-11-19T13:10:32.281204Z",
     "shell.execute_reply": "2023-11-19T13:10:32.280119Z"
    },
    "papermill": {
     "duration": 1.893227,
     "end_time": "2023-11-19T13:10:32.284459",
     "exception": false,
     "start_time": "2023-11-19T13:10:30.391232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5fa873",
   "metadata": {
    "papermill": {
     "duration": 0.005219,
     "end_time": "2023-11-19T13:10:32.295662",
     "exception": false,
     "start_time": "2023-11-19T13:10:32.290443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bringing the LightGBM models to the mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e7bfab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:10:32.308823Z",
     "iopub.status.busy": "2023-11-19T13:10:32.308401Z",
     "iopub.status.idle": "2023-11-19T13:10:33.705356Z",
     "shell.execute_reply": "2023-11-19T13:10:33.704179Z"
    },
    "papermill": {
     "duration": 1.407416,
     "end_time": "2023-11-19T13:10:33.708530",
     "exception": false,
     "start_time": "2023-11-19T13:10:32.301114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import lightgbm as lgb\n",
    "\n",
    "def imbalance_features_lgbm(df):\n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "   \n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "def other_features_lgbm(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_all_features_lgbm(df):\n",
    "    # Select relevant columns for feature generation\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features_lgbm(df)\n",
    "    df = other_features_lgbm(df)\n",
    "    gc.collect()  \n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    return df[feature_name]\n",
    "\n",
    "# model_save_path = '/kaggle/input/lightgbm-models/modelitos_para_despues'\n",
    "# num_folds = 5  # The number of folds you used during training\n",
    "\n",
    "# loaded_models = []\n",
    "\n",
    "# # Load each model\n",
    "# for i in range(1, num_folds + 1):\n",
    "#     model_filename = os.path.join(model_save_path, f'doblez_{i}.txt')\n",
    "#     if os.path.exists(model_filename):\n",
    "#         loaded_model = lgb.Booster(model_file=model_filename)\n",
    "#         loaded_models.append(loaded_model)\n",
    "#         print(f\"Model for fold {i} loaded from {model_filename}\")\n",
    "#     else:\n",
    "#         print(f\"Model file {model_filename} not found.\")\n",
    "\n",
    "# # Load the final model\n",
    "# final_model_filename = os.path.join(model_save_path, 'doblez-conjunto.txt')\n",
    "# if os.path.exists(final_model_filename):\n",
    "#     final_model = lgb.Booster(model_file=final_model_filename)\n",
    "#     loaded_models.append(final_model)\n",
    "#     print(f\"Final model loaded from {final_model_filename}\")\n",
    "# else:\n",
    "#     print(f\"Final model file {final_model_filename} not found.\")\n",
    "\n",
    "# Now 'loaded_models' contains the models loaded from the files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5aa89",
   "metadata": {
    "papermill": {
     "duration": 0.005154,
     "end_time": "2023-11-19T13:10:33.719422",
     "exception": false,
     "start_time": "2023-11-19T13:10:33.714268",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I think some of the models below are better than the others. You can choose what \"folders\" to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dfac58d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:10:33.732211Z",
     "iopub.status.busy": "2023-11-19T13:10:33.731699Z",
     "iopub.status.idle": "2023-11-19T13:13:07.115950Z",
     "shell.execute_reply": "2023-11-19T13:13:07.114713Z"
    },
    "papermill": {
     "duration": 153.394109,
     "end_time": "2023-11-19T13:13:07.118758",
     "exception": false,
     "start_time": "2023-11-19T13:10:33.724649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for fold 1 loaded from /kaggle/input/lightgbm-models/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/lightgbm-models/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/lightgbm-models/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/lightgbm-models/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/lightgbm-models/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/lightgbm-models/modelitos_para_despues/doblez-conjunto.txt\n",
      "Model for fold 1 loaded from /kaggle/input/ensemble-of-models/results/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/ensemble-of-models/results/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/ensemble-of-models/results/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/ensemble-of-models/results/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/ensemble-of-models/results/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/ensemble-of-models/results/modelitos_para_despues/doblez-conjunto.txt\n",
      "Model for fold 1 loaded from /kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues/doblez-conjunto.txt\n",
      "Model for fold 1 loaded from /kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues/doblez-conjunto.txt\n",
      "Model for fold 1 loaded from /kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues/doblez-conjunto.txt\n",
      "Model for fold 1 loaded from /kaggle/input/ensemble-of-models/results (4)/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/ensemble-of-models/results (4)/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/ensemble-of-models/results (4)/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/ensemble-of-models/results (4)/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/ensemble-of-models/results (4)/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/ensemble-of-models/results (4)/modelitos_para_despues/doblez-conjunto.txt\n",
      "Model for fold 1 loaded from /kaggle/input/ensemble-of-models/results (5)/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/ensemble-of-models/results (5)/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/ensemble-of-models/results (5)/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/ensemble-of-models/results (5)/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/ensemble-of-models/results (5)/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/ensemble-of-models/results (5)/modelitos_para_despues/doblez-conjunto.txt\n",
      "Model for fold 1 loaded from /kaggle/input/ensemble-of-models/results (6)/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/ensemble-of-models/results (6)/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/ensemble-of-models/results (6)/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/ensemble-of-models/results (6)/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/ensemble-of-models/results (6)/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/ensemble-of-models/results (6)/modelitos_para_despues/doblez-conjunto.txt\n",
      "Model for fold 1 loaded from /kaggle/input/ensemble-of-models/results (7)/modelitos_para_despues/doblez_1.txt\n",
      "Model for fold 2 loaded from /kaggle/input/ensemble-of-models/results (7)/modelitos_para_despues/doblez_2.txt\n",
      "Model for fold 3 loaded from /kaggle/input/ensemble-of-models/results (7)/modelitos_para_despues/doblez_3.txt\n",
      "Model for fold 4 loaded from /kaggle/input/ensemble-of-models/results (7)/modelitos_para_despues/doblez_4.txt\n",
      "Model for fold 5 loaded from /kaggle/input/ensemble-of-models/results (7)/modelitos_para_despues/doblez_5.txt\n",
      "Final model loaded from /kaggle/input/ensemble-of-models/results (7)/modelitos_para_despues/doblez-conjunto.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lightgbm as lgb\n",
    "\n",
    "def load_models_from_folder(model_save_path, num_folds=5):\n",
    "    loaded_models = []\n",
    "\n",
    "    # Load each fold model\n",
    "    for i in range(1, num_folds + 1):\n",
    "        model_filename = os.path.join(model_save_path, f'doblez_{i}.txt')\n",
    "        if os.path.exists(model_filename):\n",
    "            loaded_model = lgb.Booster(model_file=model_filename)\n",
    "            loaded_models.append(loaded_model)\n",
    "            print(f\"Model for fold {i} loaded from {model_filename}\")\n",
    "        else:\n",
    "            print(f\"Model file {model_filename} not found.\")\n",
    "\n",
    "    # Load the final model\n",
    "    final_model_filename = os.path.join(model_save_path, 'doblez-conjunto.txt')\n",
    "    if os.path.exists(final_model_filename):\n",
    "        final_model = lgb.Booster(model_file=final_model_filename)\n",
    "        loaded_models.append(final_model)\n",
    "        print(f\"Final model loaded from {final_model_filename}\")\n",
    "    else:\n",
    "        print(f\"Final model file {final_model_filename} not found.\")\n",
    "    \n",
    "    return loaded_models\n",
    "\n",
    "# Assuming you have a list of folders from which to load the models\n",
    "folders = [\n",
    "    '/kaggle/input/lightgbm-models/modelitos_para_despues',\n",
    "    '/kaggle/input/ensemble-of-models/results/modelitos_para_despues',\n",
    "    '/kaggle/input/ensemble-of-models/results (1)/modelitos_para_despues',\n",
    "    '/kaggle/input/ensemble-of-models/results (2)/modelitos_para_despues',\n",
    "    '/kaggle/input/ensemble-of-models/results (3)/modelitos_para_despues',\n",
    "     '/kaggle/input/ensemble-of-models/results (4)/modelitos_para_despues',\n",
    "     '/kaggle/input/ensemble-of-models/results (5)/modelitos_para_despues',\n",
    "    '/kaggle/input/ensemble-of-models/results (6)/modelitos_para_despues',\n",
    "    '/kaggle/input/ensemble-of-models/results (7)/modelitos_para_despues',\n",
    "]\n",
    "num_folds = 5\n",
    "all_loaded_models = []\n",
    "for folder in folders:\n",
    "    all_loaded_models.extend(load_models_from_folder(folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "682f3f13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T13:13:07.142419Z",
     "iopub.status.busy": "2023-11-19T13:13:07.142040Z",
     "iopub.status.idle": "2023-11-19T13:31:52.706948Z",
     "shell.execute_reply": "2023-11-19T13:31:52.705597Z"
    },
    "papermill": {
     "duration": 1125.580351,
     "end_time": "2023-11-19T13:31:52.710030",
     "exception": false,
     "start_time": "2023-11-19T13:13:07.129679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "counter: 0\n",
      "counter: 1\n",
      "counter: 2\n",
      "counter: 3\n",
      "counter: 4\n",
      "counter: 5\n",
      "counter: 6\n",
      "counter: 7\n",
      "counter: 8\n",
      "counter: 9\n",
      "10 queries per second: 6.037146282196045\n",
      "counter: 10\n",
      "counter: 11\n",
      "counter: 12\n",
      "counter: 13\n",
      "counter: 14\n",
      "counter: 15\n",
      "counter: 16\n",
      "counter: 17\n",
      "counter: 18\n",
      "counter: 19\n",
      "20 queries per second: 5.742031657695771\n",
      "counter: 20\n",
      "counter: 21\n",
      "counter: 22\n",
      "counter: 23\n",
      "counter: 24\n",
      "counter: 25\n",
      "counter: 26\n",
      "counter: 27\n",
      "counter: 28\n",
      "counter: 29\n",
      "30 queries per second: 5.637731218338013\n",
      "counter: 30\n",
      "counter: 31\n",
      "counter: 32\n",
      "counter: 33\n",
      "counter: 34\n",
      "counter: 35\n",
      "counter: 36\n",
      "counter: 37\n",
      "counter: 38\n",
      "counter: 39\n",
      "40 queries per second: 5.810316443443298\n",
      "counter: 40\n",
      "counter: 41\n",
      "counter: 42\n",
      "counter: 43\n",
      "counter: 44\n",
      "counter: 45\n",
      "counter: 46\n",
      "counter: 47\n",
      "counter: 48\n",
      "counter: 49\n",
      "50 queries per second: 5.800113725662231\n",
      "counter: 50\n",
      "counter: 51\n",
      "counter: 52\n",
      "counter: 53\n",
      "counter: 54\n",
      "counter: 55\n",
      "counter: 56\n",
      "counter: 57\n",
      "counter: 58\n",
      "counter: 59\n",
      "60 queries per second: 5.777953779697418\n",
      "counter: 60\n",
      "counter: 61\n",
      "counter: 62\n",
      "counter: 63\n",
      "counter: 64\n",
      "counter: 65\n",
      "counter: 66\n",
      "counter: 67\n",
      "counter: 68\n",
      "counter: 69\n",
      "70 queries per second: 5.773877269881112\n",
      "counter: 70\n",
      "counter: 71\n",
      "counter: 72\n",
      "counter: 73\n",
      "counter: 74\n",
      "counter: 75\n",
      "counter: 76\n",
      "counter: 77\n",
      "counter: 78\n",
      "counter: 79\n",
      "80 queries per second: 5.767326524853706\n",
      "counter: 80\n",
      "counter: 81\n",
      "counter: 82\n",
      "counter: 83\n",
      "counter: 84\n",
      "counter: 85\n",
      "counter: 86\n",
      "counter: 87\n",
      "counter: 88\n",
      "counter: 89\n",
      "90 queries per second: 5.810157243410746\n",
      "counter: 90\n",
      "counter: 91\n",
      "counter: 92\n",
      "counter: 93\n",
      "counter: 94\n",
      "counter: 95\n",
      "counter: 96\n",
      "counter: 97\n",
      "counter: 98\n",
      "counter: 99\n",
      "100 queries per second: 5.832444636821747\n",
      "counter: 100\n",
      "counter: 101\n",
      "counter: 102\n",
      "counter: 103\n",
      "counter: 104\n",
      "counter: 105\n",
      "counter: 106\n",
      "counter: 107\n",
      "counter: 108\n",
      "counter: 109\n",
      "110 queries per second: 5.9173146464607935\n",
      "counter: 110\n",
      "counter: 111\n",
      "counter: 112\n",
      "counter: 113\n",
      "counter: 114\n",
      "counter: 115\n",
      "counter: 116\n",
      "counter: 117\n",
      "counter: 118\n",
      "counter: 119\n",
      "120 queries per second: 6.022530982891719\n",
      "counter: 120\n",
      "counter: 121\n",
      "counter: 122\n",
      "counter: 123\n",
      "counter: 124\n",
      "counter: 125\n",
      "counter: 126\n",
      "counter: 127\n",
      "counter: 128\n",
      "counter: 129\n",
      "130 queries per second: 6.1121835011702315\n",
      "counter: 130\n",
      "counter: 131\n",
      "counter: 132\n",
      "counter: 133\n",
      "counter: 134\n",
      "counter: 135\n",
      "counter: 136\n",
      "counter: 137\n",
      "counter: 138\n",
      "counter: 139\n",
      "140 queries per second: 6.287362893990108\n",
      "counter: 140\n",
      "counter: 141\n",
      "counter: 142\n",
      "counter: 143\n",
      "counter: 144\n",
      "counter: 145\n",
      "counter: 146\n",
      "counter: 147\n",
      "counter: 148\n",
      "counter: 149\n",
      "150 queries per second: 6.501322875022888\n",
      "counter: 150\n",
      "counter: 151\n",
      "counter: 152\n",
      "counter: 153\n",
      "counter: 154\n",
      "counter: 155\n",
      "counter: 156\n",
      "counter: 157\n",
      "counter: 158\n",
      "counter: 159\n",
      "160 queries per second: 6.711257115006447\n",
      "counter: 160\n",
      "counter: 161\n",
      "counter: 162\n",
      "counter: 163\n",
      "counter: 164\n",
      "The code will take approximately 7.81 hours to reason about\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "\n",
    "# Inference\n",
    "if is_infer:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()  # Setting up the environment for the competition\n",
    "    iter_test = env.iter_test()   # Getting the iterator for the test set\n",
    "    counter = 0                   # Initializing a counter\n",
    "    y_min, y_max = -64, 64        # Setting prediction boundaries\n",
    "    qps = []                      # Queries per second tracking\n",
    "    cache = pd.DataFrame()        # Initializing a cache to store test data\n",
    "    \n",
    "    model_weights = [1/len(all_loaded_models)] * len(all_loaded_models)\n",
    "    \n",
    "    for (test_df, revealed_targets, sample_prediction_df) in iter_test:\n",
    "        test_df = test_df.drop('currently_scored', axis=1)\n",
    "        now_time = time.time()    # Current time for performance measurement\n",
    "        print('counter:', counter)\n",
    "        # Concatenating new test data with the cache, keeping only the last 21 observations per stock_id\n",
    "        cache = pd.concat([cache, test_df], ignore_index=True, axis=0)\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby('stock_id').tail(21).reset_index(drop=True)\n",
    "        \n",
    "        feat = generate_all_features_lgbm(cache)[-len(test_df):]\n",
    "        pred = model_weights[0] * all_loaded_models[0].predict(feat)\n",
    "        # Generate predictions for each model and calculate the weighted average\n",
    "        for model, weight in zip(all_loaded_models[1:], model_weights[1:]):\n",
    "            pred += weight * model.predict(feat)\n",
    "        \n",
    "        # Apply your zero-sum and clipping operations\n",
    "        pred = zero_sum(pred, test_df['bid_size'] + test_df['ask_size'])\n",
    "        clipped_predictions = np.clip(pred, y_min, y_max)\n",
    "        \n",
    "        # Set the predictions in the sample_prediction_df\n",
    "        sample_prediction_df['target'] = clipped_predictions\n",
    "        \n",
    "        # Use the environment to make predictions\n",
    "        env.predict(sample_prediction_df)\n",
    "        \n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        \n",
    "        if counter % 10 == 0:\n",
    "            print(f\"{counter} queries per second: {np.mean(qps)}\")\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 2)} hours to reason about\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 3986474,
     "sourceId": 6941738,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4018611,
     "sourceId": 6991591,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1315.050861,
   "end_time": "2023-11-19T13:31:55.767609",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-19T13:10:00.716748",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
