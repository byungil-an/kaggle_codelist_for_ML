{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7134bf93",
   "metadata": {
    "papermill": {
     "duration": 0.005349,
     "end_time": "2023-11-01T07:40:34.083484",
     "exception": false,
     "start_time": "2023-11-01T07:40:34.078135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ***Everyone's votes are my motivation to update the public kernel.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39b8770",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-01T07:40:34.095112Z",
     "iopub.status.busy": "2023-11-01T07:40:34.094734Z",
     "iopub.status.idle": "2023-11-01T07:40:38.972736Z",
     "shell.execute_reply": "2023-11-01T07:40:38.971844Z"
    },
    "papermill": {
     "duration": 4.88669,
     "end_time": "2023-11-01T07:40:38.975131",
     "exception": false,
     "start_time": "2023-11-01T07:40:34.088441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "is_offline = False\n",
    "is_train = True\n",
    "is_infer = True\n",
    "max_lookback = np.nan\n",
    "split_day = 435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc98cca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T07:40:38.986628Z",
     "iopub.status.busy": "2023-11-01T07:40:38.986335Z",
     "iopub.status.idle": "2023-11-01T07:40:57.204910Z",
     "shell.execute_reply": "2023-11-01T07:40:57.203989Z"
    },
    "papermill": {
     "duration": 18.226613,
     "end_time": "2023-11-01T07:40:57.206843",
     "exception": false,
     "start_time": "2023-11-01T07:40:38.980230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237892, 17)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7926d9a5",
   "metadata": {
    "papermill": {
     "duration": 0.004959,
     "end_time": "2023-11-01T07:40:57.216932",
     "exception": false,
     "start_time": "2023-11-01T07:40:57.211973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Memory reduce function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ab5e3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T07:40:57.228467Z",
     "iopub.status.busy": "2023-11-01T07:40:57.228186Z",
     "iopub.status.idle": "2023-11-01T07:40:57.340540Z",
     "shell.execute_reply": "2023-11-01T07:40:57.339694Z"
    },
    "papermill": {
     "duration": 0.120259,
     "end_time": "2023-11-01T07:40:57.342408",
     "exception": false,
     "start_time": "2023-11-01T07:40:57.222149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90672c",
   "metadata": {
    "papermill": {
     "duration": 0.004868,
     "end_time": "2023-11-01T07:40:57.352536",
     "exception": false,
     "start_time": "2023-11-01T07:40:57.347668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Speed up triplet imbalance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6715156c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T07:40:57.363645Z",
     "iopub.status.busy": "2023-11-01T07:40:57.363347Z",
     "iopub.status.idle": "2023-11-01T07:40:58.005073Z",
     "shell.execute_reply": "2023-11-01T07:40:58.004252Z"
    },
    "papermill": {
     "duration": 0.649813,
     "end_time": "2023-11-01T07:40:58.007291",
     "exception": false,
     "start_time": "2023-11-01T07:40:57.357478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            if mid_val == min_val:  # Prevent division by zero\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b97e3",
   "metadata": {
    "papermill": {
     "duration": 0.00487,
     "end_time": "2023-11-01T07:40:58.017349",
     "exception": false,
     "start_time": "2023-11-01T07:40:58.012479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d9d9aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T07:40:58.028798Z",
     "iopub.status.busy": "2023-11-01T07:40:58.028484Z",
     "iopub.status.idle": "2023-11-01T07:40:58.048570Z",
     "shell.execute_reply": "2023-11-01T07:40:58.047712Z"
    },
    "papermill": {
     "duration": 0.028086,
     "end_time": "2023-11-01T07:40:58.050367",
     "exception": false,
     "start_time": "2023-11-01T07:40:58.022281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate imbalance features\n",
    "def imbalance_features(df):\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "    # V1\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    \n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "        \n",
    "    # V2\n",
    "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
    "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
    "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
    "    \n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "    # V3\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 5, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "            \n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size',\n",
    "                'wap', 'near_price', 'far_price']:\n",
    "        for window in [1, 2, 3, 5, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# generate time & stock features\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5\n",
    "    df[\"dom\"] = df[\"date_id\"] % 20\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60\n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60\n",
    "\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "# generate all features\n",
    "def generate_all_features(df):\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    df = imbalance_features(df)\n",
    "    df = other_features(df)\n",
    "    gc.collect()\n",
    "    \n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3199d27e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T07:40:58.061692Z",
     "iopub.status.busy": "2023-11-01T07:40:58.061361Z",
     "iopub.status.idle": "2023-11-01T07:40:58.073392Z",
     "shell.execute_reply": "2023-11-01T07:40:58.072497Z"
    },
    "papermill": {
     "duration": 0.019923,
     "end_time": "2023-11-01T07:40:58.075372",
     "exception": false,
     "start_time": "2023-11-01T07:40:58.055449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d244362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T07:40:58.086388Z",
     "iopub.status.busy": "2023-11-01T07:40:58.086144Z",
     "iopub.status.idle": "2023-11-01T07:40:58.091324Z",
     "shell.execute_reply": "2023-11-01T07:40:58.090447Z"
    },
    "papermill": {
     "duration": 0.013071,
     "end_time": "2023-11-01T07:40:58.093368",
     "exception": false,
     "start_time": "2023-11-01T07:40:58.080297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online mode\n"
     ]
    }
   ],
   "source": [
    "if is_offline:\n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "else:\n",
    "    df_train = df\n",
    "    print(\"Online mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8223af9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T07:40:58.104721Z",
     "iopub.status.busy": "2023-11-01T07:40:58.104084Z",
     "iopub.status.idle": "2023-11-01T07:42:15.555247Z",
     "shell.execute_reply": "2023-11-01T07:42:15.554410Z"
    },
    "papermill": {
     "duration": 77.45919,
     "end_time": "2023-11-01T07:42:15.557489",
     "exception": false,
     "start_time": "2023-11-01T07:40:58.098299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Online Train Feats Finished.\n"
     ]
    }
   ],
   "source": [
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    if is_offline:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "        df_valid_feats = generate_all_features(df_valid)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "    else:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729fe793",
   "metadata": {
    "papermill": {
     "duration": 0.004863,
     "end_time": "2023-11-01T07:42:15.567874",
     "exception": false,
     "start_time": "2023-11-01T07:42:15.563011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Because this is a time series dataset, we cannot use random KFold to partition the data, which will lead to data leakage (the model will have difficulty converging). Therefore, we strictly ensure that a part of the test set is not used, and divide all *training* data into (train, valid, test) in time series, valid is used as the basis for train iteration, and the optimal parameters are used on train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6affe93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T07:42:15.579445Z",
     "iopub.status.busy": "2023-11-01T07:42:15.579142Z",
     "iopub.status.idle": "2023-11-01T07:50:01.851053Z",
     "shell.execute_reply": "2023-11-01T07:50:01.849877Z"
    },
    "papermill": {
     "duration": 466.280368,
     "end_time": "2023-11-01T07:50:01.853439",
     "exception": false,
     "start_time": "2023-11-01T07:42:15.573071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature length = 147\n",
      "Valid Model Trainning.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.89975\n",
      "[200]\tvalid_0's l1: 5.89271\n",
      "[300]\tvalid_0's l1: 5.89094\n",
      "[400]\tvalid_0's l1: 5.89004\n",
      "[500]\tvalid_0's l1: 5.88995\n",
      "[600]\tvalid_0's l1: 5.88964\n",
      "[700]\tvalid_0's l1: 5.88968\n",
      "Early stopping, best iteration is:\n",
      "[628]\tvalid_0's l1: 5.88949\n",
      "Infer Model Trainning.\n"
     ]
    }
   ],
   "source": [
    "if is_train:\n",
    "    feature_name = list(df_train_feats.columns)\n",
    "    lgb_params = {\n",
    "        \"objective\" : \"mae\",\n",
    "        \"n_estimators\" : 3000,\n",
    "        \"num_leaves\" : 128,\n",
    "        \"subsample\" : 0.6,\n",
    "        \"colsample_bytree\" : 0.6,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"n_jobs\" : 4,\n",
    "        \"device\" : \"gpu\",\n",
    "        \"verbosity\": -1,\n",
    "        \"importance_type\" : \"gain\",\n",
    "    }\n",
    "\n",
    "    print(f\"Feature length = {len(feature_name)}\")\n",
    "\n",
    "    offline_split = df_train['date_id']>(split_day - 45)\n",
    "    df_offline_train = df_train_feats[~offline_split]\n",
    "    df_offline_valid = df_train_feats[offline_split]\n",
    "    df_offline_train_target = df_train['target'][~offline_split]\n",
    "    df_offline_valid_target = df_train['target'][offline_split]\n",
    "\n",
    "    print(\"Valid Model Trainning.\")\n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_model.fit(\n",
    "        df_offline_train[feature_name],\n",
    "        df_offline_train_target,\n",
    "        eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n",
    "        callbacks=[\n",
    "            lgb.callback.early_stopping(stopping_rounds=100),\n",
    "            lgb.callback.log_evaluation(period=100),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target\n",
    "    gc.collect()\n",
    "\n",
    "    # infer\n",
    "    df_train_target = df_train[\"target\"]\n",
    "    print(\"Infer Model Trainning.\")\n",
    "    infer_params = lgb_params.copy()\n",
    "    infer_params[\"n_estimators\"] = int(1.2 * lgb_model.best_iteration_)\n",
    "    infer_lgb_model = lgb.LGBMRegressor(**infer_params)\n",
    "    infer_lgb_model.fit(df_train_feats[feature_name], df_train_target)\n",
    "\n",
    "    if is_offline:   \n",
    "        # offline predictions\n",
    "        df_valid_target = df_valid[\"target\"]\n",
    "        offline_predictions = infer_lgb_model.predict(df_valid_feats[feature_name])\n",
    "        offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n",
    "        print(f\"Offline Score {np.round(offline_score, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066755a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T05:17:31.034028Z",
     "iopub.status.busy": "2023-10-20T05:17:31.033711Z",
     "iopub.status.idle": "2023-10-20T05:17:31.040838Z",
     "shell.execute_reply": "2023-10-20T05:17:31.039798Z",
     "shell.execute_reply.started": "2023-10-20T05:17:31.033999Z"
    },
    "papermill": {
     "duration": 0.005892,
     "end_time": "2023-11-01T07:50:01.865826",
     "exception": false,
     "start_time": "2023-11-01T07:50:01.859934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "| Version  | Feature Description                           | MAE(t/v) offline   | MAE(t/t) online       |  Inferance QPS  | Model              | Lookback |\n",
    "| :-----:  | :-------------------------------------------: | :----------------: | :-------------------: | :-------------: | :----------------: | :------: |\n",
    "| V1       | *Baseline* (41 feats)                         | 5.9712(*5.8578*)   | 5.9161(**5.3693**)    |  0.1887s        | LightGBM           |  /       |\n",
    "| V2       | Add *imbalance* feature (58 feats)            | 5.9589(*5.8465*)   | 5.9055(**5.3700**)    |  0.2351s        | LightGBM           |  /       |\n",
    "| V3       | Add *global* feature (68 feats)               | 5.9616(*5.8449*)   | 5.9069(**5.3683**)    |  0.1964s        | LightGBM           |  /       |\n",
    "> **MAE(t/v)** means train/valid offline scoring, **MAE(t/t)** means train/test online scoring. Inferance QPS means the inference time of a single piece of data (including feature engineering, model prediction). Lookback means the maximum number of lookback seqs for data and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b54c7",
   "metadata": {
    "papermill": {
     "duration": 0.005956,
     "end_time": "2023-11-01T07:50:01.877740",
     "exception": false,
     "start_time": "2023-11-01T07:50:01.871784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29cca289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T07:50:01.895974Z",
     "iopub.status.busy": "2023-11-01T07:50:01.894997Z",
     "iopub.status.idle": "2023-11-01T07:51:01.468620Z",
     "shell.execute_reply": "2023-11-01T07:51:01.467601Z"
    },
    "papermill": {
     "duration": 59.584782,
     "end_time": "2023-11-01T07:51:01.470660",
     "exception": false,
     "start_time": "2023-11-01T07:50:01.885878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "10 qps: 0.39153788089752195\n",
      "20 qps: 0.36975682973861695\n",
      "30 qps: 0.36550401846567787\n",
      "40 qps: 0.361493855714798\n",
      "50 qps: 0.3599665451049805\n",
      "60 qps: 0.35805798768997193\n",
      "70 qps: 0.3569740159170968\n",
      "80 qps: 0.3563134133815765\n",
      "90 qps: 0.3570289664798313\n",
      "100 qps: 0.3565722036361694\n",
      "110 qps: 0.35599589564583517\n",
      "120 qps: 0.35608763098716734\n",
      "130 qps: 0.35588102707496055\n",
      "140 qps: 0.35549494028091433\n",
      "150 qps: 0.3555611213048299\n",
      "160 qps: 0.3554587081074715\n",
      "The code will take approximately 0.4072 hours to reason about\n"
     ]
    }
   ],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices)/np.sum(std_error)\n",
    "    out = prices-std_error*step\n",
    "    \n",
    "    return out\n",
    "\n",
    "if is_infer:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 0\n",
    "    y_min, y_max = -64, 64\n",
    "    qps, predictions = [], []\n",
    "    cache = pd.DataFrame()\n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        now_time = time.time()\n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        feat = generate_all_features(cache)[-len(test):]\n",
    "        lgb_prediction = infer_lgb_model.predict(feat)\n",
    "#         lgb_prediction = zero_sum(lgb_prediction, test['bid_size'] + test['ask_size'])\n",
    "        lgb_prediction = lgb_prediction - np.mean(lgb_prediction)\n",
    "        clipped_predictions = np.clip(lgb_prediction, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "           \n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 632.464807,
   "end_time": "2023-11-01T07:51:03.102876",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-01T07:40:30.638069",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
