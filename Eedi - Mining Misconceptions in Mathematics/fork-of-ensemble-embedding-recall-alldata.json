{
    "title": "Eedi - Mining Misconceptions in Mathematics",
    "overview": "Overview\nIn this competition, you’ll develop an NLP model driven by ML to accurately predict the affinity between misconceptions and incorrect answers (distractors) in multiple-choice questions. This solution will suggest candidate misconceptions for distractors, making it easier for expert human teachers to tag distractors with misconceptions.\n\nDescription\nA Diagnostic Question is a multiple-choice question with four options: one correct answer and three distractors (incorrect answers). Each distractor is carefully crafted to capture a specific misconception. For example:\n\nIf a student selects the distractor \"13,\" they may have the misconception \"Carries out operations from left to right regardless of priority order.\"\n\nTagging distractors with appropriate misconceptions is essential but time-consuming, and it is difficult to maintain consistency across multiple human labellers. Misconceptions vary significantly in terms of description granularity, and new misconceptions are often discovered as human labellers tag distractors in new topic areas.\n\nInitial efforts to use pre-trained language models have not been successful, likely due to the complexity of the mathematical content in the questions. Therefore, a more efficient and consistent approach is needed to streamline the tagging process and enhance the overall quality.\n\nThis competition challenges you to develop a Natural Language Processing (NLP) model driven by Machine Learning (ML) that predicts the affinity between misconceptions and distractors. The goal is to create a model that not only aligns with known misconceptions but also generalizes to new, emerging misconceptions. Such a model would assist human labelers in accurately selecting suitable misconceptions from both existing and newly identified options.\n\nYour work could help improve the understanding and management of misconceptions, enhancing the educational experience for both students and teachers.\n\nEedi, alongside Vanderbilt University, and together with ​The Learning Agency Lab, an independent nonprofit based in Arizona, have collaborated with Kaggle on this competition.\n\nEvaluation\nSubmissions are evaluated according to the Mean Average Precision @ 25 (MAP@25):\n\nMAP@25 = (1 / |U|) * Σ_{u=1}^{|U|} (1 / min(m, 25)) * Σ_{k=1}^{min(n, 25)} P'(k)\n\nwhere U\n is the number of observations, P(k)\n is the precision at cutoff k\n, n\n is the number predictions submitted per observation, and rel(k)\n is an indicator function equaling 1 if the item at rank k\n is a relevant (correct) label, zero otherwise.\n\nOnce a correct label has been scored for an observation, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is A for an observation, the following predictions all score an average precision of 1.0.\n\n[A, B, C, D, E]\n[A, A, A, A, A]\n[A, B, A, C, A]\nThere is only one correct label per observation (hence no divisor term in front of the inner summation.)",
    "type": "Classification",
    "model": [
        "Pytorch"
    ],
    "code": "fork-of-ensemble-embedding-recall-alldata.ipynb",
    "score": 0.555,
    "evaluation": "MAP@K",
    "min score": 0.5421,
    "medal": "Silver"
}