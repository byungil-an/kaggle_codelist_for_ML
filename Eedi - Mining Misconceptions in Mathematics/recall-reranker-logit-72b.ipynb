{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5181e9",
   "metadata": {
    "_cell_guid": "fd1b354a-917c-4776-9773-ee2d22996d25",
    "_uuid": "6b9078b5-d216-4709-afd2-27615a5a3a57",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-09T16:14:18.944383Z",
     "iopub.status.busy": "2024-12-09T16:14:18.943770Z",
     "iopub.status.idle": "2024-12-09T16:14:48.991781Z",
     "shell.execute_reply": "2024-12-09T16:14:48.990736Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 30.055383,
     "end_time": "2024-12-09T16:14:48.994477",
     "exception": false,
     "start_time": "2024-12-09T16:14:18.939094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/notebookaaf1a23696\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/peft-0.13.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/trl-0.11.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/GPUtil-1.4.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/transformers-4.46.3-py3-none-any.whl\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/accelerate-1.1.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/optimum-1.23.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==v0.11.2) (2.4.0)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==v0.11.2) (3.0.1)\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/tyro-0.9.2-py3-none-any.whl (from trl==v0.11.2)\r\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==v0.11.2) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/coloredlogs-15.0.1-py2.py3-none-any.whl (from optimum)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.13.3)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto_gptq) (0.2.0)\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/rouge-1.0.1-py3-none-any.whl (from auto_gptq)\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/gekko-1.2.1-py3-none-any.whl (from auto_gptq)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==v0.11.2) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==v0.11.2) (3.1.4)\r\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==v0.11.2) (0.16)\r\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==v0.11.2) (13.7.1)\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/shtab-1.7.1-py3-none-any.whl (from tyro>=0.5.11->trl==v0.11.2)\r\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==v0.11.2) (4.3.0)\r\n",
      "Processing /kaggle/input/notebookaaf1a23696/humanfriendly-10.0-py2.py3-none-any.whl (from coloredlogs->optimum)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==v0.11.2) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==v0.11.2) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==v0.11.2) (2.2.2)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==v0.11.2) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==v0.11.2) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==v0.11.2) (3.9.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto_gptq) (1.16.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==v0.11.2) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==v0.11.2) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==v0.11.2) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==v0.11.2) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==v0.11.2) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==v0.11.2) (4.0.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==v0.11.2) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==v0.11.2) (2.18.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==v0.11.2) (2.1.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==v0.11.2) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==v0.11.2) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==v0.11.2) (2024.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==v0.11.2) (0.1.2)\r\n",
      "Installing collected packages: GPUtil, shtab, rouge, humanfriendly, gekko, coloredlogs, tyro, bitsandbytes, accelerate, transformers, trl, peft, optimum, auto_gptq\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 0.34.2\r\n",
      "    Uninstalling accelerate-0.34.2:\r\n",
      "      Successfully uninstalled accelerate-0.34.2\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.45.1\r\n",
      "    Uninstalling transformers-4.45.1:\r\n",
      "      Successfully uninstalled transformers-4.45.1\r\n",
      "Successfully installed GPUtil-1.4.0 accelerate-1.1.1 auto_gptq-0.7.1 bitsandbytes-0.44.1 coloredlogs-15.0.1 gekko-1.2.1 humanfriendly-10.0 optimum-1.23.3 peft-0.13.2 rouge-1.0.1 shtab-1.7.1 transformers-4.46.3 trl-0.11.2 tyro-0.9.2\r\n",
      "Processing /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\r\n",
      "Installing collected packages: logits-processor-zoo\r\n",
      "Successfully installed logits-processor-zoo-0.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install  peft trl==v0.11.2 bitsandbytes GPUtil transformers accelerate bitsandbytes optimum auto_gptq  \\\n",
    "    -U --no-index --find-links /kaggle/input/notebookaaf1a23696\n",
    "\n",
    "!pip install --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "445c2b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:14:49.007412Z",
     "iopub.status.busy": "2024-12-09T16:14:49.006601Z",
     "iopub.status.idle": "2024-12-09T16:17:27.477259Z",
     "shell.execute_reply": "2024-12-09T16:17:27.476321Z"
    },
    "papermill": {
     "duration": 158.47972,
     "end_time": "2024-12-09T16:17:27.479851",
     "exception": false,
     "start_time": "2024-12-09T16:14:49.000131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/wheels-vllm-0-6-3-post1\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/torchvision-0.19.1-cp310-cp310-manylinux1_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.19.1) (1.26.4)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (from torchvision==0.19.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.19.1) (10.3.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.4.1->torchvision==0.19.1) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.1->torchvision==0.19.1) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.1->torchvision==0.19.1) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.1->torchvision==0.19.1) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.1->torchvision==0.19.1) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.1->torchvision==0.19.1) (2024.6.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.4.1->torchvision==0.19.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.4.1->torchvision==0.19.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.4.1->torchvision==0.19.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (from torch==2.4.1->torchvision==0.19.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from torch==2.4.1->torchvision==0.19.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from torch==2.4.1->torchvision==0.19.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from torch==2.4.1->torchvision==0.19.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from torch==2.4.1->torchvision==0.19.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from torch==2.4.1->torchvision==0.19.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (from torch==2.4.1->torchvision==0.19.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.4.1->torchvision==0.19.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch==2.4.1->torchvision==0.19.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->torchvision==0.19.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.1->torchvision==0.19.1) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.1->torchvision==0.19.1) (1.3.0)\r\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.4.0\r\n",
      "    Uninstalling torch-2.4.0:\r\n",
      "      Successfully uninstalled torch-2.4.0\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.19.0\r\n",
      "    Uninstalling torchvision-0.19.0:\r\n",
      "      Successfully uninstalled torchvision-0.19.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "logits-processor-zoo 0.1.0 requires accelerate<0.27.0,>=0.26.1, but you have accelerate 1.1.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 torchvision-0.19.1 triton-3.0.0\r\n",
      "Looking in links: /kaggle/input/wheels-vllm-0-6-3-post1\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/vllm-0.6.3.post1-cp38-abi3-manylinux1_x86_64.whl\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from vllm) (5.9.3)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from vllm) (0.2.0)\r\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from vllm) (1.26.4)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from vllm) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from vllm) (4.66.4)\r\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from vllm) (9.0.0)\r\n",
      "Requirement already satisfied: transformers>=4.45.2 in /opt/conda/lib/python3.10/site-packages (from vllm) (4.46.3)\r\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from vllm) (0.20.0)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from vllm) (3.20.3)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from vllm) (3.9.5)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/openai-1.52.1-py3-none-any.whl (from vllm)\r\n",
      "Requirement already satisfied: uvicorn[standard] in /opt/conda/lib/python3.10/site-packages (from vllm) (0.30.1)\r\n",
      "Requirement already satisfied: pydantic>=2.9 in /opt/conda/lib/python3.10/site-packages (from vllm) (2.9.2)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from vllm) (10.3.0)\r\n",
      "Requirement already satisfied: prometheus-client>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from vllm) (0.20.0)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl (from vllm)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/lm_format_enforcer-0.10.6-py3-none-any.whl (from vllm)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/outlines-0.0.46-py3-none-any.whl (from vllm)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10 in /opt/conda/lib/python3.10/site-packages (from vllm) (4.12.2)\r\n",
      "Requirement already satisfied: filelock>=3.10.4 in /opt/conda/lib/python3.10/site-packages (from vllm) (3.15.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/partial_json_parser-0.2.1.1.post4-py3-none-any.whl (from vllm)\r\n",
      "Requirement already satisfied: pyzmq in /opt/conda/lib/python3.10/site-packages (from vllm) (26.0.3)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/msgspec-0.18.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/gguf-0.10.0-py3-none-any.whl (from vllm)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from vllm) (7.0.0)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/mistral_common-1.4.4-py3-none-any.whl (from mistral-common[opencv]>=1.4.4->vllm)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from vllm) (6.0.2)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/einops-0.8.0-py3-none-any.whl (from vllm)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/compressed_tensors-0.6.0-py3-none-any.whl (from vllm)\r\n",
      "Requirement already satisfied: ray>=2.9 in /opt/conda/lib/python3.10/site-packages (from vllm) (2.24.0)\r\n",
      "Requirement already satisfied: nvidia-ml-py in /opt/conda/lib/python3.10/site-packages (from vllm) (11.495.46)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (from vllm)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (from vllm)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl (from vllm)\r\n",
      "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /opt/conda/lib/python3.10/site-packages (from vllm) (0.111.0)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/interegular-0.3.3-py37-none-any.whl (from lm-format-enforcer==0.10.6->vllm)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from lm-format-enforcer==0.10.6->vllm) (21.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (2024.6.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->vllm) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm) (12.6.77)\r\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.37.2)\r\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.0.4)\r\n",
      "Requirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.27.0)\r\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.0.9)\r\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (5.10.0)\r\n",
      "Requirement already satisfied: orjson>=3.2.1 in /opt/conda/lib/python3.10/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (3.10.4)\r\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (2.1.1)\r\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /opt/conda/lib/python3.10/site-packages (from mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm) (4.22.0)\r\n",
      "Requirement already satisfied: opencv-python-headless<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mistral-common[opencv]>=1.4.4->vllm) (4.10.0.84)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.40.0->vllm) (4.4.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.40.0->vllm) (1.9.0)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from openai>=1.40.0->vllm)\r\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>=1.40.0->vllm) (1.3.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/lark-1.2.2-py3-none-any.whl (from outlines<0.1,>=0.0.43->vllm)\r\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from outlines<0.1,>=0.0.43->vllm) (1.6.0)\r\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from outlines<0.1,>=0.0.43->vllm) (3.0.0)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/diskcache-5.6.3-py3-none-any.whl (from outlines<0.1,>=0.0.43->vllm)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from outlines<0.1,>=0.0.43->vllm) (0.60.0)\r\n",
      "Requirement already satisfied: referencing in /opt/conda/lib/python3.10/site-packages (from outlines<0.1,>=0.0.43->vllm) (0.35.1)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from outlines<0.1,>=0.0.43->vllm) (3.0.1)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/pycountry-24.6.1-py3-none-any.whl (from outlines<0.1,>=0.0.43->vllm)\r\n",
      "Processing /kaggle/input/wheels-vllm-0-6-3-post1/pyairports-2.1.1-py3-none-any.whl (from outlines<0.1,>=0.0.43->vllm)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.9->vllm) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.9->vllm) (2.23.4)\r\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (8.1.7)\r\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.0.8)\r\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.3.1)\r\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->vllm) (2024.8.30)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken>=0.6.0->vllm) (2024.5.15)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.19.1->vllm) (0.25.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.45.2->vllm) (0.4.5)\r\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.14.0)\r\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.6.1)\r\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (1.0.1)\r\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.19.0)\r\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.22.0)\r\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (12.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->vllm) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->vllm) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->vllm) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->vllm) (4.0.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->vllm) (3.19.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.40.0->vllm) (1.2.0)\r\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (2.6.1)\r\n",
      "Requirement already satisfied: typer>=0.12.3 in /opt/conda/lib/python3.10/site-packages (from fastapi-cli>=0.0.2->fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.12.3)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (1.0.5)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.0->vllm) (2.1.5)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm) (2023.12.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm) (0.18.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->lm-format-enforcer==0.10.6->vllm) (3.1.2)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (2.2.2)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (0.70.16)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->outlines<0.1,>=0.0.43->vllm) (0.43.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.0->vllm) (1.3.0)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (13.7.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines<0.1,>=0.0.43->vllm) (1.16.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (2.18.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.1.2)\r\n",
      "Installing collected packages: pyairports, pycountry, partial-json-parser, msgspec, lark, jiter, interegular, gguf, einops, diskcache, tiktoken, torch, prometheus-fastapi-instrumentator, openai, lm-format-enforcer, xformers, torchvision, mistral-common, outlines, compressed-tensors, vllm\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.4.1\r\n",
      "    Uninstalling torch-2.4.1:\r\n",
      "      Successfully uninstalled torch-2.4.1\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.19.1\r\n",
      "    Uninstalling torchvision-0.19.1:\r\n",
      "      Successfully uninstalled torchvision-0.19.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "logits-processor-zoo 0.1.0 requires accelerate<0.27.0,>=0.26.1, but you have accelerate 1.1.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed compressed-tensors-0.6.0 diskcache-5.6.3 einops-0.8.0 gguf-0.10.0 interegular-0.3.3 jiter-0.6.1 lark-1.2.2 lm-format-enforcer-0.10.6 mistral-common-1.4.4 msgspec-0.18.6 openai-1.52.1 outlines-0.0.46 partial-json-parser-0.2.1.1.post4 prometheus-fastapi-instrumentator-7.0.0 pyairports-2.1.1 pycountry-24.6.1 tiktoken-0.7.0 torch-2.4.0 torchvision-0.19.0 vllm-0.6.3.post1 xformers-0.0.27.post2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index --find-links=/kaggle/input/wheels-vllm-0-6-3-post1 torchvision==0.19.1\n",
    "!pip install --no-index --find-links=/kaggle/input/wheels-vllm-0-6-3-post1 vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3218d3",
   "metadata": {
    "papermill": {
     "duration": 0.007583,
     "end_time": "2024-12-09T16:17:27.495549",
     "exception": false,
     "start_time": "2024-12-09T16:17:27.487966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Retrieval using 7B LLM\n",
    "Taken from: https://www.kaggle.com/code/sayoulala/use-llm-embedding-recall-infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf244be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:17:27.512566Z",
     "iopub.status.busy": "2024-12-09T16:17:27.512268Z",
     "iopub.status.idle": "2024-12-09T16:17:27.521372Z",
     "shell.execute_reply": "2024-12-09T16:17:27.520541Z"
    },
    "papermill": {
     "duration": 0.01988,
     "end_time": "2024-12-09T16:17:27.522877",
     "exception": false,
     "start_time": "2024-12-09T16:17:27.502997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_recall.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_recall.py\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_text(x):\n",
    "    import re\n",
    "    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "model_path = \"/kaggle/input/qwenqwen2.5-7b-instruct/pytorch/qwen2.5-14b-instruct/1\"\n",
    "lora_path=\"/kaggle/input/sys-v1/sys-v1-checkpoint-4980\"\n",
    "device='cuda:0'\n",
    "\n",
    "full_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "\n",
    "PROMPT=\"\"\"\"\n",
    "SubjectName: {SubjectName}\n",
    "ConstructName: {ConstructName}\n",
    "QuestionText: {QuestionText}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Misconcepte Incorrect answer: {INcorrectAnswer}\n",
    "\"\"\"\n",
    "\n",
    "task_description = 'Given a math question and a misconcepte incorrect answer, please retrieve the most accurate reason for the misconception.'\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "\n",
    "rows = []\n",
    "for idx, row in full_df.iterrows():\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        if option == row.CorrectAnswer:\n",
    "            continue\n",
    "            \n",
    "        correct_answer = row[f\"Answer{row.CorrectAnswer}Text\"]\n",
    "\n",
    "        #query_text =f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{correct_answer}\\n###Misconcepte Incorrect answer###:{option}.{row[f'Answer{option}Text']}\"\n",
    "        query_text = PROMPT.format(SubjectName=row['SubjectName'],ConstructName=row['ConstructName'],QuestionText=row['QuestionText'], CorrectAnswer=correct_answer, INcorrectAnswer=row[f'Answer{option}Text'])\n",
    "        query_text = preprocess_text(query_text)\n",
    "        row['query_text'] = get_detailed_instruct(task_description, query_text)\n",
    "\n",
    "        rows.append({\"query_text\": query_text, \n",
    "                     \"QuestionId_Answer\": f\"{row.QuestionId}_{option}\",\n",
    "                     \"ConstructName\": row.ConstructName,\n",
    "                     \"SubjectName\": row.SubjectName,\n",
    "                     \"QuestionText\": row.QuestionText,\n",
    "                     \"correct_answer\": correct_answer,\n",
    "                     \"incorrect_answer\": row[f\"Answer{option}Text\"],\n",
    "                     \"QuestionId\": row['QuestionId'],\n",
    "                     })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df['order_index'] = list(range(len(df)))\n",
    "\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    ")\n",
    "\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "# def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "#     return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 16\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_4bit_quant_type=\"nf4\",\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "model = AutoModel.from_pretrained(model_path,\n",
    "                                  quantization_config=bnb_config, \n",
    "                                  device_map=\"auto\",\n",
    "                                  )\n",
    "\n",
    "\n",
    "# model = get_peft_model(model, config)\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n",
    "\n",
    "model = model.eval()\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "\n",
    "\n",
    "task_description = 'Given a math question and a misconcepte incorrect answer, please retrieve the most accurate reason for the misconception.'\n",
    "\n",
    "\n",
    "V_answer = inference(df, model, tokenizer, device)\n",
    "\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "if full_df.shape[0]<10:\n",
    "    misconception_df = misconception_df.sample(n=40,random_state=2023)\n",
    "misconception_df[\"query_text\"] = misconception_df[\"MisconceptionName\"]\n",
    "\n",
    "V_misconception = inference(misconception_df, model, tokenizer, device)\n",
    "print(V_misconception.shape)\n",
    "\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "import sys, os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def get_matches(V_topic, V_content, n_neighbors=25):\n",
    "    \n",
    "    neighbors_model = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm=\"brute\", n_jobs=-1)\n",
    "    neighbors_model.fit(V_content)\n",
    "    dists, indices = neighbors_model.kneighbors(V_topic)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "indices = get_matches(V_answer, V_misconception, n_neighbors=33)\n",
    "\n",
    "np.save(\"indices.npy\", indices)\n",
    "df.to_parquet(\"df.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c48474b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:17:27.539814Z",
     "iopub.status.busy": "2024-12-09T16:17:27.539105Z",
     "iopub.status.idle": "2024-12-09T16:20:15.019819Z",
     "shell.execute_reply": "2024-12-09T16:20:15.018606Z"
    },
    "papermill": {
     "duration": 167.491726,
     "end_time": "2024-12-09T16:20:15.022249",
     "exception": false,
     "start_time": "2024-12-09T16:17:27.530523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [02:14<00:00, 16.77s/it]\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:03<00:00,  3.17s/it]\r\n",
      "Batches: 100%|████████████████████████████████████| 3/3 [00:01<00:00,  1.52it/s]\r\n",
      "(40, 5120)\r\n"
     ]
    }
   ],
   "source": [
    "!python run_recall.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c296c913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:20:15.042416Z",
     "iopub.status.busy": "2024-12-09T16:20:15.042127Z",
     "iopub.status.idle": "2024-12-09T16:20:15.051069Z",
     "shell.execute_reply": "2024-12-09T16:20:15.050306Z"
    },
    "papermill": {
     "duration": 0.02108,
     "end_time": "2024-12-09T16:20:15.052660",
     "exception": false,
     "start_time": "2024-12-09T16:20:15.031580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_reranker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_reranker.py\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    ")\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rerank_model_path = \"/kaggle/input/qwenqwen2.5-7b-instruct/pytorch/qwen2.5-14b-instruct/1\"\n",
    "rerank_lora_name_or_path = \"/kaggle/input/rerank-v3-round2/rerank-v3-round2-checkpoint-654\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "tokenizer_0 = AutoTokenizer.from_pretrained(rerank_lora_name_or_path)\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(rerank_lora_name_or_path)\n",
    "tokenizer_0.bos_token_id = 151643\n",
    "tokenizer_1.bos_token_id = 151643\n",
    "\n",
    "device_0 = torch.device('cuda:0')  \n",
    "model_0 = AutoModelForCausalLM.from_pretrained(rerank_model_path, device_map=device_0,use_cache=False,quantization_config=bnb_config,)\n",
    "model_0 = PeftModel.from_pretrained(model_0, rerank_lora_name_or_path)\n",
    "# model_0 = model_0.merge_and_unload()\n",
    "\n",
    "device_1 = torch.device('cuda:1')  \n",
    "model_1 = AutoModelForCausalLM.from_pretrained(rerank_model_path, device_map=device_1, use_cache=False,quantization_config=bnb_config,)\n",
    "model_1 = PeftModel.from_pretrained(model_1, rerank_lora_name_or_path)\n",
    "model_0 = model_0.eval()\n",
    "model_1 = model_1.eval()\n",
    "\n",
    "\n",
    "def last_logit_pool(logits: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return logits[:, -1, :]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = logits.shape[0]\n",
    "        return torch.stack([logits[i, sequence_lengths[i], :] for i in range(batch_size)], dim=0)\n",
    "\n",
    "def get_inputs(pairs, tokenizer, prompt=None, max_length=512):\n",
    "    if prompt is None:\n",
    "        prompt = \"Given a math question and a misconcepte incorrect answer,determine whether the misconception is pertinent to the query by providing a prediction of either 'Yes' or 'No'.\"\n",
    "    sep = \"\\n\"\n",
    "    prompt_inputs = tokenizer(prompt,\n",
    "                              return_tensors=None,\n",
    "                              add_special_tokens=False)['input_ids']\n",
    "    sep_inputs = tokenizer(sep,\n",
    "                           return_tensors=None,\n",
    "                           add_special_tokens=False)['input_ids']\n",
    "    inputs = []\n",
    "    for query, passage in pairs:\n",
    "        query = preprocess_text(query)\n",
    "        query_inputs = tokenizer(f'A: {query}',\n",
    "                                 return_tensors=None,\n",
    "                                 add_special_tokens=False,\n",
    "                                 max_length=max_length * 3 // 4,\n",
    "                                 truncation=True)\n",
    "        passage_inputs = tokenizer(f'B: {passage}',\n",
    "                                   return_tensors=None,\n",
    "                                   add_special_tokens=False,\n",
    "                                   max_length=max_length // 4,\n",
    "                                   truncation=True)\n",
    "        item = tokenizer.prepare_for_model(\n",
    "            query_inputs['input_ids'],\n",
    "            sep_inputs + passage_inputs['input_ids'],\n",
    "            truncation='only_second',\n",
    "            max_length=max_length,\n",
    "            padding=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n",
    "        item['attention_mask'] = [1] * len(item['input_ids'])\n",
    "        inputs.append(item)\n",
    "\n",
    "    return tokenizer.pad(\n",
    "            inputs,\n",
    "            padding=True,\n",
    "            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n",
    "            pad_to_multiple_of=8,\n",
    "            return_tensors='pt',\n",
    "    )\n",
    "\n",
    "import re\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference_reranker(df, model, tokenizer, device, batch_size=4):\n",
    "    all_scores = []\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(rerank_lora_name_or_path)\n",
    "    inputs = get_inputs(list(df['predict_text'].values), tokenizer)\n",
    "    yes_loc = tokenizer('Yes', add_special_tokens=False)['input_ids'][0]\n",
    "    for i in trange(0, len(inputs['input_ids']), batch_size, desc=\"Batches\", disable=False):\n",
    "        batch_inputs = inputs[i:i+batch_size]\n",
    "        batch_inputs = batch_to_device(batch_inputs, device)\n",
    "        outputs = model(**batch_inputs, output_hidden_states=True)\n",
    "        logits = outputs.logits\n",
    "        scores = last_logit_pool(logits, batch_inputs['attention_mask'])\n",
    "        scores = scores[:, yes_loc]\n",
    "        all_scores.extend(scores.cpu().float().tolist())\n",
    "\n",
    "    df['scores'] = all_scores\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "\n",
    "train_df = pd.read_parquet(\"df.parquet\")\n",
    "indices = np.load(\"indices.npy\")\n",
    "\n",
    "train_df['recall_ids'] = indices.tolist()\n",
    "\n",
    "misconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\n",
    "\n",
    "misconception_mapping['query_text'] = misconception_mapping['MisconceptionName']\n",
    "misconception_mapping['order_index'] = misconception_mapping['MisconceptionId']\n",
    "\n",
    "misconception_mapping_dict = {}\n",
    "for _, row in misconception_mapping.iterrows():\n",
    "    misconception_mapping_dict[row['order_index']] = row['query_text']\n",
    "\n",
    "TOP_NUM = 25\n",
    "df_rerank = []\n",
    "for _,row in train_df.iterrows():\n",
    "    for mis_id in row['recall_ids'][:TOP_NUM]:\n",
    "        text = misconception_mapping_dict[mis_id]\n",
    "        df_rerank.append({\n",
    "            \"order_index\": row['order_index'],\n",
    "            'predict_text': [row['query_text'], text],\n",
    "            'question_id': row['QuestionId'],\n",
    "            'mis_id': mis_id\n",
    "        })\n",
    "df_rerank = pd.DataFrame(df_rerank)\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "df_rerank['predict_text_len'] =  df_rerank['predict_text'].apply(lambda x: len(\"\".join(x)))\n",
    "df_rerank = df_rerank.sort_values(\"predict_text_len\", ascending=False)\n",
    "\n",
    "df_rerank_1 = df_rerank.iloc[0::2].copy()\n",
    "df_rerank_2 = df_rerank.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    rerank_results = executor.map(inference_reranker, (df_rerank_1, df_rerank_2), (model_0, model_1), (tokenizer_0, tokenizer_1), (device_0, device_1))\n",
    "df_rerank = pd.concat(list(rerank_results), axis=0)\n",
    "\n",
    "rerank_predicts_test = []\n",
    "for _, df_g in df_rerank.groupby(\"order_index\"):\n",
    "    scores = df_g['scores'].values\n",
    "    score_indexs = np.argsort(-scores)\n",
    "    mis_ids = df_g['mis_id'].values\n",
    "    rerank_predicts_test.append([mis_ids[index] for index in score_indexs[:TOP_NUM]])\n",
    "train_df['rerank_ids'] = rerank_predicts_test\n",
    "\n",
    "final_predict = []\n",
    "for i in range(len(predicts_test)):\n",
    "    temp = rerank_predicts_test[i] + predicts_test[i][TOP_NUM:]\n",
    "    final_predict.append(temp)\n",
    "\n",
    "train_df['rerank_ids'] = final_predict\n",
    "\n",
    "indices = np.array(final_predict)\n",
    "np.save(\"indices.npy\", indices)\n",
    "train_df.to_parquet(\"df.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a2f68f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:20:15.071108Z",
     "iopub.status.busy": "2024-12-09T16:20:15.070599Z",
     "iopub.status.idle": "2024-12-09T16:20:15.074007Z",
     "shell.execute_reply": "2024-12-09T16:20:15.073340Z"
    },
    "papermill": {
     "duration": 0.014214,
     "end_time": "2024-12-09T16:20:15.075578",
     "exception": false,
     "start_time": "2024-12-09T16:20:15.061364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python run_reranker.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f16d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:20:15.093933Z",
     "iopub.status.busy": "2024-12-09T16:20:15.093698Z",
     "iopub.status.idle": "2024-12-09T16:20:25.102702Z",
     "shell.execute_reply": "2024-12-09T16:20:25.102033Z"
    },
    "papermill": {
     "duration": 10.020111,
     "end_time": "2024-12-09T16:20:25.104654",
     "exception": false,
     "start_time": "2024-12-09T16:20:15.084543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afea69ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:20:25.123471Z",
     "iopub.status.busy": "2024-12-09T16:20:25.123220Z",
     "iopub.status.idle": "2024-12-09T16:20:25.130734Z",
     "shell.execute_reply": "2024-12-09T16:20:25.129864Z"
    },
    "papermill": {
     "duration": 0.018694,
     "end_time": "2024-12-09T16:20:25.132232",
     "exception": false,
     "start_time": "2024-12-09T16:20:25.113538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm.py\n",
    "\n",
    "import vllm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from typing import List\n",
    "import torch\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "model_path = \"/kaggle/input/72b-gptq-cpt400/pytorch/default/1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "PROMPT  = \"\"\"\n",
    "You are an expert math tutor who knows about all grade-school level math misconceptions.Your task is to diagnose the accurate type of misconceptions your student\n",
    "have based on the (incorrect) answer he/she gives.Your selected misconception type should correspond to the given question and incorrect answer.\n",
    "\n",
    "ConstructName: {ConstructName}({SubjectName}).\n",
    "Question: {Question}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "\n",
    "Pick the misconception number from the below:\n",
    "{Retrival}\n",
    "\"\"\"\n",
    "# just directly give your answers.\n",
    "\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": preprocess_text(\n",
    "                PROMPT.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[f\"incorrect_answer\"],\n",
    "                    CorrectAnswer=row[f\"correct_answer\"],\n",
    "                    Retrival=row[f\"retrieval\"]\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "\n",
    "df = pd.read_parquet(\"df.parquet\")\n",
    "indices = np.load(\"indices.npy\")\n",
    "\n",
    "# model_path = \"/kaggle/input/72b-gptq-cpt400/pytorch/default/1\"\n",
    "\n",
    "# llm = vllm.LLM(\n",
    "#     model_path,\n",
    "#     quantization=\"gptq\",\n",
    "#     tensor_parallel_size=2,\n",
    "#     gpu_memory_utilization=0.90, \n",
    "#     trust_remote_code=True,\n",
    "#     dtype=\"half\", \n",
    "#     enforce_eager=True,\n",
    "#     max_model_len=5120,\n",
    "#     disable_log_stats=True\n",
    "# )\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization=\"gptq\",\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.98, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=1758,\n",
    "    disable_log_stats=True,\n",
    "    cpu_offload_gb=8,\n",
    "    swap_space=1,\n",
    "    device='cuda',\n",
    "    max_num_seqs=18\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "choices = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "KEEP = []\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x,add_special_tokens=False)[0]\n",
    "    KEEP.append(c)\n",
    "\n",
    "def get_candidates(c_indices):\n",
    "    candidates = []\n",
    "\n",
    "    mis_names = misconception_df[\"MisconceptionName\"].values\n",
    "    for ix in c_indices:\n",
    "        c_names = []\n",
    "        for i, name in enumerate(mis_names[ix]):\n",
    "            c_names.append(f\"{i+1}. {name}\")\n",
    "\n",
    "        candidates.append(\"\\n\".join(c_names))\n",
    "        \n",
    "    return candidates\n",
    "\n",
    "\n",
    "\n",
    "def get_logits_scores(responses):\n",
    "    results = []\n",
    "    errors = 0\n",
    "    for i,response in enumerate(responses):\n",
    "        try:\n",
    "            x = response.outputs[0].logprobs[0]\n",
    "            logprobs = []\n",
    "            for k in KEEP:\n",
    "                if k in x:\n",
    "                    logprobs.append( math.exp(x[k].logprob) )\n",
    "                else:\n",
    "                    logprobs.append( 0 )\n",
    "                    print(f\"bad logits {i}\")\n",
    "            logprobs = np.array( logprobs )\n",
    "            logprobs /= logprobs.sum()\n",
    "            results.append( logprobs )\n",
    "        except:\n",
    "            #print(f\"error {i}\")\n",
    "            results.append( np.array(list(range(9,0,-1))) )\n",
    "            errors += 1\n",
    "            \n",
    "    print(f\"There were {errors} inference errors out of {i+1} inferences\")\n",
    "    results = np.vstack(results)\n",
    "    results = np.argsort(-results)\n",
    "    return results\n",
    "\n",
    "\n",
    "survivors = indices[:, -1:]\n",
    "\n",
    "for i in range(4):\n",
    "    c_indices = np.concatenate([indices[:, -8*(i+1)-1:-8*i-1], survivors], axis=1)\n",
    "    \n",
    "    df[\"retrieval\"] = get_candidates(c_indices)\n",
    "    df[\"text\"] = df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n",
    "    \n",
    "    # print(\"Example:\")\n",
    "    # print(df[\"text\"].values[0])\n",
    "    # print()\n",
    "    \n",
    "    responses = llm.generate(\n",
    "        df[\"text\"].values,\n",
    "        vllm.SamplingParams(\n",
    "            n=1,  # Number of output sequences to return for each prompt.\n",
    "            top_k=1,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "            temperature=0,  # randomness of the sampling\n",
    "            seed=777, # Seed for reprodicibility\n",
    "            skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "            max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "            logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])],\n",
    "            logprobs = 13\n",
    "        ),\n",
    "        use_tqdm=True\n",
    "    )\n",
    "\n",
    "    # responses = [x.outputs[0].text for x in responses]\n",
    "    # df[\"response\"] = responses\n",
    "    # llm_choices = df[\"response\"].astype(int).values - 1\n",
    "    # survivors = np.array([cix[best] for best, cix in zip(llm_choices, c_indices)]).reshape(-1, 1)\n",
    "\n",
    "    sub_scores = get_logits_scores(responses)\n",
    "    sub_indices = []\n",
    "    for idx, x in enumerate(c_indices):\n",
    "        scores_idx = sub_scores[idx]\n",
    "        temp_list = []\n",
    "        for s in scores_idx:\n",
    "            temp_list.append(x[s])\n",
    "        sub_indices.append(temp_list)\n",
    "    \n",
    "    sub_indices = np.array(sub_indices)\n",
    "    \n",
    "    indices[:, -8*(i+1)-1:-8*i-1] = sub_indices[:,1:]\n",
    "    \n",
    "    llm_choices = sub_indices[:, 0].tolist()\n",
    "    survivors = sub_indices[:, 0].reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(indices.shape[0]):\n",
    "    ix = indices[i][:25]\n",
    "    llm_choice = survivors[i, 0]\n",
    "    \n",
    "    results.append(\" \".join([str(llm_choice)] + [str(x) for x in ix if x != llm_choice]))\n",
    "\n",
    "\n",
    "df[\"MisconceptionId\"] = results\n",
    "df.to_csv(\"submission.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94cea443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:20:25.150450Z",
     "iopub.status.busy": "2024-12-09T16:20:25.149709Z",
     "iopub.status.idle": "2024-12-09T16:27:06.215398Z",
     "shell.execute_reply": "2024-12-09T16:27:06.214344Z"
    },
    "papermill": {
     "duration": 401.077017,
     "end_time": "2024-12-09T16:27:06.217728",
     "exception": false,
     "start_time": "2024-12-09T16:20:25.140711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-09 16:20:28 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\n",
      "WARNING 12-09 16:20:37 config.py:1668] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-09 16:20:46 config.py:321] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-09 16:20:46 config.py:905] Defaulting to use mp for distributed inference\r\n",
      "WARNING 12-09 16:20:46 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 12-09 16:20:46 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/72b-gptq-cpt400/pytorch/default/1', speculative_config=None, tokenizer='/kaggle/input/72b-gptq-cpt400/pytorch/default/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1758, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/72b-gptq-cpt400/pytorch/default/1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\r\n",
      "WARNING 12-09 16:20:47 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 12-09 16:20:47 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "INFO 12-09 16:20:47 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m INFO 12-09 16:20:47 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m INFO 12-09 16:20:47 selector.py:115] Using XFormers backend.\r\n",
      "INFO 12-09 16:20:47 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m INFO 12-09 16:20:48 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "INFO 12-09 16:20:49 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m INFO 12-09 16:20:49 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-09 16:20:49 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m INFO 12-09 16:20:49 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-09 16:20:49 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-09 16:21:08 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m INFO 12-09 16:21:08 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-09 16:21:08 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x79492863a080>, local_subscribe_port=42347, remote_subscribe_port=None)\r\n",
      "INFO 12-09 16:21:08 model_runner.py:1056] Starting to load model /kaggle/input/72b-gptq-cpt400/pytorch/default/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m INFO 12-09 16:21:08 model_runner.py:1056] Starting to load model /kaggle/input/72b-gptq-cpt400/pytorch/default/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m INFO 12-09 16:21:08 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-09 16:21:08 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m INFO 12-09 16:21:08 selector.py:115] Using XFormers backend.\r\n",
      "INFO 12-09 16:21:08 selector.py:115] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:29<03:53, 29.17s/it]\r\n",
      "Loading safetensors checkpoint shards:  22% Completed | 2/9 [01:03<03:44, 32.10s/it]\r\n",
      "Loading safetensors checkpoint shards:  33% Completed | 3/9 [01:28<02:53, 29.00s/it]\r\n",
      "Loading safetensors checkpoint shards:  44% Completed | 4/9 [02:01<02:32, 30.58s/it]\r\n",
      "Loading safetensors checkpoint shards:  56% Completed | 5/9 [02:14<01:37, 24.27s/it]\r\n",
      "Loading safetensors checkpoint shards:  67% Completed | 6/9 [02:45<01:19, 26.44s/it]\r\n",
      "Loading safetensors checkpoint shards:  78% Completed | 7/9 [03:15<00:55, 27.64s/it]\r\n",
      "Loading safetensors checkpoint shards:  89% Completed | 8/9 [03:47<00:29, 29.00s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [04:18<00:00, 29.76s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [04:18<00:00, 28.76s/it]\r\n",
      "\r\n",
      "INFO 12-09 16:25:41 model_runner.py:1067] Loading model weights took 11.9199 GB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=136)\u001b[0;0m INFO 12-09 16:25:41 model_runner.py:1067] Loading model weights took 11.9199 GB\r\n",
      "INFO 12-09 16:25:51 distributed_gpu_executor.py:57] # GPU blocks: 110, # CPU blocks: 409\r\n",
      "INFO 12-09 16:25:51 distributed_gpu_executor.py:61] Maximum concurrency for 1758 tokens per request: 1.00x\r\n",
      "Processed prompts: 100%|█| 9/9 [00:15<00:00,  1.69s/it, est. speed input: 191.66\r\n",
      "There were 0 inference errors out of 9 inferences\r\n",
      "Processed prompts: 100%|█| 9/9 [00:15<00:00,  1.68s/it, est. speed input: 201.38\r\n",
      "There were 0 inference errors out of 9 inferences\r\n",
      "Processed prompts: 100%|█| 9/9 [00:14<00:00,  1.66s/it, est. speed input: 200.78\r\n",
      "There were 0 inference errors out of 9 inferences\r\n",
      "Processed prompts: 100%|█| 9/9 [00:14<00:00,  1.63s/it, est. speed input: 208.90\r\n",
      "There were 0 inference errors out of 9 inferences\r\n",
      "[rank0]:[W1209 16:27:00.271801031 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\r\n"
     ]
    }
   ],
   "source": [
    "!python run_vllm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a662173",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T16:27:06.242011Z",
     "iopub.status.busy": "2024-12-09T16:27:06.241684Z",
     "iopub.status.idle": "2024-12-09T16:27:07.032612Z",
     "shell.execute_reply": "2024-12-09T16:27:07.031711Z"
    },
    "papermill": {
     "duration": 0.805009,
     "end_time": "2024-12-09T16:27:07.034506",
     "exception": false,
     "start_time": "2024-12-09T16:27:06.229497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>15 38 10 22 27 36 16 29 35 30 19 18 26 17 31 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>14 38 23 22 7 31 35 18 16 19 29 27 12 36 10 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>15 31 23 38 22 18 35 36 10 4 16 29 21 30 26 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>3 0 38 24 36 37 13 25 21 15 32 16 30 23 5 31 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>3 31 13 38 37 25 36 0 22 7 20 23 35 32 5 21 16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  15 38 10 22 27 36 16 29 35 30 19 18 26 17 31 0...\n",
       "1            1869_C  14 38 23 22 7 31 35 18 16 19 29 27 12 36 10 26...\n",
       "2            1869_D  15 31 23 38 22 18 35 36 10 4 16 29 21 30 26 27...\n",
       "3            1870_A  3 0 38 24 36 37 13 25 21 15 32 16 30 23 5 31 2...\n",
       "4            1870_B  3 31 13 38 37 25 36 0 22 7 20 23 35 32 5 21 16..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sub = pd.read_csv(\"submission.csv\")\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8c09a",
   "metadata": {
    "papermill": {
     "duration": 0.01059,
     "end_time": "2024-12-09T16:27:07.055794",
     "exception": false,
     "start_time": "2024-12-09T16:27:07.045204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f61fcc",
   "metadata": {
    "papermill": {
     "duration": 0.010208,
     "end_time": "2024-12-09T16:27:07.076152",
     "exception": false,
     "start_time": "2024-12-09T16:27:07.065944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5251603,
     "sourceId": 9094368,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5920031,
     "sourceId": 9688062,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5957531,
     "sourceId": 9734430,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6116301,
     "sourceId": 9946571,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6117312,
     "sourceId": 9948011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6182744,
     "sourceId": 10037500,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10130031,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6260807,
     "sourceId": 10143305,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 202903897,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 210974141,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 211563216,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 162278,
     "modelInstanceId": 141296,
     "sourceId": 166060,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 186609,
     "modelInstanceId": 164293,
     "sourceId": 192671,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 770.985732,
   "end_time": "2024-12-09T16:27:07.421813",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-09T16:14:16.436081",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
