{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081366e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:29:57.684507Z",
     "iopub.status.busy": "2024-12-12T03:29:57.683825Z",
     "iopub.status.idle": "2024-12-12T03:29:57.688080Z",
     "shell.execute_reply": "2024-12-12T03:29:57.687245Z"
    },
    "papermill": {
     "duration": 0.016169,
     "end_time": "2024-12-12T03:29:57.689700",
     "exception": false,
     "start_time": "2024-12-12T03:29:57.673531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !md5sum /kaggle/input/qw14b-awq/transformers/default/1/model-00001-of-00002.safetensors\n",
    "# should be 25596de367acaec20e616b7c87bd5529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d3092e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:29:57.707618Z",
     "iopub.status.busy": "2024-12-12T03:29:57.707369Z",
     "iopub.status.idle": "2024-12-12T03:29:57.713505Z",
     "shell.execute_reply": "2024-12-12T03:29:57.712768Z"
    },
    "papermill": {
     "duration": 0.01636,
     "end_time": "2024-12-12T03:29:57.714987",
     "exception": false,
     "start_time": "2024-12-12T03:29:57.698627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "cutoff_time = int(time.time() + (8 * 60 + 50) * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d3a59ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:29:57.731314Z",
     "iopub.status.busy": "2024-12-12T03:29:57.731049Z",
     "iopub.status.idle": "2024-12-12T03:29:57.736232Z",
     "shell.execute_reply": "2024-12-12T03:29:57.735507Z"
    },
    "papermill": {
     "duration": 0.015213,
     "end_time": "2024-12-12T03:29:57.737910",
     "exception": false,
     "start_time": "2024-12-12T03:29:57.722697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1734005997"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoff_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d111e8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:29:57.754396Z",
     "iopub.status.busy": "2024-12-12T03:29:57.754153Z",
     "iopub.status.idle": "2024-12-12T03:30:14.901712Z",
     "shell.execute_reply": "2024-12-12T03:30:14.901040Z"
    },
    "papermill": {
     "duration": 17.158042,
     "end_time": "2024-12-12T03:30:14.903762",
     "exception": false,
     "start_time": "2024-12-12T03:29:57.745720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-12 03:29:59 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 03:30:14,097\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import vllm\n",
    "except:\n",
    "    !pip install torchvision==0.19.1\n",
    "    !pip install vllm==0.6.3.post1\n",
    "    !pip install optimum==1.23.3 autoawq==0.2.7.post2 auto-gptq==0.7.1  bitsandbytes==0.44.1 peft==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000264f3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-12T03:30:14.921851Z",
     "iopub.status.busy": "2024-12-12T03:30:14.921310Z",
     "iopub.status.idle": "2024-12-12T03:30:14.925741Z",
     "shell.execute_reply": "2024-12-12T03:30:14.925021Z"
    },
    "papermill": {
     "duration": 0.014841,
     "end_time": "2024-12-12T03:30:14.927238",
     "exception": false,
     "start_time": "2024-12-12T03:30:14.912397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, math, numpy as np\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re, gc\n",
    "import torch\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158aaef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:30:14.944830Z",
     "iopub.status.busy": "2024-12-12T03:30:14.944185Z",
     "iopub.status.idle": "2024-12-12T03:30:15.016819Z",
     "shell.execute_reply": "2024-12-12T03:30:15.016019Z"
    },
    "papermill": {
     "duration": 0.083244,
     "end_time": "2024-12-12T03:30:15.018479",
     "exception": false,
     "start_time": "2024-12-12T03:30:14.935235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SUBMISSION: False\n"
     ]
    }
   ],
   "source": [
    "IS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "\n",
    "print('IS_SUBMISSION:', IS_SUBMISSION)\n",
    "\n",
    "df_train = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").fillna(-1).sample(10, random_state=42).reset_index(drop=True)\n",
    "df_test = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "df_misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "\n",
    "if not IS_SUBMISSION:\n",
    "    df_misconception_mapping = df_misconception_mapping[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77126fca",
   "metadata": {
    "papermill": {
     "duration": 0.00765,
     "end_time": "2024-12-12T03:30:15.034513",
     "exception": false,
     "start_time": "2024-12-12T03:30:15.026863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# first retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5f0c82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:30:15.051511Z",
     "iopub.status.busy": "2024-12-12T03:30:15.050888Z",
     "iopub.status.idle": "2024-12-12T03:30:15.054719Z",
     "shell.execute_reply": "2024-12-12T03:30:15.054109Z"
    },
    "papermill": {
     "duration": 0.013911,
     "end_time": "2024-12-12T03:30:15.056159",
     "exception": false,
     "start_time": "2024-12-12T03:30:15.042248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "if not IS_SUBMISSION:\n",
    "    df_ret = df_test.copy()\n",
    "else:\n",
    "    df_ret = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697fb0ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:30:15.072983Z",
     "iopub.status.busy": "2024-12-12T03:30:15.072762Z",
     "iopub.status.idle": "2024-12-12T03:30:15.087380Z",
     "shell.execute_reply": "2024-12-12T03:30:15.086630Z"
    },
    "papermill": {
     "duration": 0.024809,
     "end_time": "2024-12-12T03:30:15.088880",
     "exception": false,
     "start_time": "2024-12-12T03:30:15.064071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>ConstructId</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectId</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>CorrectAnswer</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>AnswerAText</th>\n",
       "      <th>AnswerBText</th>\n",
       "      <th>AnswerCText</th>\n",
       "      <th>AnswerDText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>D</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>Does not simplify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>B</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nTom</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Both Tom and Katie</td>\n",
       "      <td>Neither is correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QuestionId  ConstructId                                      ConstructName  \\\n",
       "0        1869          856  Use the order of operations to carry out calcu...   \n",
       "1        1870         1612  Simplify an algebraic fraction by factorising ...   \n",
       "2        1871         2774            Calculate the range from a list of data   \n",
       "\n",
       "   SubjectId                                        SubjectName CorrectAnswer  \\\n",
       "0         33                                             BIDMAS             A   \n",
       "1       1077                    Simplifying Algebraic Fractions             D   \n",
       "2        339  Range and Interquartile Range from a List of Data             B   \n",
       "\n",
       "                                        QuestionText            AnswerAText  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n",
       "2  Tom and Katie are discussing the \\( 5 \\) plant...              Only\\nTom   \n",
       "\n",
       "              AnswerBText            AnswerCText             AnswerDText  \n",
       "0  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  Does not need brackets  \n",
       "1               \\( m+2 \\)              \\( m-1 \\)       Does not simplify  \n",
       "2             Only\\nKatie     Both Tom and Katie      Neither is correct  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ac328f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:30:15.106305Z",
     "iopub.status.busy": "2024-12-12T03:30:15.105844Z",
     "iopub.status.idle": "2024-12-12T03:30:15.115625Z",
     "shell.execute_reply": "2024-12-12T03:30:15.114909Z"
    },
    "papermill": {
     "duration": 0.019945,
     "end_time": "2024-12-12T03:30:15.117120",
     "exception": false,
     "start_time": "2024-12-12T03:30:15.097175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMPLATE_INPUT_V3 = '{QUESTION}\\nCorrect answer: {CORRECT_ANSWER}\\nStudent wrong answer: {STUDENT_WRONG_ANSWER}'\n",
    "def format_input_v3(row, wrong_choice):\n",
    "\n",
    "    assert wrong_choice in \"ABCD\"\n",
    "    # Extract values from the row\n",
    "    question_text = row.get(\"QuestionText\", \"No question text provided\")\n",
    "    subject_name = row.get(\"SubjectName\", \"Unknown subject\")\n",
    "    construct_name = row.get(\"ConstructName\", \"Unknown construct\")\n",
    "    # Extract the correct and wrong answer text based on the choice\n",
    "    correct_answer = row.get(\"CorrectAnswer\", \"Unknown\")\n",
    "    assert wrong_choice != correct_answer\n",
    "    correct_answer_text = row.get(f\"Answer{correct_answer}Text\", \"No correct answer text available\")\n",
    "    wrong_answer_text = row.get(f\"Answer{wrong_choice}Text\", \"No wrong answer text available\")\n",
    "\n",
    "    # Construct the question format\n",
    "    formatted_question = f\"\"\"Question: {question_text}\n",
    "    \n",
    "SubjectName: {subject_name}\n",
    "ConstructName: {construct_name}\"\"\"\n",
    "\n",
    "    # Return the extracted data\n",
    "    ret = {\n",
    "        \"QUESTION\": formatted_question,\n",
    "        \"CORRECT_ANSWER\": correct_answer_text,\n",
    "        \"STUDENT_WRONG_ANSWER\": wrong_answer_text,\n",
    "        \"MISCONCEPTION_ID\": row.get('Misconception{wrong_choice}Id'),\n",
    "\n",
    "        'ConstructName': row['ConstructName'],\n",
    "        'SubjectName': row['SubjectName'],\n",
    "        'QuestionText': row['QuestionText'],\n",
    "        'CorrectAnswerText': correct_answer_text,\n",
    "        'IncorrectAnswerText': wrong_answer_text,\n",
    "    }\n",
    "    ret[\"PROMPT\"] = TEMPLATE_INPUT_V3.format(**ret)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "items = []\n",
    "target_ids = []\n",
    "for _, row in df_ret.iterrows():\n",
    "    for choice in ['A', 'B', 'C', 'D']:\n",
    "        if choice == row[\"CorrectAnswer\"]:\n",
    "            continue\n",
    "        # if not IS_SUBMISSION and row[f'Misconception{choice}Id'] == -1:\n",
    "        #     continue\n",
    "            \n",
    "        correct_col = f\"Answer{row['CorrectAnswer']}Text\"\n",
    "        item = {'QuestionId_Answer': '{}_{}'.format(row['QuestionId'], choice)}\n",
    "        item['Prompt'] = format_input_v3(row, choice)['PROMPT']\n",
    "\n",
    "        item['ConstructName'] = row['ConstructName']\n",
    "        item['SubjectName'] = row['SubjectName']\n",
    "        item['QuestionText'] = row['QuestionText']\n",
    "        correct_answer = row[\"CorrectAnswer\"]\n",
    "        item['CorrectAnswerText'] = row[f'Answer{correct_answer}Text']\n",
    "        item['IncorrectAnswerText'] = row[f'Answer{choice}Text']\n",
    "        \n",
    "        items.append(item)\n",
    "        target_ids.append(int(row.get(f'Misconception{choice}Id', -1)))\n",
    "        \n",
    "df_input = pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ef7653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:30:15.134264Z",
     "iopub.status.busy": "2024-12-12T03:30:15.133816Z",
     "iopub.status.idle": "2024-12-12T03:30:15.685834Z",
     "shell.execute_reply": "2024-12-12T03:30:15.685160Z"
    },
    "papermill": {
     "duration": 0.562661,
     "end_time": "2024-12-12T03:30:15.687827",
     "exception": false,
     "start_time": "2024-12-12T03:30:15.125166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}'\n",
    "\n",
    "def get_detailed_example(task_description: str, query: str, response: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}\\n<response>{response}'\n",
    "\n",
    "def get_new_queries(queries, query_max_len, examples_prefix, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        queries,\n",
    "        max_length=query_max_len - len(tokenizer('<s>', add_special_tokens=False)['input_ids']) - len(\n",
    "            tokenizer('\\n<response></s>', add_special_tokens=False)['input_ids']),\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    prefix_ids = tokenizer(examples_prefix, add_special_tokens=False)['input_ids']\n",
    "    suffix_ids = tokenizer('\\n<response>', add_special_tokens=False)['input_ids']\n",
    "    new_max_length = (len(prefix_ids) + len(suffix_ids) + query_max_len + 8) // 8 * 8 + 8\n",
    "    new_queries = tokenizer.batch_decode(inputs['input_ids'])\n",
    "    for i in range(len(new_queries)):\n",
    "        new_queries[i] = examples_prefix + new_queries[i] + '\\n<response>'\n",
    "    return new_max_length, new_queries\n",
    "task =  \"Given a math multiple-choice problem with a student's wrong answer, retrieve the math misconceptions\"\n",
    "queries = [\n",
    "    get_detailed_instruct(task, q) for q in df_input['Prompt']\n",
    "]\n",
    "documents = df_misconception_mapping['MisconceptionName'].tolist()\n",
    "query_max_len, doc_max_len = 1024, 256\n",
    "LORA_PATH = '/kaggle/input/lora-14b-1126/transformers/default/1'\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/qw14b-awq/transformers/default/1')\n",
    "examples_prefix = ''\n",
    "new_query_max_len, new_queries = get_new_queries(queries, query_max_len, examples_prefix, tokenizer)\n",
    "\n",
    "\n",
    "import json\n",
    "with open('data.json', 'w') as f:\n",
    "    data = {'texts': new_queries+ documents}\n",
    "    f.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c28893f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:30:15.705992Z",
     "iopub.status.busy": "2024-12-12T03:30:15.705717Z",
     "iopub.status.idle": "2024-12-12T03:30:15.712216Z",
     "shell.execute_reply": "2024-12-12T03:30:15.711493Z"
    },
    "papermill": {
     "duration": 0.017412,
     "end_time": "2024-12-12T03:30:15.713754",
     "exception": false,
     "start_time": "2024-12-12T03:30:15.696342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_embed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_embed.py\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import peft\n",
    "\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[\n",
    "            torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_embeddings_in_batches(model, tokenizer, texts, max_length, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_dict = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(**batch_dict)\n",
    "            batch_embeddings = last_token_pool(\n",
    "                outputs.last_hidden_state, batch_dict[\"attention_mask\"]\n",
    "            )\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1).cpu()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(base_model_path, lora_path, load_in_4bit=True):\n",
    "    model = AutoModel.from_pretrained(\n",
    "        base_model_path,\n",
    "        device_map=0,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        lora_path if lora_path else base_model_path\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    if lora_path:\n",
    "        model = peft.PeftModel.from_pretrained(model, lora_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    output_file = args.input_text.replace(\n",
    "        \".json\", \".pt.fold.{}.{}.embed\".format(*args.fold)\n",
    "    )\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Output file {output_file} already exists. Skipping...\")\n",
    "        return\n",
    "    model, tokenizer = load_model_and_tokenizer(\n",
    "        args.base_model, args.lora_path, load_in_4bit=args.load_in_4bit\n",
    "    )\n",
    "    texts = json.load(open(args.input_text))[\"texts\"][args.fold[0] :: args.fold[1]]\n",
    "    embeddings = get_embeddings_in_batches(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        batch_size=4,\n",
    "    )\n",
    "    text2embeds = {text: emb for text, emb in zip(texts, embeddings)}\n",
    "    torch.save(text2embeds, output_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--base_model\",\n",
    "        type=str,\n",
    "        default=\"Qwen/Qwen2.5-7B\",\n",
    "        help=\"Path to the base model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to the LoRA model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_text\",\n",
    "        type=str,\n",
    "        default=\".cache/data.json\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load_in_4bit\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Load model in 4-bit mode\",\n",
    "    )\n",
    "    parser.add_argument(\"--fold\", nargs=2, type=int, default=[0, 1])\n",
    "    args = parser.parse_args()\n",
    "    if not os.path.exists(args.lora_path):\n",
    "        args.lora_path = None\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfec96c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:30:15.731124Z",
     "iopub.status.busy": "2024-12-12T03:30:15.730537Z",
     "iopub.status.idle": "2024-12-12T03:30:19.773955Z",
     "shell.execute_reply": "2024-12-12T03:30:19.772838Z"
    },
    "papermill": {
     "duration": 4.054542,
     "end_time": "2024-12-12T03:30:19.776261",
     "exception": false,
     "start_time": "2024-12-12T03:30:15.721719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!sleep 1 & sleep 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5bb31e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:30:19.797009Z",
     "iopub.status.busy": "2024-12-12T03:30:19.796666Z",
     "iopub.status.idle": "2024-12-12T03:33:16.015212Z",
     "shell.execute_reply": "2024-12-12T03:33:16.014354Z"
    },
    "papermill": {
     "duration": 176.231213,
     "end_time": "2024-12-12T03:33:16.018114",
     "exception": false,
     "start_time": "2024-12-12T03:30:19.786901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:01<00:00, 60.56s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:01<00:00, 60.74s/it]\n",
      "Embedding: 100%|██████████| 14/14 [00:14<00:00,  1.03s/it]\n",
      "Embedding:  71%|███████▏  | 10/14 [00:16<00:02,  1.49it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%writefile run.sh\n",
    "lora_path = '/kaggle/input/2211-lora-14b/transformers/default/1'\n",
    "cmd = f\"(CUDA_VISIBLE_DEVICES=0 python run_embed.py --base_model /kaggle/input/qw14b-awq/transformers/default/1 --lora_path {lora_path} --input_text data.json --fold 0 2) & (CUDA_VISIBLE_DEVICES=1 python run_embed.py --base_model /kaggle/input/qw14b-awq/transformers/default/1 --lora_path {lora_path} --input_text data.json --fold 1 2)\"\n",
    "import os\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e125247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:16.076780Z",
     "iopub.status.busy": "2024-12-12T03:33:16.076250Z",
     "iopub.status.idle": "2024-12-12T03:33:21.096462Z",
     "shell.execute_reply": "2024-12-12T03:33:21.095449Z"
    },
    "papermill": {
     "duration": 5.051186,
     "end_time": "2024-12-12T03:33:21.098434",
     "exception": false,
     "start_time": "2024-12-12T03:33:16.047248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 14/14 [00:18<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.pt.fold.1.2.embed\n",
      "data.pt.fold.0.2.embed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40/3571780413.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_to_embed.update(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import time\n",
    "text_to_embed = {}\n",
    "files = glob('*.pt*')\n",
    "while len(files) != 2:\n",
    "    time.sleep(1)\n",
    "    files = glob('*.pt*')\n",
    "\n",
    "time.sleep(3)    \n",
    "for path in files:\n",
    "    print(path)\n",
    "    text_to_embed.update(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90927cf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.119391Z",
     "iopub.status.busy": "2024-12-12T03:33:21.118846Z",
     "iopub.status.idle": "2024-12-12T03:33:21.123788Z",
     "shell.execute_reply": "2024-12-12T03:33:21.123056Z"
    },
    "papermill": {
     "duration": 0.01684,
     "end_time": "2024-12-12T03:33:21.125369",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.108529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_to_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2ca54eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.145571Z",
     "iopub.status.busy": "2024-12-12T03:33:21.145317Z",
     "iopub.status.idle": "2024-12-12T03:33:21.152466Z",
     "shell.execute_reply": "2024-12-12T03:33:21.151679Z"
    },
    "papermill": {
     "duration": 0.018948,
     "end_time": "2024-12-12T03:33:21.154010",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.135062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 5120]), torch.Size([100, 5120]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings = torch.stack([text_to_embed[t] for t in new_queries])\n",
    "doc_embeddings = torch.stack([text_to_embed[t] for t in documents])\n",
    "query_embeddings.shape, doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef0744de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.174415Z",
     "iopub.status.busy": "2024-12-12T03:33:21.174100Z",
     "iopub.status.idle": "2024-12-12T03:33:21.179078Z",
     "shell.execute_reply": "2024-12-12T03:33:21.178297Z"
    },
    "papermill": {
     "duration": 0.016969,
     "end_time": "2024-12-12T03:33:21.180674",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.163705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eedi_11scores = query_embeddings @ doc_embeddings.T  # Shape: (M, N)\n",
    "sorted_indices = torch.argsort(eedi_11scores,1, descending=True)[:,:35].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b4d5bfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.203423Z",
     "iopub.status.busy": "2024-12-12T03:33:21.202961Z",
     "iopub.status.idle": "2024-12-12T03:33:21.215956Z",
     "shell.execute_reply": "2024-12-12T03:33:21.215145Z"
    },
    "papermill": {
     "duration": 0.024748,
     "end_time": "2024-12-12T03:33:21.217483",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.192735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_input[\"MisconceptionId\"] = [\" \".join([str(x) for x in row]) for row in sorted_indices]\n",
    "df_input[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d31f49a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.237926Z",
     "iopub.status.busy": "2024-12-12T03:33:21.237683Z",
     "iopub.status.idle": "2024-12-12T03:33:21.247642Z",
     "shell.execute_reply": "2024-12-12T03:33:21.246909Z"
    },
    "papermill": {
     "duration": 0.021773,
     "end_time": "2024-12-12T03:33:21.249071",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.227298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>15 77 23 96 25 74 90 52 55 95 30 94 78 57 11 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>15 77 74 23 90 96 71 37 52 25 35 13 55 30 65 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>15 77 25 96 90 74 95 14 4 65 44 68 52 30 71 64...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>91 59 82 80 4 79 32 27 55 3 74 29 34 44 68 94 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>91 80 59 82 79 32 3 27 29 55 4 74 44 46 68 34 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>82 91 80 59 55 79 74 3 34 4 32 68 27 29 46 52 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>14 62 23 24 30 94 63 77 38 74 25 93 32 65 52 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>14 62 24 23 30 94 63 32 77 93 25 74 6 38 65 52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>23 14 63 94 24 74 77 89 65 62 32 30 42 18 29 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  15 77 23 96 25 74 90 52 55 95 30 94 78 57 11 1...\n",
       "1            1869_C  15 77 74 23 90 96 71 37 52 25 35 13 55 30 65 8...\n",
       "2            1869_D  15 77 25 96 90 74 95 14 4 65 44 68 52 30 71 64...\n",
       "3            1870_A  91 59 82 80 4 79 32 27 55 3 74 29 34 44 68 94 ...\n",
       "4            1870_B  91 80 59 82 79 32 3 27 29 55 4 74 44 46 68 34 ...\n",
       "5            1870_C  82 91 80 59 55 79 74 3 34 4 32 68 27 29 46 52 ...\n",
       "6            1871_A  14 62 23 24 30 94 63 77 38 74 25 93 32 65 52 6...\n",
       "7            1871_C  14 62 24 23 30 94 63 32 77 93 25 74 6 38 65 52...\n",
       "8            1871_D  23 14 63 94 24 74 77 89 65 62 32 30 42 18 29 2..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "712aca3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.269763Z",
     "iopub.status.busy": "2024-12-12T03:33:21.269547Z",
     "iopub.status.idle": "2024-12-12T03:33:21.274834Z",
     "shell.execute_reply": "2024-12-12T03:33:21.274105Z"
    },
    "papermill": {
     "duration": 0.017303,
     "end_time": "2024-12-12T03:33:21.276426",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.259123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stage2_data = []\n",
    "for i, row in df_input.iterrows():\n",
    "    qa_id = row['QuestionId_Answer']\n",
    "    top_mis_ids = sorted_indices[i]\n",
    "        \n",
    "    stage2_data.append({\n",
    "        'qa_id': qa_id,\n",
    "        'ConstructName': row['ConstructName'],\n",
    "        'SubjectName': row['SubjectName'],\n",
    "        'QuestionText': row['QuestionText'],\n",
    "        'CorrectAnswerText': row[f'CorrectAnswerText'],\n",
    "        'IncorrectAnswerText': row[f'IncorrectAnswerText'],\n",
    "        # 'gt_mis_id': int(gt_mis_id),\n",
    "        'top_mis_id': top_mis_ids,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b92e7ea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.297499Z",
     "iopub.status.busy": "2024-12-12T03:33:21.297002Z",
     "iopub.status.idle": "2024-12-12T03:33:21.302000Z",
     "shell.execute_reply": "2024-12-12T03:33:21.301256Z"
    },
    "papermill": {
     "duration": 0.01733,
     "end_time": "2024-12-12T03:33:21.303529",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.286199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa_id': '1869_B',\n",
       " 'ConstructName': 'Use the order of operations to carry out calculations involving powers',\n",
       " 'SubjectName': 'BIDMAS',\n",
       " 'QuestionText': '\\\\[\\n3 \\\\times 2+4-5\\n\\\\]\\nWhere do the brackets need to go to make the answer equal \\\\( 13 \\\\) ?',\n",
       " 'CorrectAnswerText': '\\\\( 3 \\\\times(2+4)-5 \\\\)',\n",
       " 'IncorrectAnswerText': '\\\\( 3 \\\\times 2+(4-5) \\\\)',\n",
       " 'top_mis_id': [15,\n",
       "  77,\n",
       "  23,\n",
       "  96,\n",
       "  25,\n",
       "  74,\n",
       "  90,\n",
       "  52,\n",
       "  55,\n",
       "  95,\n",
       "  30,\n",
       "  94,\n",
       "  78,\n",
       "  57,\n",
       "  11,\n",
       "  1,\n",
       "  68,\n",
       "  37,\n",
       "  34,\n",
       "  82,\n",
       "  60,\n",
       "  14,\n",
       "  64,\n",
       "  36,\n",
       "  4,\n",
       "  44,\n",
       "  38,\n",
       "  32,\n",
       "  31,\n",
       "  81,\n",
       "  19,\n",
       "  35,\n",
       "  58,\n",
       "  24,\n",
       "  71]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage2_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ef32982",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.324882Z",
     "iopub.status.busy": "2024-12-12T03:33:21.324671Z",
     "iopub.status.idle": "2024-12-12T03:33:21.329065Z",
     "shell.execute_reply": "2024-12-12T03:33:21.328320Z"
    },
    "papermill": {
     "duration": 0.016948,
     "end_time": "2024-12-12T03:33:21.330630",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.313682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('stage2_data.json', 'w') as file:\n",
    "    json.dump(stage2_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00495647",
   "metadata": {
    "papermill": {
     "duration": 0.01014,
     "end_time": "2024-12-12T03:33:21.350667",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.340527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# My Q14B Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc914776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.371253Z",
     "iopub.status.busy": "2024-12-12T03:33:21.371017Z",
     "iopub.status.idle": "2024-12-12T03:33:21.374738Z",
     "shell.execute_reply": "2024-12-12T03:33:21.373858Z"
    },
    "papermill": {
     "duration": 0.016017,
     "end_time": "2024-12-12T03:33:21.376531",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.360514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUERY_TEMPLATE = \"\"\"<subject>\n",
    "{SubjectName}\n",
    "</subject>\n",
    "\n",
    "<construct>\n",
    "{ConstructName}\n",
    "</construct>\n",
    "\n",
    "<question>\n",
    "{QuestionText}\n",
    "</question>\n",
    "\n",
    "<correct_answer>\n",
    "{CorrectAnswerText}\n",
    "</correct_answer>\n",
    "\n",
    "<incorrect_answer>\n",
    "{IncorrectAnswerText}\n",
    "</incorrect_answer>\"\"\"\n",
    "\n",
    "TASK_DESC = 'Given a math question with correct answer and a misconcepted incorrect answer, retrieve the most accurate misconception for the incorrect answer.'\n",
    "\n",
    "INST_TEMPLATE = 'Instruct: {instruct}\\n\\nQuery: {query}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43ae6d8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.397429Z",
     "iopub.status.busy": "2024-12-12T03:33:21.396763Z",
     "iopub.status.idle": "2024-12-12T03:33:21.427710Z",
     "shell.execute_reply": "2024-12-12T03:33:21.426988Z"
    },
    "papermill": {
     "duration": 0.043018,
     "end_time": "2024-12-12T03:33:21.429344",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.386326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>query_text</th>\n",
       "      <th>full_instruct</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>incorrect_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>&lt;subject&gt;\\nBIDMAS\\n&lt;/subject&gt;\\n\\n&lt;construct&gt;\\n...</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>&lt;subject&gt;\\nBIDMAS\\n&lt;/subject&gt;\\n\\n&lt;construct&gt;\\n...</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>&lt;subject&gt;\\nBIDMAS\\n&lt;/subject&gt;\\n\\n&lt;construct&gt;\\n...</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>&lt;subject&gt;\\nSimplifying Algebraic Fractions\\n&lt;/...</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>&lt;subject&gt;\\nSimplifying Algebraic Fractions\\n&lt;/...</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qa_id                                         query_text  \\\n",
       "0  1869_B  <subject>\\nBIDMAS\\n</subject>\\n\\n<construct>\\n...   \n",
       "1  1869_C  <subject>\\nBIDMAS\\n</subject>\\n\\n<construct>\\n...   \n",
       "2  1869_D  <subject>\\nBIDMAS\\n</subject>\\n\\n<construct>\\n...   \n",
       "3  1870_A  <subject>\\nSimplifying Algebraic Fractions\\n</...   \n",
       "4  1870_B  <subject>\\nSimplifying Algebraic Fractions\\n</...   \n",
       "\n",
       "                                       full_instruct  \\\n",
       "0  Instruct: Given a math question with correct a...   \n",
       "1  Instruct: Given a math question with correct a...   \n",
       "2  Instruct: Given a math question with correct a...   \n",
       "3  Instruct: Given a math question with correct a...   \n",
       "4  Instruct: Given a math question with correct a...   \n",
       "\n",
       "                                       ConstructName  \\\n",
       "0  Use the order of operations to carry out calcu...   \n",
       "1  Use the order of operations to carry out calcu...   \n",
       "2  Use the order of operations to carry out calcu...   \n",
       "3  Simplify an algebraic fraction by factorising ...   \n",
       "4  Simplify an algebraic fraction by factorising ...   \n",
       "\n",
       "                       SubjectName  \\\n",
       "0                           BIDMAS   \n",
       "1                           BIDMAS   \n",
       "2                           BIDMAS   \n",
       "3  Simplifying Algebraic Fractions   \n",
       "4  Simplifying Algebraic Fractions   \n",
       "\n",
       "                                        QuestionText         correct_answer  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "2  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "3  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "4  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "\n",
       "         incorrect_answer  \n",
       "0  \\( 3 \\times 2+(4-5) \\)  \n",
       "1   \\( 3 \\times(2+4-5) \\)  \n",
       "2  Does not need brackets  \n",
       "3               \\( m+1 \\)  \n",
       "4               \\( m+2 \\)  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "full_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "mis_map = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv')\n",
    "IS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "\n",
    "if not IS_SUBMISSION:\n",
    "    mis_map = mis_map[:100]\n",
    "\n",
    "rows = []\n",
    "for idx, row in full_df.iterrows():\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        if option == row.CorrectAnswer:\n",
    "            continue\n",
    "            \n",
    "        correct_answer = row[f\"Answer{row.CorrectAnswer}Text\"]\n",
    "        incorrect_answer = row[f\"Answer{option}Text\"]\n",
    "\n",
    "        query_text = QUERY_TEMPLATE.format(**{\n",
    "            'SubjectName': row['SubjectName'],\n",
    "            'ConstructName': row['ConstructName'],\n",
    "            'QuestionText': row['QuestionText'],\n",
    "            'CorrectAnswerText': correct_answer,\n",
    "            'IncorrectAnswerText': incorrect_answer,\n",
    "        })\n",
    "        full_instruct = row['query_text'] = INST_TEMPLATE.format(\n",
    "            instruct=TASK_DESC,\n",
    "            query=query_text,\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"qa_id\": f\"{row.QuestionId}_{option}\",\n",
    "            \"query_text\": query_text, \n",
    "            \"full_instruct\": full_instruct,\n",
    "            \"ConstructName\": row.ConstructName,\n",
    "            \"SubjectName\": row.SubjectName,\n",
    "            \"QuestionText\": row.QuestionText,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"incorrect_answer\": incorrect_answer,\n",
    "        })\n",
    "        \n",
    "df = pd.DataFrame(rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "463eb4eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.450724Z",
     "iopub.status.busy": "2024-12-12T03:33:21.450475Z",
     "iopub.status.idle": "2024-12-12T03:33:21.455722Z",
     "shell.execute_reply": "2024-12-12T03:33:21.454930Z"
    },
    "papermill": {
     "duration": 0.017847,
     "end_time": "2024-12-12T03:33:21.457382",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.439535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Question: 9\n",
      "Num Misconceptions: 100\n"
     ]
    }
   ],
   "source": [
    "question_texts = df['query_text'].values.tolist()\n",
    "mis_texts = mis_map['MisconceptionName'].values.tolist()\n",
    "\n",
    "print(f\"Num Question: {len(question_texts)}\")\n",
    "print(f\"Num Misconceptions: {len(mis_texts)}\")\n",
    "\n",
    "all_text_to_embed = question_texts + mis_texts\n",
    "\n",
    "with open('stage1_input_texts.json', 'w') as file:\n",
    "    json.dump(all_text_to_embed, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d04dd91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.478623Z",
     "iopub.status.busy": "2024-12-12T03:33:21.478372Z",
     "iopub.status.idle": "2024-12-12T03:33:21.484072Z",
     "shell.execute_reply": "2024-12-12T03:33:21.483300Z"
    },
    "papermill": {
     "duration": 0.018254,
     "end_time": "2024-12-12T03:33:21.485755",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.467501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_stage1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_stage1.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MODEL_PATH = sys.argv[1]\n",
    "LORA_PATH = sys.argv[2]\n",
    "SAVE_PATH = sys.argv[3]\n",
    "LEFT_PADDING = True\n",
    "\n",
    "with open('stage1_input_texts.json', 'r') as file:\n",
    "    all_text_to_embed = json.load(file)\n",
    "\n",
    "def last_token_pool(last_hidden_states, attention_mask, left_padding=None):\n",
    "    if left_padding is None:\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        \n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "    \n",
    "def inference(sentences, model, tokenizer, device):\n",
    "    batch_size = 16\n",
    "    max_length = 2048\n",
    "    \n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    \n",
    "    # Get embeddings in sorted order\n",
    "    all_embeddings = []\n",
    "    for start_index in tqdm(range(0, len(sentences), batch_size), desc=\"Batches\"):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        encoded_batch = tokenizer(\n",
    "            sentences_batch,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        encoded_batch = encoded_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_batch)\n",
    "            embeddings = last_token_pool(\n",
    "                outputs.last_hidden_state, \n",
    "                encoded_batch['attention_mask'], \n",
    "                left_padding=LEFT_PADDING,\n",
    "            )\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy()\n",
    "        all_embeddings.append(embeddings)\n",
    "\n",
    "    all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    rearranged_embeddings = all_embeddings[np.argsort(length_sorted_idx)]\n",
    "    \n",
    "    return rearranged_embeddings\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if LEFT_PADDING:\n",
    "    tokenizer.padding_side = 'left'\n",
    "else:\n",
    "    tokenizer.padding_side = 'right'\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='cuda:0',\n",
    "    # attn_implementation='flash_attention_2',\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, LORA_PATH)\n",
    "\n",
    "all_embeddings = inference(\n",
    "    all_text_to_embed, \n",
    "    model, tokenizer, 'cuda:0'\n",
    ")\n",
    "\n",
    "np.save(SAVE_PATH, all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6d2e999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:33:21.506860Z",
     "iopub.status.busy": "2024-12-12T03:33:21.506619Z",
     "iopub.status.idle": "2024-12-12T03:36:02.608964Z",
     "shell.execute_reply": "2024-12-12T03:36:02.608001Z"
    },
    "papermill": {
     "duration": 161.115217,
     "end_time": "2024-12-12T03:36:02.611106",
     "exception": false,
     "start_time": "2024-12-12T03:33:21.495889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [02:02<00:00, 40.77s/it]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [02:02<00:00, 40.79s/it]\r\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:16<00:00,  2.32s/it]\r\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:17<00:00,  2.44s/it]\r\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 python run_stage1.py \\\n",
    "    /kaggle/input/qwen2.5/transformers/qwen2.5-14b-instruct-awq/1/Qwen2.5-14B-Instruct-AWQ \\\n",
    "    /kaggle/input/eedi-kfold-embedder/stage1_sep_formatted/q14b_ep5_u2b2g8_fold_1 \\\n",
    "    q14_embedding_f1.npy & \\\n",
    "CUDA_VISIBLE_DEVICES=1 python run_stage1.py \\\n",
    "    /kaggle/input/qwen2.5/transformers/qwen2.5-14b-instruct-awq/1/Qwen2.5-14B-Instruct-AWQ \\\n",
    "    /kaggle/input/eedi-kfold-embedder/stage1_sep_formatted/q14b_ep5_u2b2g8_fold_2 \\\n",
    "    q14_embedding_f2.npy 1 & \\\n",
    "wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "333f28fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:36:02.635704Z",
     "iopub.status.busy": "2024-12-12T03:36:02.635406Z",
     "iopub.status.idle": "2024-12-12T03:36:02.639759Z",
     "shell.execute_reply": "2024-12-12T03:36:02.638981Z"
    },
    "papermill": {
     "duration": 0.018272,
     "end_time": "2024-12-12T03:36:02.641365",
     "exception": false,
     "start_time": "2024-12-12T03:36:02.623093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! CUDA_VISIBLE_DEVICES=0 python run_stage1.py \\\n",
    "#     /kaggle/input/qwen2.5/transformers/qwen2.5-14b-instruct-awq/1/Qwen2.5-14B-Instruct-AWQ \\\n",
    "#     /kaggle/input/eedi-kfold-embedder/stage1_sep_formatted/q14b_ep5_u2b2g8_fold_3 \\\n",
    "#     q14_embedding_f3.npy & \\\n",
    "# CUDA_VISIBLE_DEVICES=1 python run_stage1.py \\\n",
    "#     /kaggle/input/qwen2.5/transformers/qwen2.5-14b-instruct-awq/1/Qwen2.5-14B-Instruct-AWQ \\\n",
    "#     /kaggle/input/eedi-kfold-embedder/stage1_sep_formatted/q14b_ep5_u2b2g8_fold_4 \\\n",
    "#     q14_embedding_f4.npy 1 & \\\n",
    "# wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "269037c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:36:02.665345Z",
     "iopub.status.busy": "2024-12-12T03:36:02.665102Z",
     "iopub.status.idle": "2024-12-12T03:36:02.668835Z",
     "shell.execute_reply": "2024-12-12T03:36:02.668016Z"
    },
    "papermill": {
     "duration": 0.017667,
     "end_time": "2024-12-12T03:36:02.670477",
     "exception": false,
     "start_time": "2024-12-12T03:36:02.652810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! CUDA_VISIBLE_DEVICES=0 python run_stage1.py \\\n",
    "#     /kaggle/input/qwen2.5/transformers/qwen2.5-14b-instruct-awq/1/Qwen2.5-14B-Instruct-AWQ \\\n",
    "#     /kaggle/input/eedi-kfold-embedder/stage1_sep_formatted/qwen_14b_fold_1_ep5_bsz4_fix \\\n",
    "#     q14_embedding_f1.npy & \\\n",
    "# CUDA_VISIBLE_DEVICES=1 python run_stage1.py \\\n",
    "#     /kaggle/input/qwen2.5/transformers/qwen2.5-14b-instruct-awq/1/Qwen2.5-14B-Instruct-AWQ \\\n",
    "#     /kaggle/input/eedi-kfold-embedder/stage1_sep_formatted/qwen_14b_fold_2_ep5_bsz4_fix \\\n",
    "#     q14_embedding_f2.npy 1 & \\\n",
    "# wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdbdc782",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:36:02.693503Z",
     "iopub.status.busy": "2024-12-12T03:36:02.693214Z",
     "iopub.status.idle": "2024-12-12T03:36:46.537408Z",
     "shell.execute_reply": "2024-12-12T03:36:46.536180Z"
    },
    "papermill": {
     "duration": 43.85807,
     "end_time": "2024-12-12T03:36:46.539555",
     "exception": false,
     "start_time": "2024-12-12T03:36:02.681485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.37s/it]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.39s/it]\r\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:16<00:00,  2.30s/it]\r\n",
      "Batches: 100%|████████████████████████████████████| 7/7 [00:17<00:00,  2.46s/it]\r\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0 python run_stage1.py \\\n",
    "    /kaggle/input/qwen2.5/transformers/qwen2.5-14b-instruct-awq/1/Qwen2.5-14B-Instruct-AWQ \\\n",
    "    /kaggle/input/eedi-kfold-embedder/stage1_sep_formatted/qwen_14b_fold_3_ep5_bsz4_fix \\\n",
    "    q14_embedding_f3.npy & \\\n",
    "CUDA_VISIBLE_DEVICES=1 python run_stage1.py \\\n",
    "    /kaggle/input/qwen2.5/transformers/qwen2.5-14b-instruct-awq/1/Qwen2.5-14B-Instruct-AWQ \\\n",
    "    /kaggle/input/eedi-kfold-embedder/stage1_sep_formatted/qwen_14b_fold_4_ep5_bsz4_fix \\\n",
    "    q14_embedding_f4.npy 1 & \\\n",
    "wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f99c5210",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:36:46.566690Z",
     "iopub.status.busy": "2024-12-12T03:36:46.566031Z",
     "iopub.status.idle": "2024-12-12T03:36:46.658814Z",
     "shell.execute_reply": "2024-12-12T03:36:46.657921Z"
    },
    "papermill": {
     "duration": 0.108218,
     "end_time": "2024-12-12T03:36:46.660741",
     "exception": false,
     "start_time": "2024-12-12T03:36:46.552523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_similarities = []\n",
    "for i_fold in [1,2,3,4]:\n",
    "    fold_embedding = np.load(f'q14_embedding_f{i_fold}.npy')\n",
    "    q_embedding = fold_embedding[:len(question_texts)]\n",
    "    m_embedding = fold_embedding[len(question_texts):]\n",
    "\n",
    "    sim = q_embedding @ m_embedding.T\n",
    "    all_similarities.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68c9b2f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:36:46.686968Z",
     "iopub.status.busy": "2024-12-12T03:36:46.686682Z",
     "iopub.status.idle": "2024-12-12T03:36:46.691807Z",
     "shell.execute_reply": "2024-12-12T03:36:46.690942Z"
    },
    "papermill": {
     "duration": 0.019896,
     "end_time": "2024-12-12T03:36:46.693324",
     "exception": false,
     "start_time": "2024-12-12T03:36:46.673428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_scores = np.mean(all_similarities + [eedi_11scores, eedi_11scores], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22e2e025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:36:46.719432Z",
     "iopub.status.busy": "2024-12-12T03:36:46.718966Z",
     "iopub.status.idle": "2024-12-12T03:36:46.722915Z",
     "shell.execute_reply": "2024-12-12T03:36:46.722060Z"
    },
    "papermill": {
     "duration": 0.018933,
     "end_time": "2024-12-12T03:36:46.724507",
     "exception": false,
     "start_time": "2024-12-12T03:36:46.705574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mean_scores = torch.tensor(eedi_11scores)\n",
    "mean_scores = torch.tensor(mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a828918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:36:46.751426Z",
     "iopub.status.busy": "2024-12-12T03:36:46.750964Z",
     "iopub.status.idle": "2024-12-12T03:36:46.758327Z",
     "shell.execute_reply": "2024-12-12T03:36:46.757507Z"
    },
    "papermill": {
     "duration": 0.022295,
     "end_time": "2024-12-12T03:36:46.759930",
     "exception": false,
     "start_time": "2024-12-12T03:36:46.737635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_indices = torch.argsort(mean_scores, 1, descending=True)[:,:35].tolist()\n",
    "\n",
    "stage2_data = []\n",
    "for i, row in df_input.iterrows():\n",
    "    qa_id = row['QuestionId_Answer']\n",
    "    top_mis_ids = sorted_indices[i]\n",
    "        \n",
    "    stage2_data.append({\n",
    "        'qa_id': qa_id,\n",
    "        'ConstructName': row['ConstructName'],\n",
    "        'SubjectName': row['SubjectName'],\n",
    "        'QuestionText': row['QuestionText'],\n",
    "        'CorrectAnswerText': row[f'CorrectAnswerText'],\n",
    "        'IncorrectAnswerText': row[f'IncorrectAnswerText'],\n",
    "        'top_mis_id': top_mis_ids,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5778ca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:36:46.785950Z",
     "iopub.status.busy": "2024-12-12T03:36:46.785506Z",
     "iopub.status.idle": "2024-12-12T03:36:46.790849Z",
     "shell.execute_reply": "2024-12-12T03:36:46.790057Z"
    },
    "papermill": {
     "duration": 0.020038,
     "end_time": "2024-12-12T03:36:46.792575",
     "exception": false,
     "start_time": "2024-12-12T03:36:46.772537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('stage2_data.json', 'w') as file:\n",
    "    json.dump(stage2_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909365d",
   "metadata": {
    "papermill": {
     "duration": 0.012281,
     "end_time": "2024-12-12T03:36:46.817705",
     "exception": false,
     "start_time": "2024-12-12T03:36:46.805424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f42b28f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:36:46.843775Z",
     "iopub.status.busy": "2024-12-12T03:36:46.843261Z",
     "iopub.status.idle": "2024-12-12T03:36:46.853890Z",
     "shell.execute_reply": "2024-12-12T03:36:46.853107Z"
    },
    "papermill": {
     "duration": 0.025693,
     "end_time": "2024-12-12T03:36:46.855416",
     "exception": false,
     "start_time": "2024-12-12T03:36:46.829723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "import vllm\n",
    "from vllm.lora.request import LoRARequest\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from typing import List\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "\n",
    "cutoff_time = int(sys.argv[1])\n",
    "\n",
    "LORA_PATHS = [\n",
    "    \"/kaggle/input/eedi_qwen_reranker/transformers/eedi_stage2_q32b_ep5/1\",\n",
    "    \"/kaggle/input/eedi_qwen_reranker/transformers/eedi_stage2_q32b_ep5_fold0/1\",\n",
    "    \"/kaggle/input/eedi_qwen_reranker/transformers/eedi_stage2_q32b_ep5_fold1/1\",\n",
    "    \"/kaggle/input/eedi_qwen_reranker/transformers/eedi_stage2_q32b_ep5_fold2/1\",\n",
    "    \"/kaggle/input/eedi_qwen_reranker/transformers/eedi_stage2_q32b_ep5_fold3/1\",\n",
    "    \"/kaggle/input/eedi_qwen_reranker/transformers/eedi_stage2_q32b_syn_ep2/1\",\n",
    "]\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/qwen2.5-32b-instruct-awq/1/Qwen2.5-32B-Instruct-AWQ\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "mis_map = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "mis_list = mis_map[\"MisconceptionName\"].values\n",
    "\n",
    "with open('stage2_data.json', 'r') as f:\n",
    "    stage2_data = json.load(f)\n",
    "\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "PROMPT = \"\"\"Task:\n",
    "As a Mathematics teacher, your goal is to analyze a student's incorrect answer of a mathematics question, identify their fundamental conceptual misunderstanding, and select the single most appropriate misconception number from the given misconception options.\n",
    "\n",
    "Here is a mathematics question about {ConstructName}({SubjectName}).\n",
    "Question:\n",
    "{Question}\n",
    "\n",
    "Correct Answer:\n",
    "{CorrectAnswer}\n",
    "\n",
    "Incorrect Answer:\n",
    "{IncorrectAnswer}\n",
    "\n",
    "Carefully analyze the incorrect answer and select a single most appropriate misconception number from the given misconceptions.\n",
    "\n",
    "Here are the retrieved misconceptions:\n",
    "{Retrival}\n",
    "\n",
    "Only output the code of the selected misconception.\n",
    "Don't output any other words.\n",
    "\"\"\"\n",
    "\n",
    "mis_codes = [\n",
    "    '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n",
    "    'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n",
    "    'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "]\n",
    "\n",
    "\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": preprocess_text(\n",
    "                PROMPT.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[f\"IncorrectAnswerText\"],\n",
    "                    CorrectAnswer=row[f\"CorrectAnswerText\"],\n",
    "                    Retrival=row[f\"retrieval\"]\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "def make_cand_txt(cond_mids, mis_list):\n",
    "    candidate_mis_txts = [mis_list[x] for x in cond_mids]\n",
    "    return '\\n'.join([f'{mc}. {mt}' for mc, mt in zip(mis_codes, candidate_mis_txts)])\n",
    "\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def local_shuffle_with_mapping(lst: List, max_distance: int = 5) -> Tuple[List, Dict[int, int]]:\n",
    "    \"\"\"\n",
    "    對列表進行局部打亂，並返回新位置到原始位置的映射關係\n",
    "    通過交換操作確保所有元素的移動距離都不超過指定的最大值\n",
    "    \n",
    "    參數:\n",
    "    lst: 要打亂的列表\n",
    "    max_distance: 元素最大移動距離（預設為5）\n",
    "    \n",
    "    返回:\n",
    "    tuple: (打亂後的列表, {新位置: 原始位置} 的字典)\n",
    "    \"\"\"\n",
    "    result = lst.copy()\n",
    "    n = len(result)\n",
    "    \n",
    "    # 追蹤每個位置的元素的原始位置\n",
    "    curr_to_orig = list(range(n))  # curr_to_orig[i] 表示當前在位置i的元素原本在哪個位置\n",
    "    \n",
    "    # 隨機選擇要處理的位置順序\n",
    "    positions = list(range(n))\n",
    "    random.shuffle(positions)\n",
    "    \n",
    "    for pos in positions:\n",
    "        # 找出當前位置元素的原始位置\n",
    "        orig_pos = curr_to_orig[pos]\n",
    "        \n",
    "        # 計算可以交換的範圍\n",
    "        # 只考慮向前或向後max_distance以內的位置\n",
    "        start = max(0, pos - max_distance)\n",
    "        end = min(n - 1, pos + max_distance)\n",
    "        valid_positions = []\n",
    "        \n",
    "        # 檢查每個可能的交換位置\n",
    "        for new_pos in range(start, end + 1):\n",
    "            if new_pos != pos:\n",
    "                # 檢查如果與new_pos交換，兩個位置的元素是否都滿足最大距離限制\n",
    "                other_orig_pos = curr_to_orig[new_pos]\n",
    "                \n",
    "                # 檢查兩個元素交換後是否都滿足距離限制\n",
    "                if (abs(new_pos - orig_pos) <= max_distance and  # 當前元素移動到new_pos的距離\n",
    "                    abs(pos - other_orig_pos) <= max_distance):  # new_pos位置的元素移動到pos的距離\n",
    "                    valid_positions.append(new_pos)\n",
    "        \n",
    "        # 如果有合法的交換位置，隨機選一個進行交換\n",
    "        if valid_positions:\n",
    "            swap_pos = random.choice(valid_positions)\n",
    "            \n",
    "            # 交換元素\n",
    "            result[pos], result[swap_pos] = result[swap_pos], result[pos]\n",
    "            # 更新映射\n",
    "            curr_to_orig[pos], curr_to_orig[swap_pos] = curr_to_orig[swap_pos], curr_to_orig[pos]\n",
    "    \n",
    "    # 創建最終的映射字典\n",
    "    position_mapping = {i: curr_to_orig[i] for i in range(n)}\n",
    "    \n",
    "    return result, position_mapping\n",
    "\n",
    "for d in stage2_data:\n",
    "    top_mis_id = d['top_mis_id'][:35]\n",
    "\n",
    "    s_mid_ids, new_to_old_pos_map = local_shuffle_with_mapping(\n",
    "        top_mis_id,\n",
    "        max_distance=5\n",
    "    )\n",
    "    retrieval_txt = make_cand_txt(s_mid_ids, mis_list)\n",
    "\n",
    "    in_text = apply_template({\n",
    "        **d,\n",
    "        \"retrieval\": retrieval_txt\n",
    "    }, tokenizer)\n",
    "    d['in_text'] = in_text\n",
    "    \n",
    "print(stage2_data[0]['in_text'])\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.90, \n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=4096,\n",
    "    # disable_log_stats=True,\n",
    "\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=64,\n",
    "    max_logprobs=40,\n",
    ")\n",
    "\n",
    "\n",
    "all_lora_preds = []\n",
    "for i_lora, lora_path in enumerate(LORA_PATHS):\n",
    "    remain_time = cutoff_time - time.time()\n",
    "    print(f'Remain time: {remain_time}')\n",
    "    if remain_time < 50 * 60:\n",
    "        print('Remain time < 50 min, break')\n",
    "        break\n",
    "    \n",
    "    lora_request = LoRARequest(f'lora_{i_lora}', i_lora+1, lora_path=lora_path)\n",
    "    print(f\"Using {i_lora+1}-th LoRA from {lora_path}\")\n",
    "\n",
    "    # shuffle top_mid_id\n",
    "    for d in stage2_data:\n",
    "        top_mis_id = d['top_mis_id'][:35]\n",
    "    \n",
    "        s_mid_ids, new_to_old_pos_map = local_shuffle_with_mapping(\n",
    "            top_mis_id,\n",
    "            max_distance=5\n",
    "        )\n",
    "        retrieval_txt = make_cand_txt(s_mid_ids, mis_list)\n",
    "    \n",
    "        in_text = apply_template({\n",
    "            **d,\n",
    "            \"retrieval\": retrieval_txt\n",
    "        }, tokenizer)\n",
    "        d['in_text'] = in_text\n",
    "        d['new_to_old_pos_map'] = new_to_old_pos_map\n",
    "\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        n=1,\n",
    "        temperature=0.0,\n",
    "        max_tokens=1,\n",
    "        logprobs=40,\n",
    "    )\n",
    "    \n",
    "    llm_responses = llm.generate(\n",
    "        [d['in_text'] for d in stage2_data],\n",
    "        sampling_params,\n",
    "        use_tqdm=True,\n",
    "        lora_request=lora_request,\n",
    "    )\n",
    "\n",
    "    print('-----------MODEL OUTPUT EXAMPLE--------------')\n",
    "    print(llm_responses[0].outputs[0].text)\n",
    "    print('---------------------------------------------')\n",
    "\n",
    "    all_preds = []\n",
    "    for d, respones in zip(stage2_data, llm_responses):\n",
    "        new_to_old_pos_map = d['new_to_old_pos_map']\n",
    "        probs = [0.0] * len(mis_codes)\n",
    "        for token_id, logprob in respones.outputs[0].logprobs[0].items():\n",
    "            decoded_token = logprob.decoded_token\n",
    "            lp = logprob.logprob\n",
    "            if decoded_token in mis_codes:\n",
    "                pred_index = mis_codes.index(decoded_token)\n",
    "                pred_index = new_to_old_pos_map[pred_index]\n",
    "                probs[pred_index] = np.exp(lp)\n",
    "        all_preds.append(probs)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    all_lora_preds.append(all_preds)\n",
    "\n",
    "mean_preds = np.mean(all_lora_preds, axis=0)\n",
    "\n",
    "results = []\n",
    "result_probs = []\n",
    "all_top_mis_ids = [x['top_mis_id'] for x in stage2_data]\n",
    "for top_mis_ids, pred_probs in zip(all_top_mis_ids, mean_preds):\n",
    "    sorted_indices = np.argsort(pred_probs)[::-1]\n",
    "    pred_mis_ids = [top_mis_ids[si] for si in sorted_indices if si < len(top_mis_ids)]\n",
    "    pred_probs = [pred_probs[si] for si in sorted_indices if si < len(top_mis_ids)]\n",
    "    assert len(pred_mis_ids) == len(pred_probs)\n",
    "    results.append(pred_mis_ids)\n",
    "    result_probs.append(pred_probs)\n",
    "    \n",
    "# all_pred_ranks = []\n",
    "# for respones in llm_responses:\n",
    "#     pred_ranks = []\n",
    "#     for token_id, logprob in respones.outputs[0].logprobs[0].items():\n",
    "#         decoded_token = logprob.decoded_token\n",
    "#         if decoded_token in mis_codes:\n",
    "#             pred_ranks.append(mis_codes.index(decoded_token))\n",
    "#     all_pred_ranks.append(pred_ranks)\n",
    "    \n",
    "# results = []\n",
    "# all_top_mis_ids = [x['top_mis_id'] for x in stage2_data]\n",
    "# for top_mis_ids, pred_ranks in zip(all_top_mis_ids, all_pred_ranks):\n",
    "#     pred_mis_ids = [top_mis_ids[pr] for pr in pred_ranks if pr < len(top_mis_ids)]\n",
    "#     r = pred_mis_ids + [x for x in top_mis_ids if x not in pred_mis_ids]\n",
    "#     results.append(r[:25])\n",
    "\n",
    "result_txts = [' '.join([str(x) for x in mids]) for mids in results]\n",
    "\n",
    "submission = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/sample_submission.csv')\n",
    "submission[\"MisconceptionId\"] = result_txts\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "submission.to_csv(\"submission_vllm.csv\", index=False)\n",
    "print('Save result to submission.csv and submission_vllm.csv')\n",
    "\n",
    "with open('result_probs.json', 'w') as file:\n",
    "    json.dump(result_probs, file)\n",
    "print('Save result_probs to result_probs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f5d9565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:36:46.881424Z",
     "iopub.status.busy": "2024-12-12T03:36:46.881147Z",
     "iopub.status.idle": "2024-12-12T03:45:18.771627Z",
     "shell.execute_reply": "2024-12-12T03:45:18.770455Z"
    },
    "papermill": {
     "duration": 511.905736,
     "end_time": "2024-12-12T03:45:18.773721",
     "exception": false,
     "start_time": "2024-12-12T03:36:46.867985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-12 03:36:49 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead, and make sure to uninstall `pynvml`. When both of them are installed, `pynvml` will take precedence and cause errors. See https://pypi.org/project/pynvml for more information.\r\n",
      "<|im_start|>system\r\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Task:\r\n",
      "As a Mathematics teacher, your goal is to analyze a student's incorrect answer of a mathematics question, identify their fundamental conceptual misunderstanding, and select the single most appropriate misconception number from the given misconception options.\r\n",
      "\r\n",
      "Here is a mathematics question about Use the order of operations to carry out calculations involving powers(BIDMAS).\r\n",
      "Question:\r\n",
      "\\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "\r\n",
      "Correct Answer:\r\n",
      " 3 \\times(2+4)-5 \r\n",
      "\r\n",
      "Incorrect Answer:\r\n",
      " 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "Carefully analyze the incorrect answer and select a single most appropriate misconception number from the given misconceptions.\r\n",
      "\r\n",
      "Here are the retrieved misconceptions:\r\n",
      "1. Does not follow the arrows through a function machine, changes the order of the operations asked.\r\n",
      "2. Believes the square of a negative will also be negative\r\n",
      "3. Confuses the order of operations, believes addition comes before division\r\n",
      "4. When subtracting a negative number from a positive number, uses a method which assumes one of the negative signs can be ignored \r\n",
      "5. When asked for a specific term in a sequence gives the term before\r\n",
      "6. Mixes up squaring and multiplying by 4\r\n",
      "7. Believes they must expand brackets before solving an equation\r\n",
      "8. Does not understand the command word 'difference'\r\n",
      "9. Multiplies surds when asked to add\r\n",
      "A. Does not know about the + notation for directions in rotations\r\n",
      "B. Believes dividing a negative by a positive gives a positive answer\r\n",
      "C. When squaring a variable, believes they also need to square the coefficient\r\n",
      "D. Believes addition of terms and powers of terms are equivalent e.g. a + c = a^c\r\n",
      "E. Does not realise that when you multiply by 0 the answer will always be 0\r\n",
      "F. Does not understand place value after the decimal point\r\n",
      "G. Believes squaring a fraction requires you to double both parts\r\n",
      "H. Does not subtract from the hours, when having to borrow for a time calculation\r\n",
      "I. Believes greater than/less than symbols include end values\r\n",
      "J. Has written the value of the 'ones' column as the answer, withiut using the numbers in the question.\r\n",
      "K. Thinks you subtract rather than add when finding the previous term in a descending linear sequence\r\n",
      "L. Does not understand how to create a multiple of an equation\r\n",
      "M. Forgot to simplify the fraction\r\n",
      "N. Does not understand the term multiple\r\n",
      "O. Does not realise that division can be broken down into factors\r\n",
      "P. Believes you add 2 instead of subtracting 2 to the numbers of sides when finding total interior angles\r\n",
      "Q. Confuses AM and PM when calculating time\r\n",
      "R. Believes the sides of an inequality can be switched without changing the direction of the sign\r\n",
      "S. When adding or subtracting surds, just adds or subtracts the numbers under the surd rather than first trying to simplifying to find like surds\r\n",
      "T. Confuses a function with an expression\r\n",
      "U. Believes the arrow on a number line points to the middle value\r\n",
      "V. Does not understand bar modelling in algebra\r\n",
      "W. When asked for the value of a digit, gives an answer 10 times too big\r\n",
      "X. Does not know that angles in a triangle sum to 180 degrees\r\n",
      "Y. When asked for a number n times smaller or bigger, subtracts or adds n\r\n",
      "Z. Confuses a term with an equation\r\n",
      "\r\n",
      "Only output the code of the selected misconception.\r\n",
      "Don't output any other words.<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "WARNING 12-12 03:37:05 config.py:321] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-12 03:37:05 config.py:905] Defaulting to use mp for distributed inference\r\n",
      "WARNING 12-12 03:37:05 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 12-12 03:37:05 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/qwen2.5/transformers/qwen2.5-32b-instruct-awq/1/Qwen2.5-32B-Instruct-AWQ', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/qwen2.5-32b-instruct-awq/1/Qwen2.5-32B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/qwen2.5/transformers/qwen2.5-32b-instruct-awq/1/Qwen2.5-32B-Instruct-AWQ, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\r\n",
      "WARNING 12-12 03:37:06 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 12-12 03:37:06 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m INFO 12-12 03:37:06 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 03:37:06 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 03:37:06 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m INFO 12-12 03:37:06 selector.py:115] Using XFormers backend.\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m INFO 12-12 03:37:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "INFO 12-12 03:37:08 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m INFO 12-12 03:37:08 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-12 03:37:08 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m INFO 12-12 03:37:08 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-12 03:37:08 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-12 03:37:28 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m INFO 12-12 03:37:28 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-12 03:37:28 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7c9b2c3e0220>, local_subscribe_port=40923, remote_subscribe_port=None)\r\n",
      "INFO 12-12 03:37:28 model_runner.py:1056] Starting to load model /kaggle/input/qwen2.5/transformers/qwen2.5-32b-instruct-awq/1/Qwen2.5-32B-Instruct-AWQ...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m INFO 12-12 03:37:28 model_runner.py:1056] Starting to load model /kaggle/input/qwen2.5/transformers/qwen2.5-32b-instruct-awq/1/Qwen2.5-32B-Instruct-AWQ...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m INFO 12-12 03:37:28 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m INFO 12-12 03:37:28 selector.py:115] Using XFormers backend.\r\n",
      "INFO 12-12 03:37:28 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 03:37:28 selector.py:115] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:25<01:43, 25.76s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:53<01:20, 26.87s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [01:29<01:02, 31.20s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [02:06<00:33, 33.29s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:40<00:00, 33.76s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:40<00:00, 32.17s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=314)\u001b[0;0m INFO 12-12 03:40:09 model_runner.py:1067] Loading model weights took 9.0934 GB\r\n",
      "INFO 12-12 03:40:10 model_runner.py:1067] Loading model weights took 9.0934 GB\r\n",
      "INFO 12-12 03:40:22 distributed_gpu_executor.py:57] # GPU blocks: 416, # CPU blocks: 2048\r\n",
      "INFO 12-12 03:40:22 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 1.62x\r\n",
      "Remain time: 31170.612540006638\r\n",
      "/kaggle/working/run_vllm.py:200: DeprecationWarning: The 'lora_local_path' attribute is deprecated and will be removed in a future version. Please use 'lora_path' instead.\r\n",
      "  lora_request = LoRARequest(f'lora_{i_lora}', i_lora+1, lora_path=lora_path)\r\n",
      "Using 1-th LoRA from /kaggle/input/eedi_qwen_reranker/transformers/eedi_stage2_q32b_ep5/1\r\n",
      "Processed prompts: 100%|█| 9/9 [00:47<00:00,  5.31s/it, est. speed input: 145.33\r\n",
      "-----------MODEL OUTPUT EXAMPLE--------------\r\n",
      "1\r\n",
      "---------------------------------------------\r\n",
      "Remain time: 31122.14337849617\r\n",
      "Using 2-th LoRA from /kaggle/input/eedi_qwen_reranker/transformers/eedi_stage2_q32b_ep5_fold0/1\r\n",
      "Processed prompts: 100%|█| 9/9 [00:46<00:00,  5.14s/it, est. speed input: 150.19\r\n",
      "-----------MODEL OUTPUT EXAMPLE--------------\r\n",
      "7\r\n",
      "---------------------------------------------\r\n",
      "Remain time: 31075.319012880325\r\n",
      "Using 3-th LoRA from /kaggle/input/eedi_qwen_reranker/transformers/eedi_stage2_q32b_ep5_fold1/1\r\n",
      "Processed prompts: 100%|█| 9/9 [00:47<00:00,  5.30s/it, est. speed input: 145.53\r\n",
      "-----------MODEL OUTPUT EXAMPLE--------------\r\n",
      "5\r\n",
      "---------------------------------------------\r\n",
      "Remain time: 31026.88095331192\r\n",
      "Using 4-th LoRA from /kaggle/input/eedi_qwen_reranker/transformers/eedi_stage2_q32b_ep5_fold2/1\r\n",
      "Processed prompts: 100%|█| 9/9 [00:46<00:00,  5.12s/it, est. speed input: 150.62\r\n",
      "-----------MODEL OUTPUT EXAMPLE--------------\r\n",
      "1\r\n",
      "---------------------------------------------\r\n",
      "Remain time: 30980.116793632507\r\n",
      "Using 5-th LoRA from /kaggle/input/eedi_qwen_reranker/transformers/eedi_stage2_q32b_ep5_fold3/1\r\n",
      "Processed prompts: 100%|█| 9/9 [00:44<00:00,  4.98s/it, est. speed input: 154.85\r\n",
      "-----------MODEL OUTPUT EXAMPLE--------------\r\n",
      "2\r\n",
      "---------------------------------------------\r\n",
      "Remain time: 30934.587595939636\r\n",
      "Using 6-th LoRA from /kaggle/input/eedi_qwen_reranker/transformers/eedi_stage2_q32b_syn_ep2/1\r\n",
      "Processed prompts: 100%|█| 9/9 [00:46<00:00,  5.16s/it, est. speed input: 149.46\r\n",
      "-----------MODEL OUTPUT EXAMPLE--------------\r\n",
      "1\r\n",
      "---------------------------------------------\r\n",
      "Save result to submission.csv and submission_vllm.csv\r\n",
      "Save result_probs to result_probs.json\r\n",
      "ERROR 12-12 03:45:11 multiproc_worker_utils.py:116] Worker VllmWorkerProcess pid 314 died, exit code: -15\r\n",
      "INFO 12-12 03:45:11 multiproc_worker_utils.py:120] Killing local vLLM worker processes\r\n",
      "[rank0]:[W1212 03:45:15.216648440 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\r\n"
     ]
    }
   ],
   "source": [
    "!python run_vllm.py $cutoff_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6238325e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:45:18.805671Z",
     "iopub.status.busy": "2024-12-12T03:45:18.805330Z",
     "iopub.status.idle": "2024-12-12T03:45:18.821197Z",
     "shell.execute_reply": "2024-12-12T03:45:18.820325Z"
    },
    "papermill": {
     "duration": 0.034224,
     "end_time": "2024-12-12T03:45:18.822983",
     "exception": false,
     "start_time": "2024-12-12T03:45:18.788759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>55 15 77 24 34 52 74 23 64 82 60 96 84 13 90 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>15 52 77 82 25 34 74 73 31 68 23 29 46 96 32 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>15 77 68 55 24 74 31 64 14 95 46 60 96 13 25 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>59 80 91 79 32 3 29 46 94 25 44 6 27 4 55 49 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>59 80 79 58 55 46 82 44 32 3 6 64 91 74 29 73 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>59 80 46 3 79 49 82 64 44 32 91 74 34 4 89 27 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>30 99 93 71 46 65 64 41 86 23 34 16 98 38 2 79...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>30 65 79 38 94 78 64 71 41 98 13 49 14 93 89 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>30 89 84 49 23 56 38 93 52 46 94 64 27 99 77 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  55 15 77 24 34 52 74 23 64 82 60 96 84 13 90 1...\n",
       "1            1869_C  15 52 77 82 25 34 74 73 31 68 23 29 46 96 32 6...\n",
       "2            1869_D  15 77 68 55 24 74 31 64 14 95 46 60 96 13 25 4...\n",
       "3            1870_A  59 80 91 79 32 3 29 46 94 25 44 6 27 4 55 49 6...\n",
       "4            1870_B  59 80 79 58 55 46 82 44 32 3 6 64 91 74 29 73 ...\n",
       "5            1870_C  59 80 46 3 79 49 82 64 44 32 91 74 34 4 89 27 ...\n",
       "6            1871_A  30 99 93 71 46 65 64 41 86 23 34 16 98 38 2 79...\n",
       "7            1871_C  30 65 79 38 94 78 64 71 41 98 13 49 14 93 89 7...\n",
       "8            1871_D  30 89 84 49 23 56 38 93 52 46 94 64 27 99 77 8..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"submission_vllm.csv\")\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6e75515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:45:18.854852Z",
     "iopub.status.busy": "2024-12-12T03:45:18.854600Z",
     "iopub.status.idle": "2024-12-12T03:45:18.862256Z",
     "shell.execute_reply": "2024-12-12T03:45:18.861466Z"
    },
    "papermill": {
     "duration": 0.024753,
     "end_time": "2024-12-12T03:45:18.863909",
     "exception": false,
     "start_time": "2024-12-12T03:45:18.839156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.47902074456012,\n",
       " 0.09341742497260075,\n",
       " 0.0849409653579438,\n",
       " 0.019503722015854753,\n",
       " 0.019272562565414406,\n",
       " 0.018412652093828436,\n",
       " 0.015172199748685186,\n",
       " 0.01448667154444854,\n",
       " 0.014297883007457502,\n",
       " 0.013540329067711167,\n",
       " 0.013214858451031397,\n",
       " 0.012986003898313775,\n",
       " 0.011284559427117221,\n",
       " 0.011016912369124625,\n",
       " 0.011007700379314844,\n",
       " 0.010772409173457906,\n",
       " 0.010356878825827625,\n",
       " 0.010268283527928775,\n",
       " 0.010010688510497355,\n",
       " 0.009678680140212921,\n",
       " 0.00961114464388261,\n",
       " 0.009220271514517467,\n",
       " 0.00921780023514358,\n",
       " 0.00919266449175064,\n",
       " 0.009178374196190648,\n",
       " 0.008064827325946674,\n",
       " 0.007851268775416583,\n",
       " 0.0076363869628934935,\n",
       " 0.007547552857929776,\n",
       " 0.006793104372738863,\n",
       " 0.006034493123839287,\n",
       " 0.00569695633673428,\n",
       " 0.00531662286983058,\n",
       " 0.004601367576941298,\n",
       " 0.004168739390781904]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('result_probs.json') as file:\n",
    "    result_probs = json.load(file)\n",
    "\n",
    "result_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "449fbfad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:45:18.895039Z",
     "iopub.status.busy": "2024-12-12T03:45:18.894716Z",
     "iopub.status.idle": "2024-12-12T03:45:19.014307Z",
     "shell.execute_reply": "2024-12-12T03:45:19.013360Z"
    },
    "papermill": {
     "duration": 0.137488,
     "end_time": "2024-12-12T03:45:19.016072",
     "exception": false,
     "start_time": "2024-12-12T03:45:18.878584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4370 1604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_table = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv')\n",
    "df_train = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv')\n",
    "\n",
    "mis_cols = ['MisconceptionAId', 'MisconceptionBId', 'MisconceptionCId', 'MisconceptionDId']\n",
    "all_mis = df_train[mis_cols].values.flatten()\n",
    "all_mis = [int(mid) for mid in all_mis if pd.notna(mid)]\n",
    "print(len(all_mis), len(set(all_mis)))\n",
    "\n",
    "novel_mis = [mis_id for mis_id in range(len(mis_table)) if mis_id not in all_mis]\n",
    "len(novel_mis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03c1e2f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:45:19.047180Z",
     "iopub.status.busy": "2024-12-12T03:45:19.046733Z",
     "iopub.status.idle": "2024-12-12T03:45:19.050959Z",
     "shell.execute_reply": "2024-12-12T03:45:19.050180Z"
    },
    "papermill": {
     "duration": 0.02141,
     "end_time": "2024-12-12T03:45:19.052564",
     "exception": false,
     "start_time": "2024-12-12T03:45:19.031154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "novel_mis_set = set(novel_mis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e380b142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:45:19.083548Z",
     "iopub.status.busy": "2024-12-12T03:45:19.082827Z",
     "iopub.status.idle": "2024-12-12T03:45:19.088424Z",
     "shell.execute_reply": "2024-12-12T03:45:19.087647Z"
    },
    "papermill": {
     "duration": 0.022908,
     "end_time": "2024-12-12T03:45:19.090128",
     "exception": false,
     "start_time": "2024-12-12T03:45:19.067220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_preds = [\n",
    "    [int(m) for m in p.split(' ')] for p in submission['MisconceptionId']\n",
    "]\n",
    "all_pred_probs = [ps for ps in result_probs]\n",
    "\n",
    "all_mid_probs = []\n",
    "\n",
    "# all_first = set([int(p[0]) for p in all_preds])\n",
    "# novel_mis = [m for m in novel_mis if not m in all_first]\n",
    "# len(novel_mis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1842dbc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:45:19.122040Z",
     "iopub.status.busy": "2024-12-12T03:45:19.121454Z",
     "iopub.status.idle": "2024-12-12T03:45:19.167822Z",
     "shell.execute_reply": "2024-12-12T03:45:19.166985Z"
    },
    "papermill": {
     "duration": 0.06533,
     "end_time": "2024-12-12T03:45:19.170397",
     "exception": false,
     "start_time": "2024-12-12T03:45:19.105067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonus ratio: 1.0, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.02, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.04, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.06, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.08, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.1, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.12, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.14, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.16, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.18, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.2, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.22, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.24, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.26, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.28, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.3, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.32, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.34, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.36, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.38, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.4, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.42, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.44, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.46, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.48, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.5, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.52, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.54, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.56, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.58, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.6, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.62, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.64, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.66, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.68, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.7, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.72, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.74, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.76, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.78, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.8, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.82, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.84, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.86, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.88, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.9, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.92, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.94, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.96, first rate 0.6666666666666666\n",
      "Bonus ratio: 1.98, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.0, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.02, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.04, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.06, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.08, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.1, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.12, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.14, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.16, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.18, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.2, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.22, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.24, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.26, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.28, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.3, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.32, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.34, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.36, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.38, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.4, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.42, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.44, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.46, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.48, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.5, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.52, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.54, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.56, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.58, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.6, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.62, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.64, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.66, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.68, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.7, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.72, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.74, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.76, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.78, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.8, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.82, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.84, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.86, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.88, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.9, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.92, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.94, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.96, first rate 0.6666666666666666\n",
      "Bonus ratio: 2.98, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.0, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.02, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.04, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.06, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.08, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.1, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.12, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.14, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.16, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.18, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.2, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.22, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.24, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.26, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.28, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.3, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.32, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.34, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.36, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.38, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.4, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.42, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.44, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.46, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.48, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.5, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.52, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.54, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.56, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.58, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.6, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.62, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.64, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.66, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.68, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.7, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.72, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.74, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.76, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.78, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.8, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.82, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.84, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.86, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.88, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.9, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.92, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.94, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.96, first rate 0.6666666666666666\n",
      "Bonus ratio: 3.98, first rate 0.6666666666666666\n",
      "Bonus ratio: 4.0, first rate 0.6666666666666666\n",
      "Bonus ratio: 4.02, first rate 0.6666666666666666\n",
      "Bonus ratio: 4.04, first rate 0.6666666666666666\n",
      "Bonus ratio: 4.06, first rate 0.6666666666666666\n",
      "Bonus ratio: 4.08, first rate 0.6666666666666666\n",
      "Bonus ratio: 4.1, first rate 0.6666666666666666\n",
      "Bonus ratio: 4.12, first rate 0.6666666666666666\n",
      "Bonus ratio: 4.14, first rate 0.7777777777777778\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "TARGET_NOVEL_FIRST_RATE = 0.75\n",
    "novel_bonus = 1.0\n",
    "\n",
    "while True:\n",
    "\n",
    "    mod_pred_probs = []\n",
    "    for i_question, (preds, probs) in enumerate(zip(all_preds, all_pred_probs)):\n",
    "        pps = []\n",
    "        for i_mis, (pred_id, prob) in enumerate(zip(preds, probs)):\n",
    "            if pred_id in novel_mis_set:\n",
    "                pps.append([pred_id, prob * novel_bonus])\n",
    "            else:\n",
    "                pps.append([pred_id, prob])\n",
    "        mod_pred_probs.append(pps)\n",
    "\n",
    "    mod_pred_probs = [\n",
    "        sorted(pps, key=lambda x: x[1], reverse=True) for pps in mod_pred_probs\n",
    "    ]\n",
    "\n",
    "    first_novel_rate = np.mean([pps[0][0] in novel_mis_set for pps in mod_pred_probs])\n",
    "\n",
    "    print(f'Bonus ratio: {round(novel_bonus, 2)}, first rate {first_novel_rate}')\n",
    "\n",
    "    if first_novel_rate > TARGET_NOVEL_FIRST_RATE:\n",
    "        print('Done')\n",
    "        break\n",
    "    elif novel_bonus > 100.0:\n",
    "        print('Extreme high bonus (>100), cant find enough novel')\n",
    "        break\n",
    "    else:\n",
    "        novel_bonus += 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb547374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:45:19.210364Z",
     "iopub.status.busy": "2024-12-12T03:45:19.210019Z",
     "iopub.status.idle": "2024-12-12T03:45:19.214724Z",
     "shell.execute_reply": "2024-12-12T03:45:19.213912Z"
    },
    "papermill": {
     "duration": 0.023038,
     "end_time": "2024-12-12T03:45:19.216350",
     "exception": false,
     "start_time": "2024-12-12T03:45:19.193312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_preds = []\n",
    "for pps in mod_pred_probs:\n",
    "    new_preds.append([mid for mid, prob in pps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0cd3a160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:45:19.247718Z",
     "iopub.status.busy": "2024-12-12T03:45:19.247376Z",
     "iopub.status.idle": "2024-12-12T03:45:19.259047Z",
     "shell.execute_reply": "2024-12-12T03:45:19.258262Z"
    },
    "papermill": {
     "duration": 0.029631,
     "end_time": "2024-12-12T03:45:19.260976",
     "exception": false,
     "start_time": "2024-12-12T03:45:19.231345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>55 15 77 24 96 13 90 73 25 46 30 17 34 52 74 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>25 15 73 46 96 24 13 30 90 52 77 82 34 74 17 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>15 24 46 96 13 25 90 30 73 10 77 65 68 55 74 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>59 80 79 46 94 25 73 56 96 24 91 32 3 29 44 6 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>59 80 79 46 73 94 96 56 25 24 58 55 82 44 32 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>59 80 46 79 94 25 56 24 73 96 3 49 82 64 44 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>30 93 46 65 41 79 62 24 25 56 63 13 99 71 64 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>30 65 79 94 41 13 93 46 56 24 25 62 63 38 78 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>30 56 93 46 94 89 65 62 79 63 24 84 25 49 23 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  55 15 77 24 96 13 90 73 25 46 30 17 34 52 74 2...\n",
       "1            1869_C  25 15 73 46 96 24 13 30 90 52 77 82 34 74 17 1...\n",
       "2            1869_D  15 24 46 96 13 25 90 30 73 10 77 65 68 55 74 3...\n",
       "3            1870_A  59 80 79 46 94 25 73 56 96 24 91 32 3 29 44 6 ...\n",
       "4            1870_B  59 80 79 46 73 94 96 56 25 24 58 55 82 44 32 3...\n",
       "5            1870_C  59 80 46 79 94 25 56 24 73 96 3 49 82 64 44 32...\n",
       "6            1871_A  30 93 46 65 41 79 62 24 25 56 63 13 99 71 64 8...\n",
       "7            1871_C  30 65 79 94 41 13 93 46 56 24 25 62 63 38 78 6...\n",
       "8            1871_D  30 56 93 46 94 89 65 62 79 63 24 84 25 49 23 3..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_strs = [\n",
    "    ' '.join([str(mid) for mid in pred[:25]]) for pred in new_preds\n",
    "]\n",
    "submission['MisconceptionId'] = pred_strs\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef34768f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:45:19.293064Z",
     "iopub.status.busy": "2024-12-12T03:45:19.292496Z",
     "iopub.status.idle": "2024-12-12T03:45:19.298195Z",
     "shell.execute_reply": "2024-12-12T03:45:19.297420Z"
    },
    "papermill": {
     "duration": 0.02401,
     "end_time": "2024-12-12T03:45:19.300209",
     "exception": false,
     "start_time": "2024-12-12T03:45:19.276199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "053589ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:45:19.339521Z",
     "iopub.status.busy": "2024-12-12T03:45:19.339182Z",
     "iopub.status.idle": "2024-12-12T03:45:19.350974Z",
     "shell.execute_reply": "2024-12-12T03:45:19.350136Z"
    },
    "papermill": {
     "duration": 0.030321,
     "end_time": "2024-12-12T03:45:19.353263",
     "exception": false,
     "start_time": "2024-12-12T03:45:19.322942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>55 15 77 24 96 13 90 73 25 46 30 17 34 52 74 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>25 15 73 46 96 24 13 30 90 52 77 82 34 74 17 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>15 24 46 96 13 25 90 30 73 10 77 65 68 55 74 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>59 80 79 46 94 25 73 56 96 24 91 32 3 29 44 6 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>59 80 79 46 73 94 96 56 25 24 58 55 82 44 32 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>59 80 46 79 94 25 56 24 73 96 3 49 82 64 44 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>30 93 46 65 41 79 62 24 25 56 63 13 99 71 64 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>30 65 79 94 41 13 93 46 56 24 25 62 63 38 78 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>30 56 93 46 94 89 65 62 79 63 24 84 25 49 23 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  55 15 77 24 96 13 90 73 25 46 30 17 34 52 74 2...\n",
       "1            1869_C  25 15 73 46 96 24 13 30 90 52 77 82 34 74 17 1...\n",
       "2            1869_D  15 24 46 96 13 25 90 30 73 10 77 65 68 55 74 3...\n",
       "3            1870_A  59 80 79 46 94 25 73 56 96 24 91 32 3 29 44 6 ...\n",
       "4            1870_B  59 80 79 46 73 94 96 56 25 24 58 55 82 44 32 3...\n",
       "5            1870_C  59 80 46 79 94 25 56 24 73 96 3 49 82 64 44 32...\n",
       "6            1871_A  30 93 46 65 41 79 62 24 25 56 63 13 99 71 64 8...\n",
       "7            1871_C  30 65 79 94 41 13 93 46 56 24 25 62 63 38 78 6...\n",
       "8            1871_D  30 56 93 46 94 89 65 62 79 63 24 84 25 49 23 3..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"submission.csv\")\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33a24317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T03:45:19.393689Z",
     "iopub.status.busy": "2024-12-12T03:45:19.393360Z",
     "iopub.status.idle": "2024-12-12T03:45:19.403711Z",
     "shell.execute_reply": "2024-12-12T03:45:19.402804Z"
    },
    "papermill": {
     "duration": 0.028264,
     "end_time": "2024-12-12T03:45:19.405265",
     "exception": false,
     "start_time": "2024-12-12T03:45:19.377001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>55 15 77 24 34 52 74 23 64 82 60 96 84 13 90 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>15 52 77 82 25 34 74 73 31 68 23 29 46 96 32 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>15 77 68 55 24 74 31 64 14 95 46 60 96 13 25 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>59 80 91 79 32 3 29 46 94 25 44 6 27 4 55 49 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>59 80 79 58 55 46 82 44 32 3 6 64 91 74 29 73 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>59 80 46 3 79 49 82 64 44 32 91 74 34 4 89 27 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>30 99 93 71 46 65 64 41 86 23 34 16 98 38 2 79...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>30 65 79 38 94 78 64 71 41 98 13 49 14 93 89 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>30 89 84 49 23 56 38 93 52 46 94 64 27 99 77 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  55 15 77 24 34 52 74 23 64 82 60 96 84 13 90 1...\n",
       "1            1869_C  15 52 77 82 25 34 74 73 31 68 23 29 46 96 32 6...\n",
       "2            1869_D  15 77 68 55 24 74 31 64 14 95 46 60 96 13 25 4...\n",
       "3            1870_A  59 80 91 79 32 3 29 46 94 25 44 6 27 4 55 49 6...\n",
       "4            1870_B  59 80 79 58 55 46 82 44 32 3 6 64 91 74 29 73 ...\n",
       "5            1870_C  59 80 46 3 79 49 82 64 44 32 91 74 34 4 89 27 ...\n",
       "6            1871_A  30 99 93 71 46 65 64 41 86 23 34 16 98 38 2 79...\n",
       "7            1871_C  30 65 79 38 94 78 64 71 41 98 13 49 14 93 89 7...\n",
       "8            1871_D  30 89 84 49 23 56 38 93 52 46 94 64 27 99 77 8..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"submission_vllm.csv\")\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6253164,
     "sourceId": 10131981,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10171817,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 210246302,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 171421,
     "modelInstanceId": 148911,
     "sourceId": 174909,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171434,
     "modelInstanceId": 148923,
     "sourceId": 174921,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 175891,
     "modelInstanceId": 153419,
     "sourceId": 180070,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 175891,
     "modelInstanceId": 153454,
     "sourceId": 180107,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 182415,
     "modelInstanceId": 161550,
     "sourceId": 189483,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 182449,
     "modelInstanceId": 162762,
     "sourceId": 190956,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 182415,
     "modelInstanceId": 163845,
     "sourceId": 192178,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 182415,
     "modelInstanceId": 163854,
     "sourceId": 192189,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 182415,
     "modelInstanceId": 164149,
     "sourceId": 192507,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 182415,
     "modelInstanceId": 164257,
     "sourceId": 192627,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 182415,
     "modelInstanceId": 164261,
     "sourceId": 192631,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 926.920359,
   "end_time": "2024-12-12T03:45:22.324918",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-12T03:29:55.404559",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
