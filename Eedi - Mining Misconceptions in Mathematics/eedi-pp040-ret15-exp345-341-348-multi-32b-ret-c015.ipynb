{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7a526a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-13T01:53:04.803649Z",
     "iopub.status.busy": "2024-12-13T01:53:04.803394Z",
     "iopub.status.idle": "2024-12-13T01:55:22.305847Z",
     "shell.execute_reply": "2024-12-13T01:55:22.304941Z"
    },
    "papermill": {
     "duration": 137.51757,
     "end_time": "2024-12-13T01:55:22.307960",
     "exception": false,
     "start_time": "2024-12-13T01:53:04.790390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n",
      "Found existing installation: torchvision 0.19.0\r\n",
      "Uninstalling torchvision-0.19.0:\r\n",
      "  Successfully uninstalled torchvision-0.19.0\r\n",
      "Found existing installation: torchaudio 2.4.0\r\n",
      "Uninstalling torchaudio-2.4.0:\r\n",
      "  Successfully uninstalled torchaudio-2.4.0\r\n",
      "Looking in links: /kaggle/input/wheels-torch-cu121\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/torch-2.5.1+cu121-cp310-cp310-linux_x86_64.whl\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/torchvision-0.20.1+cu121-cp310-cp310-linux_x86_64.whl\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/torchaudio-2.5.1+cu121-cp310-cp310-linux_x86_64.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/sympy-1.13.1-py3-none-any.whl (from torch)\r\n",
      "Processing /kaggle/input/wheels-torch-cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from nvidia-cusolver-cu12==11.4.5.107->torch)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\r\n",
      "  Attempting uninstall: sympy\r\n",
      "    Found existing installation: sympy 1.13.3\r\n",
      "    Uninstalling sympy-1.13.3:\r\n",
      "      Successfully uninstalled sympy-1.13.3\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastai 2.7.17 requires torch<2.5,>=1.10, but you have torch 2.5.1+cu121 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio --no-index --find-links=/kaggle/input/wheels-torch-cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8af7eb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:55:22.332664Z",
     "iopub.status.busy": "2024-12-13T01:55:22.332400Z",
     "iopub.status.idle": "2024-12-13T01:55:36.438872Z",
     "shell.execute_reply": "2024-12-13T01:55:36.437800Z"
    },
    "papermill": {
     "duration": 14.121413,
     "end_time": "2024-12-13T01:55:36.441267",
     "exception": false,
     "start_time": "2024-12-13T01:55:22.319854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/pip-eedi\r\n",
      "Processing /kaggle/input/pip-eedi/peft-0.12.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\r\n",
      "Processing /kaggle/input/pip-eedi/accelerate-1.0.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/pip-eedi/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.1.105)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\r\n",
      "Installing collected packages: bitsandbytes, accelerate, peft\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 0.34.2\r\n",
      "    Uninstalling accelerate-0.34.2:\r\n",
      "      Successfully uninstalled accelerate-0.34.2\r\n",
      "Successfully installed accelerate-1.0.1 bitsandbytes-0.43.1 peft-0.12.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install peft accelerate bitsandbytes \\\n",
    "    -U --no-index --find-links /kaggle/input/pip-eedi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db87509a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:55:36.467358Z",
     "iopub.status.busy": "2024-12-13T01:55:36.466787Z",
     "iopub.status.idle": "2024-12-13T01:56:09.841457Z",
     "shell.execute_reply": "2024-12-13T01:56:09.840585Z"
    },
    "papermill": {
     "duration": 33.389658,
     "end_time": "2024-12-13T01:56:09.843395",
     "exception": false,
     "start_time": "2024-12-13T01:55:36.453737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "gpustat 1.0.0 requires nvidia-ml-py<=11.495.46,>=11.450.129, but you have nvidia-ml-py 12.560.30 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --no-index --find-links=/kaggle/input/wheels-vllm-0-6-4-post1 vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc58759a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:56:09.869570Z",
     "iopub.status.busy": "2024-12-13T01:56:09.869237Z",
     "iopub.status.idle": "2024-12-13T01:56:34.844254Z",
     "shell.execute_reply": "2024-12-13T01:56:34.843071Z"
    },
    "papermill": {
     "duration": 24.990364,
     "end_time": "2024-12-13T01:56:34.846464",
     "exception": false,
     "start_time": "2024-12-13T01:56:09.856100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/gte-large-en-v15-save /kaggle/working/\n",
    "!cp -r /kaggle/input/scripts-for-bge-weights/configuration.py /kaggle/working/gte-large-en-v15-save/alibaba_combined_model\n",
    "!cp -r /kaggle/input/scripts-for-bge-weights/modeling.py /kaggle/working/gte-large-en-v15-save/alibaba_combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706f2241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:56:34.874118Z",
     "iopub.status.busy": "2024-12-13T01:56:34.873119Z",
     "iopub.status.idle": "2024-12-13T01:56:34.879825Z",
     "shell.execute_reply": "2024-12-13T01:56:34.879263Z"
    },
    "papermill": {
     "duration": 0.022061,
     "end_time": "2024-12-13T01:56:34.881442",
     "exception": false,
     "start_time": "2024-12-13T01:56:34.859381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open the file in read+write mode\n",
    "with open('/kaggle/working/gte-large-en-v15-save/alibaba_combined_model/config.json', 'r+') as file:\n",
    "    # Load the existing data\n",
    "    data = json.load(file)\n",
    "    \n",
    "    # Update the data with new 'auto_map' entries\n",
    "    data['auto_map'] = {\n",
    "        \"AutoConfig\": \"configuration.NewConfig\",\n",
    "        \"AutoModel\": \"modeling.NewModel\",\n",
    "        \"AutoModelForMaskedLM\": \"modeling.NewForMaskedLM\",\n",
    "        \"AutoModelForMultipleChoice\": \"modeling.NewForMultipleChoice\",\n",
    "        \"AutoModelForQuestionAnswering\": \"modeling.NewForQuestionAnswering\",\n",
    "        \"AutoModelForSequenceClassification\": \"modeling.NewForSequenceClassification\",\n",
    "        \"AutoModelForTokenClassification\": \"modeling.NewForTokenClassification\"\n",
    "    }\n",
    "    \n",
    "    # Move the file pointer to the beginning\n",
    "    file.seek(0)\n",
    "    \n",
    "    # Write the updated data back to the file\n",
    "    json.dump(data, file, indent=4)\n",
    "    \n",
    "    # Truncate the file in case the new content is shorter\n",
    "    file.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a32f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:56:34.908238Z",
     "iopub.status.busy": "2024-12-13T01:56:34.908003Z",
     "iopub.status.idle": "2024-12-13T01:56:34.911778Z",
     "shell.execute_reply": "2024-12-13T01:56:34.911146Z"
    },
    "papermill": {
     "duration": 0.019311,
     "end_time": "2024-12-13T01:56:34.913256",
     "exception": false,
     "start_time": "2024-12-13T01:56:34.893945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/working/gte-large-en-v15-save/alibaba_combined_model/config.json', 'r+') as file:\n",
    "    # Load the existing data\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e309b7",
   "metadata": {
    "papermill": {
     "duration": 0.011896,
     "end_time": "2024-12-13T01:56:34.937239",
     "exception": false,
     "start_time": "2024-12-13T01:56:34.925343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3e734b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:56:34.962402Z",
     "iopub.status.busy": "2024-12-13T01:56:34.961915Z",
     "iopub.status.idle": "2024-12-13T01:56:57.035797Z",
     "shell.execute_reply": "2024-12-13T01:56:57.035099Z"
    },
    "papermill": {
     "duration": 22.088652,
     "end_time": "2024-12-13T01:56:57.037931",
     "exception": false,
     "start_time": "2024-12-13T01:56:34.949279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# Libraries\n",
    "# =================================\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from transformers import BitsAndBytesConfig\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch import nn, Tensor\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a349ba9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:56:57.064466Z",
     "iopub.status.busy": "2024-12-13T01:56:57.063804Z",
     "iopub.status.idle": "2024-12-13T01:56:57.068184Z",
     "shell.execute_reply": "2024-12-13T01:56:57.067361Z"
    },
    "papermill": {
     "duration": 0.019561,
     "end_time": "2024-12-13T01:56:57.069995",
     "exception": false,
     "start_time": "2024-12-13T01:56:57.050434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =================================\n",
    "# Constants\n",
    "# =================================\n",
    "DATA_DIR = Path(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics\")\n",
    "TEST_PATH = DATA_DIR / \"test.csv\"\n",
    "MISCONCEPTION_PATH = DATA_DIR / \"misconception_mapping.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a1b99d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:56:57.099130Z",
     "iopub.status.busy": "2024-12-13T01:56:57.098892Z",
     "iopub.status.idle": "2024-12-13T01:56:57.103606Z",
     "shell.execute_reply": "2024-12-13T01:56:57.102876Z"
    },
    "papermill": {
     "duration": 0.020608,
     "end_time": "2024-12-13T01:56:57.105086",
     "exception": false,
     "start_time": "2024-12-13T01:56:57.084478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =================================\n",
    "# Settings\n",
    "# =================================\n",
    "\n",
    "# =================================\n",
    "# LLM generate\n",
    "# =================================\n",
    "llm_generate_model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "\n",
    "llm_generate_prompt_template = \"\"\"You are an expert in mathematics. \n",
    "Refer to the examples below to identify and describe the misconception that led to the incorrect answer. \n",
    "Example1 \n",
    "ConstructName: Recognise and use efficient methods for mental multiplication \n",
    "SubjectName: Mental Multiplication and Division \n",
    "Math problem: Tom and Katie are discussing ways to calculate \\\\( 21 \\\\times 12 \\\\) mentally. Tom does \\\\( 12 \\\\times 7 \\\\) and then multiplies his answer by \\\\( 3 \\\\); Katie does \\\\( 21 \\\\times 6 \\\\) and then doubles her answer. Who would get the correct answer? \n",
    "Incorrect answer: Only Katie \n",
    "Misconception: Does not correctly apply the distributive property of multiplication \n",
    "\n",
    "Example2 \n",
    "ConstructName: Multiply a decimal by an integer \n",
    "SubjectName: Mental Multiplication and Division \n",
    "Math problem: \\\\( 9.4 \\\\times 50= \\\\) \n",
    "Incorrect answer: \\\\( 4700 \\\\) \n",
    "Misconception: When multiplying a decimal by an integer, ignores decimal point and just multiplies the digits \n",
    "\n",
    "ConstructName: {ConstructName} \n",
    "SubjectName: {SubjectName}\n",
    "Math problem: {QuestionText} \n",
    "Incorrect answer: {AnswerText} \n",
    "Misconception: \n",
    "\"\"\"\n",
    "\n",
    "                   \n",
    "def llm_generate_make_prompt(row):\n",
    "    \"\"\"\n",
    "    Generate a prompt based on the given row and template version.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": llm_generate_prompt_template.format(\n",
    "                            ConstructName=row[\"ConstructName\"],\n",
    "                            SubjectName=row[\"SubjectName\"],\n",
    "                            QuestionText=row[\"QuestionText\"],\n",
    "                            AnswerText=row[\"AnswerText\"],\n",
    "                            )}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4b21863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:56:57.130049Z",
     "iopub.status.busy": "2024-12-13T01:56:57.129793Z",
     "iopub.status.idle": "2024-12-13T01:56:57.133616Z",
     "shell.execute_reply": "2024-12-13T01:56:57.132973Z"
    },
    "papermill": {
     "duration": 0.018077,
     "end_time": "2024-12-13T01:56:57.135158",
     "exception": false,
     "start_time": "2024-12-13T01:56:57.117081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =================================\n",
    "# retrieval\n",
    "# =================================\n",
    "tokenizer_path=\"/kaggle/input/baai-bge-large-en-v15-save/baai-bge-large-en-v15\"\n",
    "baai_bge_large_path = \"/kaggle/input/baai-bge-large-en-v15-save/baai-bge-large-en-v15\"\n",
    "baai_bge_base_path = \"/kaggle/input/baai-bge-base-en-v15-save/baai-bge-base-en-v15\"\n",
    "gte_large_path = \"/kaggle/working/gte-large-en-v15-save/alibaba_combined_model\"\n",
    "candidate_first = 25\n",
    "candidate_first_test = 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb8df370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:56:57.159832Z",
     "iopub.status.busy": "2024-12-13T01:56:57.159588Z",
     "iopub.status.idle": "2024-12-13T01:56:57.163259Z",
     "shell.execute_reply": "2024-12-13T01:56:57.162653Z"
    },
    "papermill": {
     "duration": 0.017804,
     "end_time": "2024-12-13T01:56:57.164837",
     "exception": false,
     "start_time": "2024-12-13T01:56:57.147033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =================================\n",
    "# Retrieval Functions1\n",
    "# =================================\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "\n",
    "task = 'Given a math problem statement and an incorrect answer as a query, retrieve relevant passages that identify and explain the nature of the error.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa6e10b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:56:57.189686Z",
     "iopub.status.busy": "2024-12-13T01:56:57.189468Z",
     "iopub.status.idle": "2024-12-13T01:56:57.194286Z",
     "shell.execute_reply": "2024-12-13T01:56:57.193578Z"
    },
    "papermill": {
     "duration": 0.018985,
     "end_time": "2024-12-13T01:56:57.195850",
     "exception": false,
     "start_time": "2024-12-13T01:56:57.176865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =================================\n",
    "# 3rd stage\n",
    "# =================================\n",
    "tokenizer_3rd_path = \"/kaggle/input/qwen-qwen2-5-32b-instruct-gptq-int4/Qwen2.5-32B-Instruct-GPTQ-Int4\"\n",
    "prompt_template_3rd = \"\"\"\n",
    "You are a Mathematics teacher. Your task is to determine if the incorrect AnswerText aligns with the provided Misconception for the given Question about {ConstructName} ({SubjectName}).\n",
    "\n",
    "Return \"Yes\" if the incorrect AnswerText reflects the Misconception, otherwise return \"No\". Do not provide explanations.\n",
    "\n",
    "Answer \"Yes\" or \"No\" only.\n",
    "- Question: {QuestionText}\n",
    "- Correct Answer: {CorrectAnswerText}\n",
    "- Incorrect Answer: {AnswerText}\n",
    "- Misconception: {MisconceptionName}\n",
    "\"\"\"\n",
    "\n",
    "def make_prompt_3rd(row,tokenizer,prompt_template):\n",
    "    \"\"\"\n",
    "    Generate a prompt based on the given row and template version.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt_template.format(\n",
    "        ConstructName=row[\"ConstructName\"],\n",
    "            AnswerText=row[\"AnswerText\"],\n",
    "            CorrectAnswerText=row[\"CorrectAnswerText\"],\n",
    "            SubjectName=row[\"SubjectName\"],\n",
    "            QuestionText=row[\"QuestionText\"],\n",
    "            MisconceptionName=row[\"MisconceptionName\"]\n",
    "    )}]\n",
    "    return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70470cc6",
   "metadata": {
    "papermill": {
     "duration": 0.011692,
     "end_time": "2024-12-13T01:56:57.219787",
     "exception": false,
     "start_time": "2024-12-13T01:56:57.208095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d4f1373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:56:57.244667Z",
     "iopub.status.busy": "2024-12-13T01:56:57.244447Z",
     "iopub.status.idle": "2024-12-13T01:56:57.248634Z",
     "shell.execute_reply": "2024-12-13T01:56:57.247848Z"
    },
    "papermill": {
     "duration": 0.018452,
     "end_time": "2024-12-13T01:56:57.250123",
     "exception": false,
     "start_time": "2024-12-13T01:56:57.231671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ===================\n",
    "# # 2nd\n",
    "# # ===================\n",
    "\n",
    "# def make_prompt(row,prompt_template):\n",
    "#     \"\"\"\n",
    "#     Generate a prompt based on the given row and template version.\n",
    "#     \"\"\"\n",
    "#     return prompt_template.format(\n",
    "#         ConstructName=row[\"ConstructName\"],\n",
    "#         AnswerText=row[\"AnswerText\"],\n",
    "#         SubjectName=row[\"SubjectName\"],\n",
    "#         QuestionText=row[\"QuestionText\"],\n",
    "#         MisconceptionName=row[\"MisconceptionName\"]\n",
    "#     )\n",
    "\n",
    "# def tokenize(\n",
    "#     tokenizer, prompt, max_length=max_length_2nd_stage \n",
    "#     ):\n",
    "#     prompt = [i for i in prompt]\n",
    "#     tokenized = tokenizer(prompt, max_length=max_length, truncation=True, padding=False)\n",
    "#     input_ids = tokenized.input_ids\n",
    "#     attention_mask = tokenized.attention_mask\n",
    "#     return input_ids, attention_mask\n",
    "\n",
    "# @torch.no_grad()\n",
    "# @torch.cuda.amp.autocast()\n",
    "# def inference(df, model, device, batch_size=batch_size_2nd_stage, max_length=max_length_2nd_stage ):\n",
    "#     pred_list = []\n",
    "#     for start_idx in tqdm(range(0, len(df), batch_size)):\n",
    "#         end_idx = min(start_idx + batch_size, len(df))\n",
    "#         tmp = df.iloc[start_idx:end_idx]\n",
    "#         input_ids = tmp[\"input_ids\"].to_list()\n",
    "#         attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "#         inputs = pad_without_fast_tokenizer_warning(\n",
    "#             tokenizer,\n",
    "#             {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "#             padding=\"longest\",\n",
    "#             pad_to_multiple_of=None,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "#         outputs = model(**inputs.to(device))\n",
    "#         outputs.logits.detach().cpu().numpy()\n",
    "#         pred_list.extend(outputs.logits.detach().cpu().numpy().reshape(-1))\n",
    "    \n",
    "#     df[\"pred\"] = pred_list\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f89f95a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:56:57.275027Z",
     "iopub.status.busy": "2024-12-13T01:56:57.274787Z",
     "iopub.status.idle": "2024-12-13T01:56:57.315169Z",
     "shell.execute_reply": "2024-12-13T01:56:57.314128Z"
    },
    "papermill": {
     "duration": 0.054353,
     "end_time": "2024-12-13T01:56:57.316512",
     "exception": true,
     "start_time": "2024-12-13T01:56:57.262159",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# main\n",
    "# ============================\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "misconception = pd.read_csv(MISCONCEPTION_PATH)\n",
    "if len(test) == 3:\n",
    "    sample_submission = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/sample_submission.csv\")\n",
    "    sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "    import sys\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f529aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:39:22.254193Z",
     "iopub.status.busy": "2024-12-12T08:39:22.253816Z",
     "iopub.status.idle": "2024-12-12T08:39:22.830132Z",
     "shell.execute_reply": "2024-12-12T08:39:22.829163Z",
     "shell.execute_reply.started": "2024-12-12T08:39:22.254161Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# llm generate\n",
    "# ============================\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_generate_model_path)\n",
    "test_pivot = []\n",
    "common_cols = ['QuestionId', 'ConstructId', 'ConstructName', 'SubjectId',\n",
    "               'SubjectName', 'CorrectAnswer', 'QuestionText']\n",
    "for i in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    test_ = test.copy()\n",
    "    test_ = test_[common_cols + [f\"Answer{i}Text\"]]\n",
    "    test_ = test_.rename({f\"Answer{i}Text\": \"AnswerText\"}, axis=1)\n",
    "    test_[\"ans\"] = i\n",
    "    test_pivot.append(test_)\n",
    "test_pivot = pd.concat(test_pivot).reset_index(drop=True)\n",
    "test_pivot = test_pivot[test_pivot[\"CorrectAnswer\"] != test_pivot[\"ans\"]].reset_index(drop=True)\n",
    "test_pivot[\"prompt\"] = test_pivot.apply(\n",
    "    lambda row: llm_generate_make_prompt(row), axis=1)\n",
    "text_list = []\n",
    "from tqdm import tqdm\n",
    "for p in tqdm(test_pivot[\"prompt\"]):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        p,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    text_list.append(text)\n",
    "test_pivot[\"text\"] = text_list\n",
    "test_pivot = test_pivot.sort_values(by=\"QuestionId\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89f2c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:39:22.832034Z",
     "iopub.status.busy": "2024-12-12T08:39:22.831580Z",
     "iopub.status.idle": "2024-12-12T08:39:22.898079Z",
     "shell.execute_reply": "2024-12-12T08:39:22.897278Z",
     "shell.execute_reply.started": "2024-12-12T08:39:22.831983Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pivot.to_parquet(\"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa378a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:39:22.900036Z",
     "iopub.status.busy": "2024-12-12T08:39:22.899393Z",
     "iopub.status.idle": "2024-12-12T08:39:22.907136Z",
     "shell.execute_reply": "2024-12-12T08:39:22.905992Z",
     "shell.execute_reply.started": "2024-12-12T08:39:22.899990Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile run_vllm.py\n",
    "\n",
    "import re\n",
    "import vllm\n",
    "import pandas as pd\n",
    "import gc\n",
    "import torch\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.90, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=1028,\n",
    "    disable_log_stats=True,\n",
    "    enable_prefix_caching=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "train_pivot = pd.read_parquet(\"test.parquet\")\n",
    "\n",
    "responses = llm.generate(\n",
    "    train_pivot[\"text\"].values,\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  # Number of output sequences to return for each prompt.\n",
    "        top_p=0.8,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "        temperature=0,  # randomness of the sampling\n",
    "        seed=777, # Seed for reprodicibility\n",
    "        skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "        max_tokens=160,  # Maximum number of tokens to generate per output sequence.\n",
    "    ),\n",
    "    use_tqdm=True\n",
    ")\n",
    "del llm\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "responses = [x.outputs[0].text for x in responses]\n",
    "train_pivot[\"llmMisconception\"] = responses\n",
    "train_pivot.to_csv(\"test_add_text.csv\", index=False)\n",
    "del responses\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee172ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:39:22.908511Z",
     "iopub.status.busy": "2024-12-12T08:39:22.908252Z",
     "iopub.status.idle": "2024-12-12T08:43:25.494818Z",
     "shell.execute_reply": "2024-12-12T08:43:25.493540Z",
     "shell.execute_reply.started": "2024-12-12T08:39:22.908485Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python run_vllm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd597caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:43:25.496778Z",
     "iopub.status.busy": "2024-12-12T08:43:25.496446Z",
     "iopub.status.idle": "2024-12-12T08:43:25.998574Z",
     "shell.execute_reply": "2024-12-12T08:43:25.997470Z",
     "shell.execute_reply.started": "2024-12-12T08:43:25.496746Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930ca2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:43:26.000375Z",
     "iopub.status.busy": "2024-12-12T08:43:25.999950Z",
     "iopub.status.idle": "2024-12-12T08:43:26.057016Z",
     "shell.execute_reply": "2024-12-12T08:43:26.056258Z",
     "shell.execute_reply.started": "2024-12-12T08:43:26.000334Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# retrieval \n",
    "# ============================\n",
    "\n",
    "# preprocess\n",
    "test_pivot = []\n",
    "common_cols = ['QuestionId', 'ConstructId', 'ConstructName', 'SubjectId',\n",
    "       'SubjectName', 'CorrectAnswer', 'QuestionText']\n",
    "for i in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "    test_ = test.copy()\n",
    "    test_ = test_[common_cols + [f\"Answer{i}Text\"]].reset_index(drop=True)\n",
    "    test_ = test_.rename({f\"Answer{i}Text\":\"AnswerText\"},axis=1)\n",
    "    test_[\"ans\"] = i\n",
    "    test_pivot.append(test_)\n",
    "test_pivot =  pd.concat(test_pivot).reset_index(drop=True)\n",
    "\n",
    "# add correct answer\n",
    "test_pivot_correct_ans = test_pivot[\n",
    "    test_pivot[\"CorrectAnswer\"] == test_pivot[\"ans\"]].reset_index(drop=True)\n",
    "test_pivot_correct_ans = test_pivot_correct_ans[[\n",
    "    \"QuestionId\", \"AnswerText\"]].reset_index(drop=True)\n",
    "test_pivot_correct_ans.columns = [\"QuestionId\", \"CorrectAnswerText\"]\n",
    "test_pivot = test_pivot.merge(test_pivot_correct_ans,how=\"left\",on=\"QuestionId\")\n",
    "\n",
    "# llmoutput\n",
    "llm_text = pd.read_csv(\"test_add_text.csv\")\n",
    "test_pivot = test_pivot.merge(llm_text[[\"QuestionId\",\"ans\",\"llmMisconception\"]],how=\"left\",on=[\"QuestionId\",\"ans\"])\n",
    "test_pivot = test_pivot[test_pivot[\"CorrectAnswer\"] != test_pivot[\"ans\"]].reset_index(drop=True)\n",
    "\n",
    "# BAAI/bge-large-en-v1.5\n",
    "# Alibaba-NLP/gte-large-en-v1.5\n",
    "test_pivot[\"all_text\"] = '<Construct> ' + test_pivot['ConstructName'] + \\\n",
    "                         ' <Subject> ' + test_pivot['SubjectName'] + \\\n",
    "                         ' <Question> ' + test_pivot['QuestionText'] + \\\n",
    "                         ' <Answer> ' + test_pivot['AnswerText'] + \\\n",
    "                         ' <LLM OUTPUT> ' +  test_pivot['llmMisconception']\n",
    "\n",
    "# Salesforce/SFR-Embedding-2_R\n",
    "test_pivot[\"all_text_SFR\"] = ' <Question> ' + test_pivot['QuestionText'] + \\\n",
    "    ' <Correct Answer> ' + test_pivot['CorrectAnswerText'] + \\\n",
    "    ' <Incorrect Answer> ' + test_pivot['AnswerText'] + \\\n",
    "    ' <Construct> ' + test_pivot['ConstructName'] + \\\n",
    "    ' <Subject> ' + test_pivot['SubjectName'] + \\\n",
    "    ' <LLMOutput> ' + test_pivot['llmMisconception']\n",
    "\n",
    "all_text_SFR_list = []\n",
    "for t in test_pivot[\"all_text_SFR\"].values:\n",
    "    all_text_SFR_list.append(get_detailed_instruct(task, t))\n",
    "test_pivot[\"all_text_SFR\"] = all_text_SFR_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89787d59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:43:26.058596Z",
     "iopub.status.busy": "2024-12-12T08:43:26.058260Z",
     "iopub.status.idle": "2024-12-12T08:43:26.064599Z",
     "shell.execute_reply": "2024-12-12T08:43:26.063574Z",
     "shell.execute_reply.started": "2024-12-12T08:43:26.058564Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pivot[\"num\"] = np.arange(len(test_pivot))\n",
    "misconception[\"num\"] = np.arange(len(misconception))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586189e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:43:26.069901Z",
     "iopub.status.busy": "2024-12-12T08:43:26.069599Z",
     "iopub.status.idle": "2024-12-12T08:43:26.872704Z",
     "shell.execute_reply": "2024-12-12T08:43:26.871701Z",
     "shell.execute_reply.started": "2024-12-12T08:43:26.069872Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# qwen14b\n",
    "# ================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/m/qwen-lm/qwen2.5/transformers/14b-instruct/1\")\n",
    "token_len = []\n",
    "for t in test_pivot[\"all_text_SFR\"]:\n",
    "    token_len.append(len(tokenizer(t)[\"input_ids\"]))\n",
    "test_pivot[\"token_len\"] = token_len\n",
    "token_len = []\n",
    "for t in misconception[\"MisconceptionName\"]:\n",
    "    token_len.append(len(tokenizer(t)[\"input_ids\"]))\n",
    "misconception[\"token_len\"] = token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ed61b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:43:26.874395Z",
     "iopub.status.busy": "2024-12-12T08:43:26.873945Z",
     "iopub.status.idle": "2024-12-12T08:43:26.883210Z",
     "shell.execute_reply": "2024-12-12T08:43:26.882133Z",
     "shell.execute_reply.started": "2024-12-12T08:43:26.874352Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pivot = test_pivot.sort_values(by=\"token_len\").reset_index(drop=True)\n",
    "misconception = misconception.sort_values(by=\"token_len\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a798ed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:43:26.884741Z",
     "iopub.status.busy": "2024-12-12T08:43:26.884370Z",
     "iopub.status.idle": "2024-12-12T08:43:26.893907Z",
     "shell.execute_reply": "2024-12-12T08:43:26.893038Z",
     "shell.execute_reply.started": "2024-12-12T08:43:26.884711Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pivot1 = test_pivot.iloc[0::2].copy()\n",
    "test_pivot2 = test_pivot.iloc[1::2].copy()\n",
    "misconception1 = misconception.iloc[0::2].copy()\n",
    "misconception2 = misconception.iloc[1::2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7146dc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:43:26.895404Z",
     "iopub.status.busy": "2024-12-12T08:43:26.895012Z",
     "iopub.status.idle": "2024-12-12T08:43:26.927340Z",
     "shell.execute_reply": "2024-12-12T08:43:26.926455Z",
     "shell.execute_reply.started": "2024-12-12T08:43:26.895368Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save\n",
    "test_pivot.to_csv('test2.csv', index=False)\n",
    "test_pivot1.to_csv(\"test2_1.csv\",index=False)\n",
    "test_pivot2.to_csv(\"test2_2.csv\",index=False)\n",
    "misconception.to_csv('misconception2.csv', index=False)\n",
    "misconception1.to_csv(\"misconception2_1.csv\",index=False)\n",
    "misconception2.to_csv(\"misconception2_2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f835a3c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:43:26.928848Z",
     "iopub.status.busy": "2024-12-12T08:43:26.928490Z",
     "iopub.status.idle": "2024-12-12T08:43:26.950544Z",
     "shell.execute_reply": "2024-12-12T08:43:26.949614Z",
     "shell.execute_reply.started": "2024-12-12T08:43:26.928808Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe757bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:43:26.951946Z",
     "iopub.status.busy": "2024-12-12T08:43:26.951680Z",
     "iopub.status.idle": "2024-12-12T08:43:26.962071Z",
     "shell.execute_reply": "2024-12-12T08:43:26.961116Z",
     "shell.execute_reply.started": "2024-12-12T08:43:26.951919Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "misconception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798944a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:43:26.963748Z",
     "iopub.status.busy": "2024-12-12T08:43:26.963447Z",
     "iopub.status.idle": "2024-12-12T08:43:26.976385Z",
     "shell.execute_reply": "2024-12-12T08:43:26.975543Z",
     "shell.execute_reply.started": "2024-12-12T08:43:26.963722Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile qwen_inference.py\n",
    "# Qwen/Qwen2.5-14B-Instruct\n",
    "# ==========================\n",
    "# exp345\n",
    "# ==========================\n",
    "import argparse\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "#%env TOKENIZERS_PARALLELISM=true\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from transformers import BitsAndBytesConfig\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch import nn, Tensor\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# ==========================\n",
    "# settings\n",
    "# ==========================\n",
    "qwen_14b_path = \"/kaggle/input/m/qwen-lm/qwen2.5/transformers/14b-instruct/1\"\n",
    "#test_path = \"test2.parquet\"\n",
    "#misconception_path = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\"\n",
    "device = torch.device('cuda')\n",
    "class CFG:\n",
    "    exp=\"345\"\n",
    "    max_len=384\n",
    "    batch_size=6\n",
    "    tokenizer_path=qwen_14b_path\n",
    "    model_path=f\"/kaggle/input/eedi-exp{exp}\"\n",
    "    n_fold=[0,1,2,3,4]\n",
    "    lora_r=32\n",
    "    lora_alpha=64\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Qwen/Qwen2.5-14B-Instruct\n",
    "# ==============================\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "    \n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference_2nd_stage(model, sentences, misconceptions,tokenizer,\n",
    "                        device, batch_size=8, max_length=384):\n",
    "    sentences_emb = []\n",
    "    for start_index in range(0, len(sentences), batch_size):\n",
    "        sentences_batch = sentences[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().astype(np.float32)\n",
    "        sentences_emb.append(embeddings)\n",
    "    sentences_emb = np.concatenate(sentences_emb,axis=0)\n",
    "    misconception_emb = []\n",
    "    for start_index in range(0, len(misconceptions), batch_size):\n",
    "        sentences_batch = misconceptions[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().astype(np.float32)\n",
    "        misconception_emb.append(embeddings)\n",
    "    \n",
    "    misconception_emb = np.concatenate(misconception_emb,axis=0)\n",
    "    return sentences_emb,misconception_emb\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('test_path')\n",
    "    parser.add_argument('misconception_path')\n",
    "    parser.add_argument('result_file_test')\n",
    "    parser.add_argument('result_file_misconception')\n",
    "    args = parser.parse_args()\n",
    "    test_emb_all = []\n",
    "    misconception_emb_all = []\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.tokenizer_path)\n",
    "    test_pivot = pd.read_csv(args.test_path )\n",
    "    misconception = pd.read_csv(args.misconception_path)\n",
    "    for fold in tqdm(CFG.n_fold):\n",
    "        lora_path = CFG.model_path + f\"/fold{fold}/adapter.bin\"\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"fp4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "        model = AutoModel.from_pretrained(CFG.tokenizer_path, \n",
    "                                          quantization_config=bnb_config,\n",
    "                                          device_map=device)\n",
    "        config = LoraConfig(\n",
    "            r=CFG.lora_r,\n",
    "            lora_alpha=CFG.lora_alpha,\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "            ],\n",
    "            bias=\"none\",\n",
    "            lora_dropout=0.05,  # Conventional\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, config)\n",
    "        d = torch.load(lora_path, map_location=model.device)\n",
    "        model.load_state_dict(d, strict=False)\n",
    "        model = model.eval()\n",
    "        sentences = list(test_pivot['all_text_SFR'].values)\n",
    "        misconception_list = list(misconception['MisconceptionName'].values)\n",
    "        sentences_emb,misconception_emb = inference_2nd_stage(model, sentences, misconception_list,tokenizer,\n",
    "                            device)\n",
    "        test_emb_all.append(sentences_emb)\n",
    "        misconception_emb_all.append(misconception_emb)\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    misconception_emb_all = np.concatenate(misconception_emb_all,axis=1)\n",
    "    test_emb_all = np.concatenate(test_emb_all,axis=1)\n",
    "    np.save(args.result_file_test,test_emb_all)\n",
    "    np.save(args.result_file_misconception,misconception_emb_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01392979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T08:43:26.977936Z",
     "iopub.status.busy": "2024-12-12T08:43:26.977576Z",
     "iopub.status.idle": "2024-12-12T09:05:59.553403Z",
     "shell.execute_reply": "2024-12-12T09:05:59.550800Z",
     "shell.execute_reply.started": "2024-12-12T08:43:26.977892Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!(CUDA_VISIBLE_DEVICES=0 TOKENIZERS_PARALLELISM=false python qwen_inference.py test2_1.csv misconception2_1.csv qwen_test_emb_all1.npy qwen_misconception_emb_all1.npy \\\n",
    " &CUDA_VISIBLE_DEVICES=1 TOKENIZERS_PARALLELISM=false python qwen_inference.py test2_2.csv misconception2_2.csv qwen_test_emb_all2.npy qwen_misconception_emb_all2.npy \\\n",
    " & wait \\\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da942a5d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 32b retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75978a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T09:05:59.557501Z",
     "iopub.status.busy": "2024-12-12T09:05:59.556429Z",
     "iopub.status.idle": "2024-12-12T09:07:50.893875Z",
     "shell.execute_reply": "2024-12-12T09:07:50.892181Z",
     "shell.execute_reply.started": "2024-12-12T09:05:59.557460Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config 書き換え用に一旦移す\n",
    "!mkdir /kaggle/temp\n",
    "!cp -r /kaggle/input/m/qwen-lm/qwen2.5/transformers/32b-instruct-gptq-int4/1 /kaggle/temp/qwen2.5-32b-instruct-gptq-int4\n",
    "!ls /kaggle/temp/qwen2.5-32b-instruct-gptq-int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86acb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T09:07:50.897971Z",
     "iopub.status.busy": "2024-12-12T09:07:50.897478Z",
     "iopub.status.idle": "2024-12-12T09:07:50.911449Z",
     "shell.execute_reply": "2024-12-12T09:07:50.910249Z",
     "shell.execute_reply.started": "2024-12-12T09:07:50.897916Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile qwen_inference.py\n",
    "\n",
    "import re\n",
    "import vllm\n",
    "import pandas as pd\n",
    "import gc\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from contextlib import ContextDecorator\n",
    "import os\n",
    "misconception_path = \"misconception2.csv\" #\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\"\n",
    "\n",
    "class CFG:\n",
    "    exp=\"009\"\n",
    "    max_len=384\n",
    "    batch_size=6\n",
    "    model_path=\"/kaggle/temp/qwen2.5-32b-instruct-gptq-int4\"\n",
    "    lora_path_dir=\"/kaggle/input/charmq-qwen2-32b-gptq-int4-convert-loras-exp012\"\n",
    "    n_fold=[0,1]\n",
    "    lora_r=32\n",
    "    lora_alpha=64\n",
    "    \n",
    "print(vllm.__version__)\n",
    "class ModifyJson(ContextDecorator):\n",
    "    def __init__(self, json_file, changes):\n",
    "        self.json_file = json_file\n",
    "        self.changes = changes\n",
    "        self.original_data = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        with open(self.json_file, \"r\") as f:\n",
    "            self.original_data = json.load(f)\n",
    "        modified_data = self.original_data.copy()\n",
    "        modified_data.update(self.changes)\n",
    "        with open(self.json_file, \"w\") as f:\n",
    "            json.dump(modified_data, f, indent=4)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        with open(self.json_file, \"w\") as f:\n",
    "            json.dump(self.original_data, f, indent=4)\n",
    "\n",
    "\n",
    "def vllm_init():\n",
    "    import sys\n",
    "\n",
    "    from vllm.model_executor.layers.pooler import Pooler, PoolingType\n",
    "    from vllm.model_executor.models.qwen2 import Qwen2EmbeddingModel\n",
    "\n",
    "    class Qwen2LastPoolEmbeddingModel(Qwen2EmbeddingModel):\n",
    "        def __init__(self, *, vllm_config, prefix: str = \"\"):\n",
    "            super().__init__(vllm_config=vllm_config, prefix=prefix)\n",
    "            pooler_config = vllm_config.model_config.pooler_config\n",
    "            self._pooler = Pooler.from_config_with_defaults(\n",
    "                pooler_config, pooling_type=PoolingType.LAST, normalize=True, softmax=False\n",
    "            )\n",
    "            print(\"Qwen2LastPoolEmbeddingModel\")\n",
    "\n",
    "        @staticmethod\n",
    "        def hello():\n",
    "            print(\"hello\")\n",
    "\n",
    "    sys.modules[\"vllm\"].model_executor.models.qwen2.Qwen2EmbeddingModel = Qwen2LastPoolEmbeddingModel\n",
    "    from vllm.model_executor.models.qwen2 import Qwen2EmbeddingModel\n",
    "\n",
    "@ModifyJson(\n",
    "    json_file=f\"{CFG.model_path}/config.json\",\n",
    "    changes={\"architectures\": [\"Qwen2Model\"]},\n",
    ")\n",
    "def inference(df):\n",
    "    import vllm\n",
    "    from vllm.lora.request import LoRARequest\n",
    "\n",
    "    vllm_init()\n",
    "    df_misconception_mapping = pd.read_csv(misconception_path)\n",
    "\n",
    "    llm = vllm.LLM(\n",
    "        CFG.model_path,\n",
    "        tensor_parallel_size=2,\n",
    "        gpu_memory_utilization=0.95,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=4096,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=CFG.lora_r,\n",
    "    )\n",
    "\n",
    "    queries = df[\"all_text_SFR\"].to_numpy() # TODO: プロンプトを変える\n",
    "    misconceptions = df_misconception_mapping[\"MisconceptionName\"].to_numpy()\n",
    "\n",
    "    test_emb_all_list = []\n",
    "    misconception_emb_all_list = []\n",
    "\n",
    "    for fold in CFG.n_fold:\n",
    "        lora_path = os.path.join(CFG.lora_path_dir, f'fold{fold}')\n",
    "        responses = llm.encode(\n",
    "            np.concatenate([queries, misconceptions]),\n",
    "            use_tqdm=True,\n",
    "            lora_request=LoRARequest(\"sql_adapter\", 1, lora_path),\n",
    "        )\n",
    "        embeddings = np.array([response.outputs.embedding for response in responses])\n",
    "        test_emb_all = embeddings[: len(queries)]\n",
    "        misconception_emb_all = embeddings[len(queries) :]\n",
    "        test_emb_all_list.append(test_emb_all)\n",
    "        misconception_emb_all_list.append(misconception_emb_all)\n",
    "\n",
    "    test_emb_all = np.concatenate(test_emb_all_list,axis=1)\n",
    "    misconception_emb_all = np.concatenate(misconception_emb_all_list,axis=1)\n",
    "    return test_emb_all, misconception_emb_all\n",
    "\n",
    "\n",
    "train_pivot = pd.read_csv(\"test2.csv\")\n",
    "test_emb_all, misconception_emb_all = inference(train_pivot)\n",
    "\n",
    "np.save(f\"qwen_misconception_emb_all_32b.npy\",misconception_emb_all)\n",
    "np.save(f\"qwen_test_emb_all_32b.npy\",test_emb_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e705c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T09:07:50.913314Z",
     "iopub.status.busy": "2024-12-12T09:07:50.912976Z",
     "iopub.status.idle": "2024-12-12T09:10:30.870243Z",
     "shell.execute_reply": "2024-12-12T09:10:30.868919Z",
     "shell.execute_reply.started": "2024-12-12T09:07:50.913281Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python qwen_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea174080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T09:10:30.872535Z",
     "iopub.status.busy": "2024-12-12T09:10:30.872156Z",
     "iopub.status.idle": "2024-12-12T09:10:32.857703Z",
     "shell.execute_reply": "2024-12-12T09:10:32.856883Z",
     "shell.execute_reply.started": "2024-12-12T09:10:30.872499Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# candidate\n",
    "# ====================\n",
    "misconception_emb_all1 = np.load(\"/kaggle/working/qwen_misconception_emb_all1.npy\")\n",
    "misconception_emb_all2 = np.load(\"/kaggle/working/qwen_misconception_emb_all2.npy\")\n",
    "test_emb_all1 = np.load(\"/kaggle/working/qwen_test_emb_all1.npy\")\n",
    "test_emb_all2 = np.load(\"/kaggle/working/qwen_test_emb_all2.npy\")\n",
    "test_pivot = pd.read_csv('test2.csv')\n",
    "misconception = pd.read_csv('misconception2.csv')\n",
    "\n",
    "test_pivot_ = pd.concat([test_pivot1,test_pivot2]).reset_index(drop=True)\n",
    "misconception_ = pd.concat([misconception1,misconception2]).reset_index(drop=True)\n",
    "misconception_emb_all = np.concatenate([misconception_emb_all1,misconception_emb_all2],axis=0)\n",
    "test_emb_all = np.concatenate([test_emb_all1,test_emb_all2],axis=0)\n",
    "del misconception_emb_all1,misconception_emb_all2,test_emb_all1,test_emb_all2\n",
    "gc.collect()\n",
    "\n",
    "test_pivot_[\"num2\"] = np.arange(len(test_pivot_))\n",
    "misconception_[\"num2\"] = np.arange(len(misconception_))\n",
    "test_pivot_ = test_pivot_.sort_values(by=\"num\").reset_index(drop=True)\n",
    "misconception_ = misconception_.sort_values(by=\"num\").reset_index(drop=True)\n",
    "misconception_emb_all1 = misconception_emb_all[misconception_[\"num2\"].values]\n",
    "test_emb_all1 = test_emb_all[test_pivot_[\"num2\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baec632",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T09:10:32.859394Z",
     "iopub.status.busy": "2024-12-12T09:10:32.858999Z",
     "iopub.status.idle": "2024-12-12T09:10:33.527095Z",
     "shell.execute_reply": "2024-12-12T09:10:33.525599Z",
     "shell.execute_reply.started": "2024-12-12T09:10:32.859361Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "misconception_emb_all = np.load(\"/kaggle/working/qwen_misconception_emb_all_32b.npy\")\n",
    "test_emb_all = np.load(\"/kaggle/working/qwen_test_emb_all_32b.npy\")\n",
    "test_pivot = pd.read_csv('test2.csv')\n",
    "misconception = pd.read_csv('misconception2.csv')\n",
    "\n",
    "test_pivot_ = pd.read_csv('test2.csv')\n",
    "misconception_ = pd.read_csv('misconception2.csv')\n",
    "\n",
    "test_pivot_[\"num2\"] = np.arange(len(test_pivot_))\n",
    "misconception_[\"num2\"] = np.arange(len(misconception_))\n",
    "test_pivot_ = test_pivot_.sort_values(by=\"num\").reset_index(drop=True)\n",
    "misconception_ = misconception_.sort_values(by=\"num\").reset_index(drop=True)\n",
    "misconception_emb_all2 = misconception_emb_all[misconception_[\"num2\"].values]\n",
    "test_emb_all2 = test_emb_all[test_pivot_[\"num2\"].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7e752",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.528245Z",
     "iopub.status.idle": "2024-12-12T09:10:33.528784Z",
     "shell.execute_reply": "2024-12-12T09:10:33.528538Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.528509Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "misconception_emb_all = np.concatenate([misconception_emb_all1, misconception_emb_all2],axis=1)\n",
    "test_emb_all = np.concatenate([test_emb_all1, test_emb_all2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a88e45",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.530615Z",
     "iopub.status.idle": "2024-12-12T09:10:33.530942Z",
     "shell.execute_reply": "2024-12-12T09:10:33.530805Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.530789Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pivot = pd.read_csv('test2.csv')\n",
    "misconception = pd.read_csv('misconception2.csv')\n",
    "\n",
    "test_pivot = test_pivot.sort_values(by=\"num\").reset_index(drop=True)\n",
    "misconception = misconception.sort_values(by=\"num\").reset_index(drop=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195945a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.532289Z",
     "iopub.status.idle": "2024-12-12T09:10:33.532655Z",
     "shell.execute_reply": "2024-12-12T09:10:33.532508Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.532489Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn = NearestNeighbors(n_neighbors=candidate_first,\n",
    "                       metric=\"cosine\")\n",
    "knn.fit(misconception_emb_all)\n",
    "dists, nears = knn.kneighbors(test_emb_all)\n",
    "#del misconception_emb_all,test_emb_all\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1582d0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.533905Z",
     "iopub.status.idle": "2024-12-12T09:10:33.534278Z",
     "shell.execute_reply": "2024-12-12T09:10:33.534131Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.534108Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================\n",
    "# not train\n",
    "# ====================\n",
    "train = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\")\n",
    "misconception_ = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "misconception_list = []\n",
    "for i in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "    misconception_list += list(train[f\"Misconception{i}Id\"].unique())\n",
    "misconception_list = pd.Series(misconception_list)\n",
    "misconception_list = misconception_list.dropna()\n",
    "misconception_list = misconception_list.astype(int).values\n",
    "\n",
    "train_misconception_list = []\n",
    "for n,m in enumerate(misconception_[\"MisconceptionId\"]):\n",
    "    if m in misconception_list:\n",
    "        train_misconception_list.append(1)\n",
    "    else:\n",
    "        train_misconception_list.append(0)\n",
    "train_misconception_list = np.array(train_misconception_list)\n",
    "\n",
    "test_misconception = misconception_[\"MisconceptionId\"].values\n",
    "test_misconception = test_misconception[train_misconception_list == 0]\n",
    "\n",
    "test_misconception_dict={}\n",
    "for n,m in enumerate(test_misconception):\n",
    "    test_misconception_dict[n] = m\n",
    "\n",
    "\n",
    "misconception_emb_all_test = misconception_emb_all[train_misconception_list == 0]\n",
    "knn = NearestNeighbors(n_neighbors=candidate_first_test,\n",
    "                       metric=\"cosine\")\n",
    "knn.fit(misconception_emb_all_test)\n",
    "_, nears_test = knn.kneighbors(test_emb_all)\n",
    "del misconception_emb_all,test_emb_all,misconception_emb_all_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c25624",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.535453Z",
     "iopub.status.idle": "2024-12-12T09:10:33.535777Z",
     "shell.execute_reply": "2024-12-12T09:10:33.535639Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.535623Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pivot[\"QuestionId_Answer\"] = test_pivot[\"QuestionId\"].astype(str) + \"_\" + test_pivot[\"ans\"].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55807d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.537481Z",
     "iopub.status.idle": "2024-12-12T09:10:33.537792Z",
     "shell.execute_reply": "2024-12-12T09:10:33.537657Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.537641Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub = []\n",
    "# for i,p in zip(test_pivot[\"QuestionId_Answer\"],nears):\n",
    "#     pred_list = ' '.join(map(str, p))\n",
    "#     sub.append([i,pred_list])\n",
    "# sub = pd.DataFrame(sub)\n",
    "# sub.columns = [\"QuestionId_Answer\",\"MisconceptionId\"]\n",
    "# sub[[\"QuestionId_Answer\",\"MisconceptionId\"]].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee05d01",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.538592Z",
     "iopub.status.idle": "2024-12-12T09:10:33.538963Z",
     "shell.execute_reply": "2024-12-12T09:10:33.538810Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.538791Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub[[\"QuestionId_Answer\",\"MisconceptionId\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44015b10",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.540290Z",
     "iopub.status.idle": "2024-12-12T09:10:33.540650Z",
     "shell.execute_reply": "2024-12-12T09:10:33.540487Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.540469Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 3rd preprocess\n",
    "# =====================\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_3rd_path)\n",
    "candidate_pivot = []\n",
    "for n,(q,a) in enumerate(zip(test_pivot[\"QuestionId\"],test_pivot[\"ans\"])):\n",
    "    candidate = pd.DataFrame()\n",
    "    candidate_ = nears[n]\n",
    "    candidate[\"MisconceptionId\"] = candidate_\n",
    "    candidate[\"QuestionId\"] = q\n",
    "    candidate[\"ans\"] = a\n",
    "    candidate_pivot.append(candidate)\n",
    "    \n",
    "for n,(q,a) in enumerate(zip(test_pivot[\"QuestionId\"],test_pivot[\"ans\"])):\n",
    "    candidate = pd.DataFrame()\n",
    "    candidate_ = nears_test[n]\n",
    "    candidate_ = [test_misconception_dict[i] for i in candidate_]\n",
    "    candidate[\"MisconceptionId\"] = candidate_\n",
    "    candidate[\"QuestionId\"] = q\n",
    "    candidate[\"ans\"] = a\n",
    "    candidate_pivot.append(candidate)\n",
    "\n",
    "candidate_pivot = pd.concat(candidate_pivot).reset_index(drop=True)\n",
    "candidate_pivot = candidate_pivot.drop_duplicates(subset = [\"MisconceptionId\",\"QuestionId\",\"ans\"]).reset_index(drop=True)\n",
    "merge_cols = [\"QuestionId\", \"ConstructName\",\n",
    "              \"SubjectName\", \"QuestionText\", \"AnswerText\", \"ans\",\"CorrectAnswerText\"]\n",
    "candidate_pivot = candidate_pivot.merge(\n",
    "    test_pivot[merge_cols], how=\"left\", on=[\"QuestionId\", \"ans\"])\n",
    "candidate_pivot = candidate_pivot.merge(\n",
    "    misconception, how=\"left\", on=\"MisconceptionId\")\n",
    "candidate_pivot[\"prompt\"] = candidate_pivot.apply(\n",
    "    lambda row: make_prompt_3rd(row,tokenizer,prompt_template_3rd), axis=1)\n",
    "candidate_pivot[\"prompt\"] = candidate_pivot[\"prompt\"] + \"Answer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e952a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.541931Z",
     "iopub.status.idle": "2024-12-12T09:10:33.542452Z",
     "shell.execute_reply": "2024-12-12T09:10:33.542220Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.542193Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidate_pivot = candidate_pivot.sort_values(by=[\"QuestionId\",\"ans\"]).reset_index(drop=True)\n",
    "candidate_pivot.to_csv(\"test3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c4f88",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.543785Z",
     "iopub.status.idle": "2024-12-12T09:10:33.544300Z",
     "shell.execute_reply": "2024-12-12T09:10:33.544037Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.544012Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidate_pivot[\"prompt\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5b724",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.546056Z",
     "iopub.status.idle": "2024-12-12T09:10:33.546564Z",
     "shell.execute_reply": "2024-12-12T09:10:33.546339Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.546313Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile run_3rd.py\n",
    "\n",
    "import vllm\n",
    "from typing import Any, Dict, List\n",
    "from transformers import LogitsProcessor\n",
    "import torch\n",
    "import pandas as pd\n",
    "from vllm.lora.request import LoRARequest\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def generate_text_vllm(prompts, tokenizer, model):\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        n=1,  # Number of output sequences to return for each prompt.\n",
    "        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "        temperature=0,  # randomness of the sampling\n",
    "        seed=777, # Seed for reprodicibility\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output.\n",
    "        max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "        logits_processors=logits_processors,\n",
    "        logprobs = 5,\n",
    "    )\n",
    "    \n",
    "    responses = model.generate(\n",
    "        prompts,\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"sql_adapter\", 1,\"/kaggle/input/exp341-347-lora-merge/exp341-347\"),\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    errors = 0    \n",
    "    \n",
    "    for i,response in enumerate(responses):\n",
    "        try:\n",
    "            x = response.outputs[0].logprobs[0]\n",
    "            logprobs = []\n",
    "            for k in KEEP:\n",
    "                if k in x:\n",
    "                    logprobs.append( math.exp(x[k].logprob) )\n",
    "                else:\n",
    "                    logprobs.append( 0 )\n",
    "                    print(f\"bad logits {i}\")\n",
    "            logprobs = np.array( logprobs )\n",
    "            #logprobs /= logprobs.sum()\n",
    "            results.append( logprobs )\n",
    "        except:\n",
    "            #print(f\"error {i}\")\n",
    "            results.append( np.array([0.5]) )\n",
    "            errors += 1    \n",
    "    \n",
    "#     response_texts = []\n",
    "#     for response in responses:\n",
    "#         # total_tokens += len(response.outputs[0].token_ids)\n",
    "#         # response_texts.append(response.outputs[0].text)\n",
    "#         response_texts.append(response.outputs[0].logprobs[0])\n",
    "    return results, errors\n",
    "    \n",
    "df = pd.read_csv(\"test3.csv\")\n",
    "prompts = df[\"prompt\"].values\n",
    "llm = vllm.LLM(\n",
    "    \"/kaggle/input/qwen-qwen2-5-32b-instruct-gptq-int4/Qwen2.5-32B-Instruct-GPTQ-Int4\", # \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "    #quantization=\"gptq\",\n",
    "    tensor_parallel_size=2, # 2, 4 \n",
    "    gpu_memory_utilization=0.95, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=3096,\n",
    "    enable_lora=True, \n",
    "    enable_prefix_caching=True,\n",
    "    max_lora_rank=32\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "choices = [\"Yes\",\"No\"]\n",
    "\n",
    "KEEP = []\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x, add_special_tokens=False)[0]\n",
    "    KEEP.append(c)\n",
    "print(f\"Force predictions to be tokens {KEEP} which are {choices}.\")\n",
    "\n",
    "class DigitLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.allowed_ids = KEEP\n",
    "        \n",
    "    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores[self.allowed_ids] += 100\n",
    "        return scores\n",
    "\n",
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "\n",
    "results, errors = generate_text_vllm(prompts, tokenizer, llm)\n",
    "print(errors)\n",
    "df[\"results\"] = results\n",
    "df.to_csv(\"test4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d973a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.548193Z",
     "iopub.status.idle": "2024-12-12T09:10:33.549094Z",
     "shell.execute_reply": "2024-12-12T09:10:33.548814Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.548783Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python run_3rd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64452f1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.550834Z",
     "iopub.status.idle": "2024-12-12T09:10:33.551384Z",
     "shell.execute_reply": "2024-12-12T09:10:33.551141Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.551114Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile run_3rd_2.py\n",
    "\n",
    "import vllm\n",
    "from typing import Any, Dict, List\n",
    "from transformers import LogitsProcessor\n",
    "import torch\n",
    "import pandas as pd\n",
    "from vllm.lora.request import LoRARequest\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def generate_text_vllm(prompts, tokenizer, model):\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        n=1,  # Number of output sequences to return for each prompt.\n",
    "        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "        temperature=0,  # randomness of the sampling\n",
    "        seed=777, # Seed for reprodicibility\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output.\n",
    "        max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "        logits_processors=logits_processors,\n",
    "        logprobs = 5,\n",
    "    )\n",
    "    \n",
    "    responses = model.generate(\n",
    "        prompts,\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"sql_adapter\", 1,\"/kaggle/input/exp348-349-lora-merge/exp348-349\"),\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    errors = 0    \n",
    "    \n",
    "    for i,response in enumerate(responses):\n",
    "        try:\n",
    "            x = response.outputs[0].logprobs[0]\n",
    "            logprobs = []\n",
    "            for k in KEEP:\n",
    "                if k in x:\n",
    "                    logprobs.append( math.exp(x[k].logprob) )\n",
    "                else:\n",
    "                    logprobs.append( 0 )\n",
    "                    print(f\"bad logits {i}\")\n",
    "            logprobs = np.array( logprobs )\n",
    "            #logprobs /= logprobs.sum()\n",
    "            results.append( logprobs )\n",
    "        except:\n",
    "            #print(f\"error {i}\")\n",
    "            results.append( np.array([0.5]) )\n",
    "            errors += 1    \n",
    "    \n",
    "#     response_texts = []\n",
    "#     for response in responses:\n",
    "#         # total_tokens += len(response.outputs[0].token_ids)\n",
    "#         # response_texts.append(response.outputs[0].text)\n",
    "#         response_texts.append(response.outputs[0].logprobs[0])\n",
    "    return results, errors\n",
    "    \n",
    "df = pd.read_csv(\"test3.csv\")\n",
    "prompts = df[\"prompt\"].values\n",
    "llm = vllm.LLM(\n",
    "    \"/kaggle/input/qwen-qwen2-5-32b-instruct-gptq-int4/Qwen2.5-32B-Instruct-GPTQ-Int4\", # \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "    #quantization=\"gptq\",\n",
    "    tensor_parallel_size=2, # 2, 4 \n",
    "    gpu_memory_utilization=0.95, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=3096,\n",
    "    enable_lora=True, \n",
    "    enable_prefix_caching=True,\n",
    "    max_lora_rank=32\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "choices = [\"Yes\",\"No\"]\n",
    "\n",
    "KEEP = []\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x, add_special_tokens=False)[0]\n",
    "    KEEP.append(c)\n",
    "print(f\"Force predictions to be tokens {KEEP} which are {choices}.\")\n",
    "\n",
    "class DigitLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.allowed_ids = KEEP\n",
    "        \n",
    "    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores[self.allowed_ids] += 100\n",
    "        return scores\n",
    "\n",
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "\n",
    "results, errors = generate_text_vllm(prompts, tokenizer, llm)\n",
    "print(errors)\n",
    "df[\"results\"] = results\n",
    "df.to_csv(\"test5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22c546",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.552852Z",
     "iopub.status.idle": "2024-12-12T09:10:33.553364Z",
     "shell.execute_reply": "2024-12-12T09:10:33.553129Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.553102Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python run_3rd_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6099de",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.555432Z",
     "iopub.status.idle": "2024-12-12T09:10:33.555908Z",
     "shell.execute_reply": "2024-12-12T09:10:33.555686Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.555661Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile run_3rd_3.py\n",
    "\n",
    "import vllm\n",
    "from typing import Any, Dict, List\n",
    "from transformers import LogitsProcessor\n",
    "import torch\n",
    "import pandas as pd\n",
    "from vllm.lora.request import LoRARequest\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def generate_text_vllm(prompts, tokenizer, model):\n",
    "    sampling_params = vllm.SamplingParams(\n",
    "        n=1,  # Number of output sequences to return for each prompt.\n",
    "        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "        temperature=0,  # randomness of the sampling\n",
    "        seed=777, # Seed for reprodicibility\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output.\n",
    "        max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "        logits_processors=logits_processors,\n",
    "        logprobs = 5,\n",
    "    )\n",
    "    \n",
    "    responses = model.generate(\n",
    "        prompts,\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"sql_adapter\", 1,\"/kaggle/input/charmq-eedi-exp015-lora-merge/charmq-exp015\"),\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    errors = 0    \n",
    "    \n",
    "    for i,response in enumerate(responses):\n",
    "        try:\n",
    "            x = response.outputs[0].logprobs[0]\n",
    "            logprobs = []\n",
    "            for k in KEEP:\n",
    "                if k in x:\n",
    "                    logprobs.append( math.exp(x[k].logprob) )\n",
    "                else:\n",
    "                    logprobs.append( 0 )\n",
    "                    print(f\"bad logits {i}\")\n",
    "            logprobs = np.array( logprobs )\n",
    "            #logprobs /= logprobs.sum()\n",
    "            results.append( logprobs )\n",
    "        except:\n",
    "            #print(f\"error {i}\")\n",
    "            results.append( np.array([0.5]) )\n",
    "            errors += 1    \n",
    "    \n",
    "#     response_texts = []\n",
    "#     for response in responses:\n",
    "#         # total_tokens += len(response.outputs[0].token_ids)\n",
    "#         # response_texts.append(response.outputs[0].text)\n",
    "#         response_texts.append(response.outputs[0].logprobs[0])\n",
    "    return results, errors\n",
    "    \n",
    "df = pd.read_csv(\"test3.csv\")\n",
    "prompts = df[\"prompt\"].values\n",
    "llm = vllm.LLM(\n",
    "    \"/kaggle/input/qwen-qwen2-5-32b-instruct-gptq-int4/Qwen2.5-32B-Instruct-GPTQ-Int4\", # \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "    #quantization=\"gptq\",\n",
    "    tensor_parallel_size=2, # 2, 4 \n",
    "    gpu_memory_utilization=0.95, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=3096,\n",
    "    enable_lora=True, \n",
    "    enable_prefix_caching=True,\n",
    "    max_lora_rank=32\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "choices = [\"Yes\",\"No\"]\n",
    "\n",
    "KEEP = []\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x, add_special_tokens=False)[0]\n",
    "    KEEP.append(c)\n",
    "print(f\"Force predictions to be tokens {KEEP} which are {choices}.\")\n",
    "\n",
    "class DigitLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.allowed_ids = KEEP\n",
    "        \n",
    "    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores[self.allowed_ids] += 100\n",
    "        return scores\n",
    "\n",
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "\n",
    "results, errors = generate_text_vllm(prompts, tokenizer, llm)\n",
    "print(errors)\n",
    "df[\"results\"] = results\n",
    "df.to_csv(\"test6.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7700c0ec",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.557034Z",
     "iopub.status.idle": "2024-12-12T09:10:33.557388Z",
     "shell.execute_reply": "2024-12-12T09:10:33.557251Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.557234Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python run_3rd_3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b60431",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.558762Z",
     "iopub.status.idle": "2024-12-12T09:10:33.559120Z",
     "shell.execute_reply": "2024-12-12T09:10:33.558944Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.558928Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred = pd.read_csv(\"/kaggle/working/test4.csv\")\n",
    "test_pred2 = pd.read_csv(\"/kaggle/working/test5.csv\")\n",
    "test_pred3 = pd.read_csv(\"/kaggle/working/test6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda4ec7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.560780Z",
     "iopub.status.idle": "2024-12-12T09:10:33.561131Z",
     "shell.execute_reply": "2024-12-12T09:10:33.560958Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.560942Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "results = test_pred[\"results\"]\n",
    "for r in results:\n",
    "    r = float(r.split(\" \")[0][1:])\n",
    "    pred.append(r)\n",
    "test_pred[\"pred\"] = pred\n",
    "\n",
    "pred = []\n",
    "results = test_pred2[\"results\"]\n",
    "for r in results:\n",
    "    r = float(r.split(\" \")[0][1:])\n",
    "    pred.append(r)\n",
    "test_pred2[\"pred2\"] = pred\n",
    "\n",
    "pred = []\n",
    "results = test_pred3[\"results\"]\n",
    "for r in results:\n",
    "    r = float(r.split(\" \")[0][1:])\n",
    "    pred.append(r)\n",
    "test_pred3[\"pred3\"] = pred\n",
    "\n",
    "test_pred[\"pred\"] = (test_pred[\"pred\"] + test_pred2[\"pred2\"] + 4*test_pred3[\"pred3\"])/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b3b79",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.561947Z",
     "iopub.status.idle": "2024-12-12T09:10:33.562289Z",
     "shell.execute_reply": "2024-12-12T09:10:33.562147Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.562129Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\")\n",
    "misconception_list = []\n",
    "for i in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "    misconception_list += list(train[f\"Misconception{i}Id\"].unique())\n",
    "misconception_list = pd.Series(misconception_list)\n",
    "misconception_list = misconception_list.dropna()\n",
    "misconception_list = misconception_list.astype(int).values\n",
    "test_pred.loc[test_pred[\"MisconceptionId\"].isin(misconception_list),\n",
    "\"pred\"] = test_pred.loc[test_pred[\"MisconceptionId\"].isin(misconception_list),\"pred\"]*0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e8094",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.563273Z",
     "iopub.status.idle": "2024-12-12T09:10:33.563604Z",
     "shell.execute_reply": "2024-12-12T09:10:33.563462Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.563445Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred[\"id\"] = test_pred[\"QuestionId\"].astype(str) + \"_\" + test_pred[\"ans\"].astype(str) \n",
    "test_pred[\"pred_rank\"] = test_pred.groupby(by=\"id\")[\"pred\"].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4493448",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.564949Z",
     "iopub.status.idle": "2024-12-12T09:10:33.565327Z",
     "shell.execute_reply": "2024-12-12T09:10:33.565176Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.565156Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred = test_pred.sort_values(by=[\"id\",\"pred_rank\"]).reset_index(drop=True)\n",
    "group = test_pred.groupby(by=\"id\")\n",
    "sub = []\n",
    "for g,df in group:\n",
    "    df = df.sort_values(by=\"pred_rank\").reset_index(drop=True)\n",
    "    pred_ = df[\"MisconceptionId\"].iloc[:25].values\n",
    "    pred_list = ' '.join(map(str, pred_))\n",
    "    sub.append([g,pred_list])\n",
    "sub = pd.DataFrame(sub)\n",
    "sub.columns = [\"QuestionId_Answer\",\"MisconceptionId\"]\n",
    "sub[[\"QuestionId_Answer\",\"MisconceptionId\"]].to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b863163",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-12T09:10:33.566910Z",
     "iopub.status.idle": "2024-12-12T09:10:33.567279Z",
     "shell.execute_reply": "2024-12-12T09:10:33.567131Z",
     "shell.execute_reply.started": "2024-12-12T09:10:33.567107Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80701b49",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5784969,
     "sourceId": 9504861,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5939560,
     "sourceId": 9710485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5950019,
     "sourceId": 9724251,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5950024,
     "sourceId": 9724258,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5962595,
     "sourceId": 9741173,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5962606,
     "sourceId": 9741186,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5962633,
     "sourceId": 9741218,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5985321,
     "sourceId": 9771655,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5985339,
     "sourceId": 9771676,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5991560,
     "sourceId": 9780169,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5999378,
     "sourceId": 9790793,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6060607,
     "sourceId": 9872489,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6060685,
     "sourceId": 9872593,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6069293,
     "sourceId": 9883905,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6069369,
     "sourceId": 9883999,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6099229,
     "sourceId": 9923659,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6104732,
     "sourceId": 9931305,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6125220,
     "sourceId": 9958768,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6125286,
     "sourceId": 9958863,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6125357,
     "sourceId": 9958954,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6128300,
     "sourceId": 9962735,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6128407,
     "sourceId": 9962868,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6128474,
     "sourceId": 9962964,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6158280,
     "sourceId": 10004493,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6158320,
     "sourceId": 10004545,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6158343,
     "sourceId": 10004576,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6160941,
     "sourceId": 10008057,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6164660,
     "sourceId": 10013069,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6189829,
     "sourceId": 10047086,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6243748,
     "sourceId": 10119255,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6251354,
     "sourceId": 10129597,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6253417,
     "sourceId": 10132347,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6256316,
     "sourceId": 10137041,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 202903897,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 203059180,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 203330016,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 203425234,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 203500713,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 204303131,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 204667247,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 204997914,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 205067945,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 207769425,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 207784883,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 207868261,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 209448859,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 209742308,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 209933011,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 210246076,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 211878990,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 211885569,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 211966495,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 212398122,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 212400047,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 212640856,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141475,
     "sourceId": 166264,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 145971,
     "sourceId": 171509,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 237.749417,
   "end_time": "2024-12-13T01:57:00.066985",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-13T01:53:02.317568",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
