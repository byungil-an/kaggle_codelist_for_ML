{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcfa515",
   "metadata": {
    "papermill": {
     "duration": 0.014842,
     "end_time": "2024-12-08T18:38:15.957130",
     "exception": false,
     "start_time": "2024-12-08T18:38:15.942288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd9e31f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:38:15.982991Z",
     "iopub.status.busy": "2024-12-08T18:38:15.982676Z",
     "iopub.status.idle": "2024-12-08T18:38:15.990986Z",
     "shell.execute_reply": "2024-12-08T18:38:15.990133Z"
    },
    "papermill": {
     "duration": 0.023248,
     "end_time": "2024-12-08T18:38:15.992793",
     "exception": false,
     "start_time": "2024-12-08T18:38:15.969545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: N_EX=4\n"
     ]
    }
   ],
   "source": [
    "%env N_EX=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac3ac9",
   "metadata": {
    "papermill": {
     "duration": 0.012148,
     "end_time": "2024-12-08T18:38:16.017858",
     "exception": false,
     "start_time": "2024-12-08T18:38:16.005710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stage 1: Retriever\n",
    "\n",
    "Retrieve candidate misconceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a4c6dd",
   "metadata": {
    "papermill": {
     "duration": 0.012203,
     "end_time": "2024-12-08T18:38:16.042416",
     "exception": false,
     "start_time": "2024-12-08T18:38:16.030213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Retriever Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb032561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:38:16.068027Z",
     "iopub.status.busy": "2024-12-08T18:38:16.067784Z",
     "iopub.status.idle": "2024-12-08T18:38:16.076679Z",
     "shell.execute_reply": "2024-12-08T18:38:16.076005Z"
    },
    "papermill": {
     "duration": 0.023812,
     "end_time": "2024-12-08T18:38:16.078399",
     "exception": false,
     "start_time": "2024-12-08T18:38:16.054587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing eedi_llm_retriever.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile eedi_llm_retriever.py\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/kaggle/input/eedi-utils-v04')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from llm_embedding.eedi_dataset import MathDataset\n",
    "from llm_embedding.eedi_loader import TextCollator\n",
    "from llm_embedding.eedi_model import BiEncoderModel\n",
    "\n",
    "from utils.retriever_utils import semantic_search\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch.distributed as dist\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoConfig, AutoModel, BitsAndBytesConfig\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def print_line(): print(\"--\"*40)\n",
    "\n",
    "def query_formatting_func(query):\n",
    "    task_description = \"\"\"Retrieve the key misconception behind the wrong answer when given a math problem and its incorrect and correct solutions.\"\"\"\n",
    "    return f\"Instruct: {task_description}\\nQuery: {query}\"\n",
    "\n",
    "\n",
    "def get_base_model(cfg):\n",
    "    config = AutoConfig.from_pretrained(cfg.model.backbone_path, trust_remote_code=False)\n",
    "    config.use_cache = False\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    model = AutoModel.from_pretrained(\n",
    "        cfg.model.backbone_path, \n",
    "        config=config, \n",
    "        quantization_config=bnb_config,\n",
    "        attn_implementation=cfg.model.attn_implementation, \n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "        \n",
    "    model.config.pretraining_tp = 1\n",
    "    return model\n",
    "    \n",
    "# ---\n",
    "def show_batch(batch, tokenizer, n_examples=4, print_fn=print):\n",
    "    bs = batch[\"input_ids\"].size(0)\n",
    "    print_fn(f\"batch size: {bs}\")\n",
    "\n",
    "    print_fn(f\"shape of input_ids: {batch['input_ids'].shape}\")\n",
    "\n",
    "    print(\"--\" * 80)\n",
    "    for idx in range(n_examples):\n",
    "        print_fn(f\"[Text]:\\n{tokenizer.decode(batch['input_ids'][idx], skip_special_tokens=False)}\")\n",
    "        print_fn(\"--\" * 80)\n",
    "    \n",
    "def eedi_process_df(df):\n",
    "    df = deepcopy(df)\n",
    "    grouped = df.groupby(\"QuestionId\")\n",
    "\n",
    "    question_dict = {}\n",
    "    for question_id, group in grouped:\n",
    "        question_data = group.to_dict(orient=\"records\")[0]\n",
    "        del question_data[\"QuestionId\"]\n",
    "        question_dict[question_id] = question_data\n",
    "\n",
    "    all_questions = list(question_dict.keys())\n",
    "\n",
    "    queries = []\n",
    "\n",
    "    # ---\n",
    "    for qid in all_questions:\n",
    "        info = question_dict[qid]\n",
    "\n",
    "        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if info[\"CorrectAnswer\"] == answer_key:\n",
    "                continue\n",
    "            \n",
    "            this_example = dict()\n",
    "            this_key = f\"{qid}_{answer_key}\"\n",
    "            this_example[\"query_id\"] = this_key\n",
    "            \n",
    "            # ---\n",
    "            for col in [\"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n",
    "                this_example[col] = info[col]\n",
    "\n",
    "            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n",
    "            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n",
    "            queries.append(this_example)\n",
    "    # --\n",
    "    query_df = pd.DataFrame(queries)\n",
    "    return query_df\n",
    "\n",
    "\n",
    "def add_retrieved_results(cfg, infer_df, content_df):\n",
    "    \"\"\"\n",
    "    find top-k similar chunks for each question\n",
    "    \"\"\"\n",
    "    # accelerator -----------------------------------------------------------------------#\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    query_df = eedi_process_df(infer_df)\n",
    "\n",
    "    # queries ---------------------------------------------------------------------------#\n",
    "    ds_handle = MathDataset(cfg, query_formatting_func=query_formatting_func)\n",
    "    tokenizer = ds_handle.tokenizer\n",
    "    \n",
    "    query_ds = ds_handle.get_dataset(query_df, is_query=True)\n",
    "    content_ds = ds_handle.get_dataset(content_df, is_query=False)\n",
    "    \n",
    "    query_ds = query_ds.sort(\"input_length\")\n",
    "    content_ds = content_ds.sort(\"input_length\")\n",
    "    \n",
    "    query_ids = query_ds[\"query_id\"]\n",
    "    content_ids = content_ds[\"content_id\"]\n",
    "\n",
    "    collator = TextCollator(tokenizer=tokenizer)\n",
    "\n",
    "    query_dl = DataLoader(\n",
    "        query_ds,\n",
    "        batch_size=cfg.predict_params.query_bs,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "\n",
    "    content_dl = DataLoader(\n",
    "        content_ds,\n",
    "        batch_size=cfg.predict_params.content_bs,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "    \n",
    "    # show ---\n",
    "    accelerator.print(\"Showing a batch (Query)...\")\n",
    "    for b in query_dl:\n",
    "        show_batch(b, tokenizer, print_fn=accelerator.print)\n",
    "        break\n",
    "        \n",
    "    accelerator.print(\"Showing a batch (Content)...\")\n",
    "    for b in content_dl:\n",
    "        show_batch(b, tokenizer, print_fn=accelerator.print)\n",
    "        break\n",
    "    \n",
    "    # model -----------------------------------------------------------------------------#\n",
    "    base_model = get_base_model(cfg)\n",
    "    model = BiEncoderModel(cfg, base_model, accelerator)\n",
    "    \n",
    "    # prepare ---------------------------------------------------------------------------#\n",
    "    model, query_dl, content_dl = accelerator.prepare(model, query_dl, content_dl)\n",
    "\n",
    "    # query embeddings ------------------------------------------------------------------#\n",
    "    query_embeddings = []\n",
    "    progress_bar = tqdm(range(len(query_dl)))\n",
    "\n",
    "    for batch in query_dl:\n",
    "        with torch.no_grad():\n",
    "            batch_query_embeddings = accelerator.unwrap_model(model).encode(batch)\n",
    "        batch_query_embeddings = accelerator.gather_for_metrics(batch_query_embeddings)\n",
    "        query_embeddings.append(batch_query_embeddings)\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "    query_embeddings = torch.cat(query_embeddings, dim=0)\n",
    "    accelerator.print(f\"shape of query embeddings: {query_embeddings.shape}\")\n",
    "    assert query_embeddings.shape[0] == len(query_ids)\n",
    "    \n",
    "    # content embeddings ----------------------------------------------------------------#\n",
    "    content_embeddings = []\n",
    "    progress_bar = tqdm(range(len(content_dl)))\n",
    "\n",
    "    for batch in content_dl:\n",
    "        with torch.no_grad():\n",
    "            batch_content_embeddings = accelerator.unwrap_model(model).encode(batch)\n",
    "        batch_content_embeddings = accelerator.gather_for_metrics(batch_content_embeddings)\n",
    "        content_embeddings.append(batch_content_embeddings)\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "    content_embeddings = torch.cat(content_embeddings, dim=0)\n",
    "    accelerator.print(f\"shape of content embeddings: {content_embeddings.shape}\")\n",
    "    assert content_embeddings.shape[0] == len(content_ids)\n",
    "\n",
    "    # top-k search ----------------------------------------------------------------------#\n",
    "    results = semantic_search(query_embeddings, content_embeddings, top_k = cfg.model.n_neighbour)\n",
    "    \n",
    "    pred_content_ids, pred_scores = [], []\n",
    "    for idx, re_i in enumerate(results):\n",
    "        query_id = query_ids[idx]\n",
    "        hit_i = [node[\"corpus_id\"] for node in re_i]\n",
    "        top_scores_i = [node[\"score\"] for node in re_i]\n",
    "        top_content_ids_i = [content_ids[pos] for pos in hit_i]\n",
    "        pred_content_ids.append(top_content_ids_i)\n",
    "        pred_scores.append(top_scores_i)\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"query_id\"] = query_ids\n",
    "    result_df[\"pred_ids\"] = pred_content_ids\n",
    "    result_df[\"pred_scores\"] = pred_scores\n",
    "    \n",
    "    # get oof df\n",
    "    oof_df = result_df.copy()\n",
    "    oof_df = oof_df.rename(columns={\"query_id\": \"QuestionId_Answer\"})\n",
    "    oof_df = oof_df.rename(columns={\"pred_ids\": \"MisconceptionId\"})\n",
    "    oof_df[\"MisconceptionId\"] = oof_df[\"MisconceptionId\"].apply(lambda x: list(map(str, x)))\n",
    "\n",
    "    print_line()\n",
    "    accelerator.print(\"Sample Prediction:\")\n",
    "    accelerator.print(oof_df.sample().T)\n",
    "    print_line()\n",
    "    \n",
    "    return oof_df\n",
    "\n",
    "# ------\n",
    "\n",
    "\n",
    "def execute_inference(cfg, save_dir, model_name):\n",
    "    test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "    \n",
    "    if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        n_ex = int(os.getenv(\"N_EX\"))\n",
    "        test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").head(n_ex)\n",
    "    content_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "    content_df = content_df.rename(columns={\"MisconceptionId\": \"content_id\"})\n",
    "        \n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        test_df = add_retrieved_results(cfg, test_df, content_df)\n",
    "    elif cfg.run_on_save:\n",
    "        test_df = add_retrieved_results(cfg, test_df, content_df)\n",
    "    else:\n",
    "        test_df = pd.read_parquet(\"./retriever_outputs/intfloat.parquet\")\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"{model_name}.parquet\")\n",
    "    test_df.to_parquet(save_path)\n",
    "    \n",
    "    if dist.is_initialized():\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('--config_path', type=str, required=True)\n",
    "    ap.add_argument('--save_dir', type=str, required=True)\n",
    "    ap.add_argument('--model_name', type=str, required=True)\n",
    "\n",
    "    args = ap.parse_args()\n",
    "    cfg = OmegaConf.load(args.config_path)\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "    # execution\n",
    "    execute_inference(cfg, save_dir=args.save_dir, model_name=args.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c351cb5",
   "metadata": {
    "papermill": {
     "duration": 0.012876,
     "end_time": "2024-12-08T18:38:16.103734",
     "exception": false,
     "start_time": "2024-12-08T18:38:16.090858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Retriever Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35dad57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:38:16.132235Z",
     "iopub.status.busy": "2024-12-08T18:38:16.131962Z",
     "iopub.status.idle": "2024-12-08T18:38:16.136717Z",
     "shell.execute_reply": "2024-12-08T18:38:16.135914Z"
    },
    "papermill": {
     "duration": 0.020349,
     "end_time": "2024-12-08T18:38:16.138436",
     "exception": false,
     "start_time": "2024-12-08T18:38:16.118087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing eedi_retriever_intfloat.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile eedi_retriever_intfloat.yaml\n",
    "run_on_save: true\n",
    "\n",
    "model:\n",
    "    backbone_path: /kaggle/input/eedi-embed-intfloat-cv476-ff-4bit\n",
    "    max_length: 768\n",
    "    sentence_pooling_method: last\n",
    "    attn_implementation: eager\n",
    "    negatives_cross_device: false\n",
    "    add_eos_token: true\n",
    "    padding_side: left\n",
    "    trust_remote_code: false\n",
    "\n",
    "    n_neighbour: 128\n",
    "\n",
    "predict_params:\n",
    "    query_bs: 8\n",
    "    content_bs: 8\n",
    "\n",
    "train_params: # fix, should not need these for infer\n",
    "    sub_batch_size: 8 \n",
    "    num_hard_negatives: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd82552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:38:16.165338Z",
     "iopub.status.busy": "2024-12-08T18:38:16.165097Z",
     "iopub.status.idle": "2024-12-08T18:38:16.170012Z",
     "shell.execute_reply": "2024-12-08T18:38:16.169053Z"
    },
    "papermill": {
     "duration": 0.020733,
     "end_time": "2024-12-08T18:38:16.172002",
     "exception": false,
     "start_time": "2024-12-08T18:38:16.151269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing eedi_retriever_qwen.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile eedi_retriever_qwen.yaml\n",
    "run_on_save: false\n",
    "model:\n",
    "    backbone_path: /kaggle/input/eedi-embed-qwen14b-cv486-ff-4bit\n",
    "    max_length: 768\n",
    "    sentence_pooling_method: last\n",
    "    attn_implementation: eager\n",
    "    negatives_cross_device: false\n",
    "    add_eos_token: true\n",
    "    padding_side: left\n",
    "    trust_remote_code: false\n",
    "\n",
    "    n_neighbour: 128\n",
    "\n",
    "predict_params:\n",
    "    query_bs: 8\n",
    "    content_bs: 8\n",
    "\n",
    "train_params:\n",
    "    sub_batch_size: 8 # fix\n",
    "    num_hard_negatives: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4adce472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:38:16.200042Z",
     "iopub.status.busy": "2024-12-08T18:38:16.199793Z",
     "iopub.status.idle": "2024-12-08T18:38:16.204490Z",
     "shell.execute_reply": "2024-12-08T18:38:16.203761Z"
    },
    "papermill": {
     "duration": 0.020406,
     "end_time": "2024-12-08T18:38:16.206338",
     "exception": false,
     "start_time": "2024-12-08T18:38:16.185932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing eedi_retriever_bge.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile eedi_retriever_bge.yaml\n",
    "run_on_save: true\n",
    "\n",
    "model:\n",
    "    backbone_path: /kaggle/input/eedi-embed-bge-cv493-ff-4bit\n",
    "    max_length: 768\n",
    "    sentence_pooling_method: last\n",
    "    attn_implementation: eager\n",
    "    negatives_cross_device: false\n",
    "    add_eos_token: true\n",
    "    padding_side: left\n",
    "    trust_remote_code: false\n",
    "\n",
    "    n_neighbour: 128\n",
    "\n",
    "predict_params:\n",
    "    query_bs: 8\n",
    "    content_bs: 8\n",
    "\n",
    "train_params:\n",
    "    sub_batch_size: 8 # fix\n",
    "    num_hard_negatives: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d939bc1b",
   "metadata": {
    "papermill": {
     "duration": 0.011816,
     "end_time": "2024-12-08T18:38:16.231043",
     "exception": false,
     "start_time": "2024-12-08T18:38:16.219227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.3 Retriever Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "581945bc",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-08T18:38:16.256772Z",
     "iopub.status.busy": "2024-12-08T18:38:16.256496Z",
     "iopub.status.idle": "2024-12-08T18:40:29.060321Z",
     "shell.execute_reply": "2024-12-08T18:40:29.059097Z"
    },
    "papermill": {
     "duration": 132.818859,
     "end_time": "2024-12-08T18:40:29.062247",
     "exception": false,
     "start_time": "2024-12-08T18:38:16.243388",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████| 12/12 [00:00<00:00, 524.44 examples/s]\r\n",
      "Map: 100%|█████████████████████████████| 12/12 [00:00<00:00, 1310.00 examples/s]\r\n",
      "Map: 100%|████████████████████████| 2587/2587 [00:00<00:00, 17393.90 examples/s]\r\n",
      "Showing a batch (Query)...\r\n",
      "Map: 100%|████████████████████████| 2587/2587 [00:00<00:00, 20174.52 examples/s]\r\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "batch size: 8\r\n",
      "shape of input_ids: torch.Size([8, 136])\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Instruct: Retrieve the key misconception behind the wrong answer when given a math problem and its incorrect and correct solutions.\r\n",
      "Query: Simplifying Algebraic Fractions - Simplify an algebraic fraction by factorising the numerator\r\n",
      "# Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\r\n",
      "# Correct Answer: Does not simplify\r\n",
      "# Wrong Answer: \\( m+1 \\)</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Instruct: Retrieve the key misconception behind the wrong answer when given a math problem and its incorrect and correct solutions.\r\n",
      "Query: Simplifying Algebraic Fractions - Simplify an algebraic fraction by factorising the numerator\r\n",
      "# Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\r\n",
      "# Correct Answer: Does not simplify\r\n",
      "# Wrong Answer: \\( m+2 \\)</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Instruct: Retrieve the key misconception behind the wrong answer when given a math problem and its incorrect and correct solutions.\r\n",
      "Query: Simplifying Algebraic Fractions - Simplify an algebraic fraction by factorising the numerator\r\n",
      "# Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\r\n",
      "# Correct Answer: Does not simplify\r\n",
      "# Wrong Answer: \\( m-1 \\)</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Instruct: Retrieve the key misconception behind the wrong answer when given a math problem and its incorrect and correct solutions.\r\n",
      "Query: BIDMAS - Use the order of operations to carry out calculations involving powers\r\n",
      "# Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal \\( 13 \\) ?\r\n",
      "# Correct Answer: \\( 3 \\times(2+4)-5 \\)\r\n",
      "# Wrong Answer: Does not need brackets</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "Showing a batch (Content)...\r\n",
      "batch size: 8\r\n",
      "shape of input_ids: torch.Size([8, 7])\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "</s><s> Misreads scale</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "</s><s> Only counts visible edges</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "</s><s> Only counts visible faces</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "</s><s> Only counts visible vertices</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\n",
      "negatives_cross_device: False\r\n",
      "world_size: 2\r\n",
      "process_rank: 0\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.01s/it]\r\n",
      "shape of query embeddings: torch.Size([12, 4096])\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.44s/it]\r\n",
      "100%|█████████████████████████████████████████| 162/162 [00:48<00:00,  3.32it/s]\r\n",
      "shape of content embeddings: torch.Size([2587, 4096])\r\n",
      "100%|█████████████████████████████████████████| 162/162 [00:48<00:00,  3.34it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  7.19it/s]\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Sample Prediction:\r\n",
      "                                                                   1\r\n",
      "QuestionId_Answer                                                1_B\r\n",
      "MisconceptionId    [143, 1755, 2078, 891, 167, 2398, 1610, 1871, ...\r\n",
      "pred_scores        [0.8671875, 0.8486328125, 0.84375, 0.838867187...\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 14.89it/s]\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "CPU times: user 1.67 s, sys: 428 ms, total: 2.1 s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 eedi_llm_retriever.py \\\n",
    "--config_path ./eedi_retriever_intfloat.yaml \\\n",
    "--save_dir ./retriever_outputs \\\n",
    "--model_name intfloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef571420",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-08T18:40:29.112531Z",
     "iopub.status.busy": "2024-12-08T18:40:29.112227Z",
     "iopub.status.idle": "2024-12-08T18:40:44.145875Z",
     "shell.execute_reply": "2024-12-08T18:40:44.144643Z"
    },
    "papermill": {
     "duration": 15.060171,
     "end_time": "2024-12-08T18:40:44.147920",
     "exception": false,
     "start_time": "2024-12-08T18:40:29.087749",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 206 ms, sys: 47.4 ms, total: 253 ms\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 eedi_llm_retriever.py \\\n",
    "--config_path ./eedi_retriever_qwen.yaml \\\n",
    "--save_dir ./retriever_outputs \\\n",
    "--model_name qwen_14b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c39c06",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-08T18:40:44.198563Z",
     "iopub.status.busy": "2024-12-08T18:40:44.198259Z",
     "iopub.status.idle": "2024-12-08T18:42:52.873162Z",
     "shell.execute_reply": "2024-12-08T18:42:52.872097Z"
    },
    "papermill": {
     "duration": 128.740483,
     "end_time": "2024-12-08T18:42:52.913113",
     "exception": false,
     "start_time": "2024-12-08T18:40:44.172630",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████| 12/12 [00:00<00:00, 1271.74 examples/s]\r\n",
      "Map: 100%|██████████████████████████████| 12/12 [00:00<00:00, 798.70 examples/s]\r\n",
      "Map: 100%|████████████████████████| 2587/2587 [00:00<00:00, 22661.07 examples/s]\r\n",
      "Showing a batch (Query)...\r\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "batch size: 8\r\n",
      "shape of input_ids: torch.Size([8, 136])\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> Instruct: Retrieve the key misconception behind the wrong answer when given a math problem and its incorrect and correct solutions.\r\n",
      "Query: Simplifying Algebraic Fractions - Simplify an algebraic fraction by factorising the numerator\r\n",
      "# Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\r\n",
      "# Correct Answer: Does not simplify\r\n",
      "# Wrong Answer: \\( m+1 \\)</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> Instruct: Retrieve the key misconception behind the wrong answer when given a math problem and its incorrect and correct solutions.\r\n",
      "Query: Simplifying Algebraic Fractions - Simplify an algebraic fraction by factorising the numerator\r\n",
      "# Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\r\n",
      "# Correct Answer: Does not simplify\r\n",
      "# Wrong Answer: \\( m+2 \\)</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> Instruct: Retrieve the key misconception behind the wrong answer when given a math problem and its incorrect and correct solutions.\r\n",
      "Query: Simplifying Algebraic Fractions - Simplify an algebraic fraction by factorising the numerator\r\n",
      "# Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\r\n",
      "# Correct Answer: Does not simplify\r\n",
      "# Wrong Answer: \\( m-1 \\)</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> Instruct: Retrieve the key misconception behind the wrong answer when given a math problem and its incorrect and correct solutions.\r\n",
      "Query: BIDMAS - Use the order of operations to carry out calculations involving powers\r\n",
      "# Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal \\( 13 \\) ?\r\n",
      "# Correct Answer: \\( 3 \\times(2+4)-5 \\)\r\n",
      "# Wrong Answer: Does not need brackets</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "Showing a batch (Content)...\r\n",
      "batch size: 8\r\n",
      "shape of input_ids: torch.Size([8, 7])\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "<unk><s> Misreads scale</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "<unk><s> Only counts visible edges</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "<unk><s> Only counts visible faces</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "[Text]:\r\n",
      "<unk><s> Only counts visible vertices</s>\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "Map: 100%|████████████████████████| 2587/2587 [00:00<00:00, 24222.18 examples/s]\r\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\r\n",
      "negatives_cross_device: False\r\n",
      "world_size: 2\r\n",
      "process_rank: 0\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.70s/it]\r\n",
      "shape of query embeddings: torch.Size([12, 4096])\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.33s/it]\r\n",
      "100%|█████████████████████████████████████████| 162/162 [00:57<00:00,  2.82it/s]\r\n",
      "shape of content embeddings: torch.Size([2587, 4096])\r\n",
      "100%|█████████████████████████████████████████| 162/162 [00:56<00:00,  2.85it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.80it/s]\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Sample Prediction:\r\n",
      "                                                                   9\r\n",
      "QuestionId_Answer                                                2_A\r\n",
      "MisconceptionId    [1287, 1073, 2439, 1059, 557, 1521, 2471, 129,...\r\n",
      "pred_scores        [0.76171875, 0.74267578125, 0.703125, 0.698242...\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 15.25it/s]\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "CPU times: user 1.66 s, sys: 425 ms, total: 2.09 s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 eedi_llm_retriever.py \\\n",
    "--config_path ./eedi_retriever_bge.yaml \\\n",
    "--save_dir ./retriever_outputs \\\n",
    "--model_name bge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3029fd",
   "metadata": {
    "papermill": {
     "duration": 0.037285,
     "end_time": "2024-12-08T18:42:52.988260",
     "exception": false,
     "start_time": "2024-12-08T18:42:52.950975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.4 Prepare for Ranker\n",
    "- Prepares input for re-ranking (\"./retriever_outputs/ranker_input.parquet\")\n",
    "- Prepares blended predictions from retrievers (\"./retriever_outputs/stage_one_blended.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501ca9d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:42:53.104188Z",
     "iopub.status.busy": "2024-12-08T18:42:53.103343Z",
     "iopub.status.idle": "2024-12-08T18:42:53.111873Z",
     "shell.execute_reply": "2024-12-08T18:42:53.111032Z"
    },
    "papermill": {
     "duration": 0.087861,
     "end_time": "2024-12-08T18:42:53.113640",
     "exception": false,
     "start_time": "2024-12-08T18:42:53.025779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prepare_for_ranker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prepare_for_ranker.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "def process_df(df):\n",
    "    df = deepcopy(df)\n",
    "    grouped = df.groupby(\"QuestionId\")\n",
    "\n",
    "    question_dict = {}\n",
    "    for question_id, group in grouped:\n",
    "        question_data = group.to_dict(orient=\"records\")[0]\n",
    "        del question_data[\"QuestionId\"]\n",
    "        question_dict[question_id] = question_data\n",
    "\n",
    "    all_questions = list(question_dict.keys())\n",
    "\n",
    "    queries = []\n",
    "\n",
    "    for qid in all_questions:\n",
    "        info = question_dict[qid]\n",
    "\n",
    "        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if info[\"CorrectAnswer\"] == answer_key:\n",
    "                continue\n",
    "            this_example = dict()\n",
    "            this_key = f\"{qid}_{answer_key}\"\n",
    "            this_example[\"QuestionId_Answer\"] = this_key\n",
    "\n",
    "            # ---\n",
    "            for col in [\"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n",
    "                this_example[col] = info[col]\n",
    "\n",
    "            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n",
    "            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n",
    "            this_example[\"AllOptionText\"] = \"\\n- \".join([info[f\"Answer{x}Text\"] for x in [\"A\", \"B\", \"C\", \"D\"]])\n",
    "            this_example[\"AllOptionText\"] = f\"\\n- {this_example['AllOptionText']}\"\n",
    "            queries.append(this_example)\n",
    "\n",
    "    query_df = pd.DataFrame(queries)\n",
    "    return query_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config-path\", type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.config_path, \"r\") as f:\n",
    "        cfg = OmegaConf.load(f)\n",
    "\n",
    "    os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "\n",
    "    # load data ---\n",
    "    test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "        n_ex = int(os.getenv(\"N_EX\"))\n",
    "        test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").head(n_ex)\n",
    "    concept_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "    concept_df[\"MisconceptionId\"] = concept_df[\"MisconceptionId\"].astype(str)\n",
    "\n",
    "    # load retriever outputs ---\n",
    "    flat_dfs = []\n",
    "    for fp in cfg.retriever_outputs:\n",
    "        df = pd.read_parquet(fp)\n",
    "        print(f\"Loading from {fp}\")\n",
    "        print(df.head(1).T)\n",
    "        print(\"--\"*40)\n",
    "        flat_df = df.explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n",
    "        flat_dfs.append(flat_df)\n",
    "\n",
    "    # blend retriever scores ---\n",
    "    ret_preds = pd.concat(flat_dfs, ignore_index=True)\n",
    "    ret_preds = ret_preds.groupby([\"QuestionId_Answer\", \"MisconceptionId\"])[\"pred_scores\"].agg(\"sum\").reset_index()\n",
    "    ret_preds[\"pred_scores\"] = ret_preds[\"pred_scores\"] / len(cfg.retriever_outputs)\n",
    "\n",
    "    # re-sort by pred_scores\n",
    "    grouped = ret_preds.groupby(\"QuestionId_Answer\")\n",
    "\n",
    "    results = []\n",
    "    for question_id, group in grouped:\n",
    "        sorted_group = group.sort_values(\"pred_scores\", ascending=False)\n",
    "        result = {\"QuestionId_Answer\": question_id, \"MisconceptionId\": list(sorted_group[\"MisconceptionId\"]), \"pred_scores\": list(sorted_group[\"pred_scores\"])}\n",
    "        results.append(result)\n",
    "    ret_preds = pd.DataFrame(results)\n",
    "\n",
    "    # save for later use ---\n",
    "    print(\"--\" * 40)\n",
    "    save_path = os.path.join(cfg.output_dir, cfg.blned_file_name)\n",
    "    print(f\"Saving stage one blended predictions to {save_path}\")\n",
    "    ret_preds.to_parquet(save_path)  \n",
    "    print(ret_preds.sample().T)\n",
    "    print(\"--\" * 40)\n",
    "\n",
    "    # prepare candidate set ---\n",
    "    margin = cfg.margin\n",
    "    ret_preds[\"threshold\"] = ret_preds[\"pred_scores\"].apply(lambda x: x[0] - margin)\n",
    "    ret_preds[\"cutoff\"] = ret_preds.apply(lambda x: sum([y > x[\"threshold\"] for y in x[\"pred_scores\"]]), axis=1)\n",
    "    ret_preds[\"cutoff\"] = ret_preds[\"cutoff\"].clip(lower=cfg.min_top_k, upper=cfg.max_top_k)\n",
    "\n",
    "    print(\"--\" * 40)\n",
    "    print(\"Cutoff distribution:\\n-------\")\n",
    "    print(ret_preds[\"cutoff\"].value_counts().sort_index())\n",
    "    print(f\"Average # candidates: {ret_preds['cutoff'].mean()}\")\n",
    "    print(\"--\" * 40)\n",
    "\n",
    "    ret_preds[\"MisconceptionId\"] = ret_preds.apply(lambda row: row[\"MisconceptionId\"][:row[\"cutoff\"]], axis=1)\n",
    "    ret_preds = ret_preds[[\"QuestionId_Answer\", \"MisconceptionId\"]].copy()\n",
    "    flat_df = ret_preds[[\"QuestionId_Answer\", \"MisconceptionId\"]].explode([\"MisconceptionId\"]).reset_index(drop=True)\n",
    "\n",
    "    # prepare ranker input ---\n",
    "    rank_df = process_df(test_df)\n",
    "    rank_df = rank_df.merge(flat_df, on=\"QuestionId_Answer\", how=\"left\")\n",
    "    rank_df = rank_df.merge(concept_df, on=\"MisconceptionId\", how=\"left\")\n",
    "    rank_df[\"MisconceptionId\"] = rank_df[\"MisconceptionId\"].astype(str)\n",
    "    rank_df = rank_df[\n",
    "        [\"QuestionId_Answer\", \"MisconceptionId\", \"SubjectName\", \"ConstructName\", \"QuestionText\", \"CorrectAnswerText\", \"InCorrectAnswerText\", \"AllOptionText\", \"MisconceptionName\"]\n",
    "    ].copy()\n",
    "    \n",
    "    print(\"--\" * 40)\n",
    "    save_path = os.path.join(cfg.output_dir, cfg.ranker_input_file_name)\n",
    "    print(f\"Saving re-ranker input to: {save_path}\")\n",
    "    rank_df.to_parquet(save_path)\n",
    "    print(\"--\"*40)\n",
    "    \n",
    "    print(rank_df.sample().T)\n",
    "    print(f\"shape of ranker input: {rank_df.shape}\")\n",
    "    print(\"--\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19a1739e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:42:53.190750Z",
     "iopub.status.busy": "2024-12-08T18:42:53.190274Z",
     "iopub.status.idle": "2024-12-08T18:42:53.195524Z",
     "shell.execute_reply": "2024-12-08T18:42:53.194597Z"
    },
    "papermill": {
     "duration": 0.045979,
     "end_time": "2024-12-08T18:42:53.197219",
     "exception": false,
     "start_time": "2024-12-08T18:42:53.151240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ranker_prep.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ranker_prep.yaml\n",
    "\n",
    "retriever_outputs:\n",
    "    - ./retriever_outputs/intfloat.parquet\n",
    "    - ./retriever_outputs/qwen_14b.parquet\n",
    "    - ./retriever_outputs/bge.parquet\n",
    "\n",
    "min_top_k: 25\n",
    "max_top_k: 50\n",
    "margin: 0.06\n",
    "\n",
    "output_dir: ./retriever_outputs\n",
    "blned_file_name: stage_one_blended.parquet\n",
    "ranker_input_file_name: ranker_input_stage_one.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2e688aa",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-08T18:42:53.274158Z",
     "iopub.status.busy": "2024-12-08T18:42:53.273940Z",
     "iopub.status.idle": "2024-12-08T18:42:55.140253Z",
     "shell.execute_reply": "2024-12-08T18:42:55.139281Z"
    },
    "papermill": {
     "duration": 1.90682,
     "end_time": "2024-12-08T18:42:55.142174",
     "exception": false,
     "start_time": "2024-12-08T18:42:53.235354",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ./retriever_outputs/intfloat.parquet\r\n",
      "                                                                   0\r\n",
      "QuestionId_Answer                                                1_A\r\n",
      "MisconceptionId    [1755, 143, 418, 2142, 1535, 2078, 167, 2068, ...\r\n",
      "pred_scores        [0.86572265625, 0.85546875, 0.84912109375, 0.8...\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Loading from ./retriever_outputs/qwen_14b.parquet\r\n",
      "                                                                   0\r\n",
      "QuestionId_Answer                                                1_A\r\n",
      "MisconceptionId    [1755, 143, 418, 2142, 1535, 2078, 167, 2068, ...\r\n",
      "pred_scores        [0.86572265625, 0.85546875, 0.84912109375, 0.8...\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Loading from ./retriever_outputs/bge.parquet\r\n",
      "                                                                   0\r\n",
      "QuestionId_Answer                                                1_A\r\n",
      "MisconceptionId    [2142, 1755, 418, 2068, 1256, 2256, 143, 2078,...\r\n",
      "pred_scores        [0.75537109375, 0.751953125, 0.748046875, 0.74...\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Saving stage one blended predictions to ./retriever_outputs/stage_one_blended.parquet\r\n",
      "                                                                   9\r\n",
      "QuestionId_Answer                                                3_A\r\n",
      "MisconceptionId    [1180, 1202, 1071, 2464, 226, 618, 219, 1326, ...\r\n",
      "pred_scores        [0.8274739583333334, 0.7921549479166666, 0.787...\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Cutoff distribution:\r\n",
      "-------\r\n",
      "cutoff\r\n",
      "25    7\r\n",
      "38    1\r\n",
      "39    2\r\n",
      "49    1\r\n",
      "50    1\r\n",
      "Name: count, dtype: int64\r\n",
      "Average # candidates: 32.5\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Saving re-ranker input to: ./retriever_outputs/ranker_input_stage_one.parquet\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "                                                                   301\r\n",
      "QuestionId_Answer                                                  2_D\r\n",
      "MisconceptionId                                                   1677\r\n",
      "SubjectName          Range and Interquartile Range from a List of Data\r\n",
      "ConstructName                  Calculate the range from a list of data\r\n",
      "QuestionText         Tom and Katie are discussing the \\( 5 \\) plant...\r\n",
      "CorrectAnswerText                                          Only\\nKatie\r\n",
      "InCorrectAnswerText                                 Neither is correct\r\n",
      "AllOptionText        \\n- Only\\nTom\\n- Only\\nKatie\\n- Both Tom and K...\r\n",
      "MisconceptionName    Thinks the range is just the first value subtr...\r\n",
      "shape of ranker input: (390, 9)\r\n",
      "--------------------------------------------------------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "!python prepare_for_ranker.py --config-path ranker_prep.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4cdc65",
   "metadata": {
    "papermill": {
     "duration": 0.037519,
     "end_time": "2024-12-08T18:42:55.219425",
     "exception": false,
     "start_time": "2024-12-08T18:42:55.181906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e12da4c",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-08T18:42:55.296001Z",
     "iopub.status.busy": "2024-12-08T18:42:55.295251Z",
     "iopub.status.idle": "2024-12-08T18:46:08.588014Z",
     "shell.execute_reply": "2024-12-08T18:46:08.586298Z"
    },
    "papermill": {
     "duration": 193.333226,
     "end_time": "2024-12-08T18:46:08.590103",
     "exception": false,
     "start_time": "2024-12-08T18:42:55.256877",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\r\n",
      "pointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "ydata-profiling 4.9.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing requirements for pydantic: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/pydantic-2.8.2.dist-info/METADATA'\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing requirements for pydantic: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/pydantic-2.8.2.dist-info/METADATA'\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "vllm 0.6.3.post1 requires pydantic>=2.9, which is not installed.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mFound existing installation: pynvml 11.4.1\r\n",
      "Uninstalling pynvml-11.4.1:\r\n",
      "  Successfully uninstalled pynvml-11.4.1\r\n",
      "Looking in links: /kaggle/input/pm-72463015-at-12-08-2024-18-33-22/\r\n",
      "Processing /kaggle/input/0-6-3-post1-wheels-vllm/nvidia_ml_py-12.560.30-py3-none-any.whl\r\n",
      "Installing collected packages: nvidia-ml-py\r\n",
      "  Attempting uninstall: nvidia-ml-py\r\n",
      "    Found existing installation: nvidia-ml-py 11.495.46\r\n",
      "    Uninstalling nvidia-ml-py-11.495.46:\r\n",
      "      Successfully uninstalled nvidia-ml-py-11.495.46\r\n",
      "Successfully installed nvidia-ml-py-12.560.30\r\n",
      "CPU times: user 2.07 s, sys: 589 ms, total: 2.66 s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip uninstall -y torch\n",
    "\n",
    "!pip install -q --no-index --find-links=/kaggle/input/wheels-vllm-0-6-3-post1 torchvision==0.19.1\n",
    "!pip install -q --no-index --find-links=/kaggle/input/wheels-vllm-0-6-3-post1 vllm\n",
    "\n",
    "!pip install -q -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "\n",
    "!pip uninstall -y pynvml\n",
    "!pip install --no-deps --no-index /kaggle/input/0-6-3-post1-wheels-vllm/nvidia_ml_py-12.560.30-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4251d36",
   "metadata": {
    "papermill": {
     "duration": 0.037353,
     "end_time": "2024-12-08T18:46:08.666803",
     "exception": false,
     "start_time": "2024-12-08T18:46:08.629450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CoT Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b40a6f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:46:08.745712Z",
     "iopub.status.busy": "2024-12-08T18:46:08.745395Z",
     "iopub.status.idle": "2024-12-08T18:46:08.753732Z",
     "shell.execute_reply": "2024-12-08T18:46:08.752812Z"
    },
    "papermill": {
     "duration": 0.049849,
     "end_time": "2024-12-08T18:46:08.755280",
     "exception": false,
     "start_time": "2024-12-08T18:46:08.705431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_gen_cot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_gen_cot.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import vllm\n",
    "print('vllm version=', vllm.__version__)\n",
    "\n",
    "from datasets import Dataset\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sp = \"Analyze the incorrect answer to detect flaws in the student's reasoning.\"\n",
    "\n",
    "\n",
    "def get_tokenizer(backbone_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(backbone_path, add_eos_token=True)\n",
    "\n",
    "    if tokenizer.eos_token == \"\":\n",
    "        tokenizer.add_special_tokens({\"eos_token\": \"</s>\"})\n",
    "        tokenizer.eos_token = \"</s>\"\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        if tokenizer.unk_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "            tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "        else:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    tokenizer.bos_token = \"<|im_start|>\"\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def is_nan(x): return x != x\n",
    "\n",
    "\n",
    "def eedi_process_df(df):\n",
    "    df = deepcopy(df)\n",
    "    grouped = df.groupby(\"query_id\")\n",
    "\n",
    "    question_dict = {}\n",
    "    for question_id, group in grouped:\n",
    "        question_data = group.to_dict(orient=\"records\")[0]\n",
    "        del question_data[\"query_id\"]\n",
    "        question_dict[question_id] = question_data\n",
    "\n",
    "    all_questions = list(question_dict.keys())\n",
    "\n",
    "    queries = []\n",
    "    for qid in all_questions:\n",
    "        info = question_dict[qid]\n",
    "\n",
    "        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if info[\"CorrectAnswer\"] == answer_key:\n",
    "                continue\n",
    "\n",
    "            this_example = dict()\n",
    "            this_key = f\"{qid}_{answer_key}\"\n",
    "            this_example[\"query_id\"] = this_key\n",
    "\n",
    "            for col in [\"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n",
    "                this_example[col] = info[col]\n",
    "\n",
    "            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n",
    "            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n",
    "            queries.append(this_example)\n",
    "    # --\n",
    "    query_df = pd.DataFrame(queries)\n",
    "    return query_df\n",
    "\n",
    "\n",
    "def main(cfg, save_dir, model_id):\n",
    "    # load data ---\n",
    "    test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "\n",
    "    if not os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "        n_ex = int(os.getenv(\"N_EX\"))\n",
    "        test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").head(n_ex)\n",
    "\n",
    "    test_df = test_df.rename(columns={\"QuestionId\": \"query_id\"})\n",
    "    test_df = eedi_process_df(test_df)\n",
    "\n",
    "    content_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "    id2name = dict(zip(content_df['MisconceptionId'], content_df['MisconceptionName']))\n",
    "\n",
    "\n",
    "    ds = Dataset.from_pandas(test_df)\n",
    "    print(f\"Number of examples: {len(ds)}\")\n",
    "    query_ids = ds[\"query_id\"]\n",
    "\n",
    "    print(\"==\" * 50)\n",
    "    print(f\"Generating for model: {cfg.model.backbone_path}\")\n",
    "    print(\"==\" * 50)\n",
    "\n",
    "    llm = vllm.LLM(\n",
    "        cfg.model.backbone_path,\n",
    "        tensor_parallel_size=2,\n",
    "        quantization=cfg.model.quantization,\n",
    "        gpu_memory_utilization=0.99,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=1296,\n",
    "        enable_prefix_caching=True\n",
    "    )\n",
    "\n",
    "    tokenizer = get_tokenizer(cfg.model.backbone_path)\n",
    "\n",
    "    prompts = []\n",
    "    for example in ds:\n",
    "        question = example[\"QuestionText\"]\n",
    "        correct_answer = example[\"CorrectAnswerText\"]\n",
    "        incorrect_answer = example[\"InCorrectAnswerText\"]\n",
    "\n",
    "        user_message = f\"Question: {question}\\nCorrect Answer: {correct_answer}\\nIncorrect Answer: {incorrect_answer}\"\n",
    "        text = f\"{sp}\\n\\nQuery: {user_message}\\nAnswer:\\n\"\n",
    "        prompts.append(text)\n",
    "\n",
    "    for p in prompts[:5]:\n",
    "        print(p)\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "    sampling_params = vllm.SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.0, max_tokens=cfg.max_new_tokens)\n",
    "\n",
    "    # do in chunks ---\n",
    "    response_dfs = []\n",
    "\n",
    "    chunk_size = 256\n",
    "    for i in range(0, len(prompts), chunk_size):\n",
    "        results = []\n",
    "        generated_texts = []\n",
    "        all_prompts = []\n",
    "\n",
    "        chunk_prompts = prompts[i : i + chunk_size]\n",
    "        chunk_query_ids = query_ids[i : i + chunk_size]\n",
    "\n",
    "        print(f\"Processing chunk {i//chunk_size + 1} of {(len(prompts)-1)//chunk_size + 1}\")\n",
    "\n",
    "        generations = llm.generate(chunk_prompts, sampling_params=sampling_params)\n",
    "\n",
    "        for output in generations:\n",
    "            prompt = output.prompt\n",
    "            generated_text = output.outputs[0].text\n",
    "            all_prompts.append(prompt)\n",
    "\n",
    "            full_text = f\"{prompt}{generated_text}\"\n",
    "            results.append(full_text)\n",
    "            generated_texts.append(generated_text)\n",
    "\n",
    "        # Save intermediate results\n",
    "        df = pd.DataFrame()\n",
    "        df[\"query_id\"] = chunk_query_ids\n",
    "        df[\"prompt\"] = all_prompts\n",
    "        df[\"cot\"] = generated_texts\n",
    "        response_dfs.append(df)\n",
    "\n",
    "    result_df = pd.concat(response_dfs).reset_index(drop=True)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"gen_{model_id}.parquet\")\n",
    "    result_df.to_parquet(save_path)\n",
    "    # ---\n",
    "    n = min(5, len(result_df))\n",
    "    samples = result_df.sample(n)['cot'].values.tolist()\n",
    "    for samp in samples:\n",
    "        print(samp)\n",
    "        print('--'*50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--config_path\", type=str, required=True)\n",
    "    ap.add_argument(\"--save_dir\", type=str, required=True)\n",
    "    ap.add_argument(\"--model_id\", type=str, required=True)\n",
    "\n",
    "    args = ap.parse_args()\n",
    "    cfg = OmegaConf.load(args.config_path)\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "    # execution ---\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        main(cfg, save_dir=args.save_dir, model_id=args.model_id)\n",
    "    elif cfg.run_on_save:\n",
    "        main(cfg, save_dir=args.save_dir, model_id=args.model_id)    \n",
    "    else:\n",
    "        result_df = pd.read_parquet(\"./gen/gen_qwen_7b.parquet\")\n",
    "        save_path = os.path.join(args.save_dir, f\"gen_{args.model_id}.parquet\")\n",
    "        result_df.to_parquet(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffb2bf51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:46:08.831872Z",
     "iopub.status.busy": "2024-12-08T18:46:08.831579Z",
     "iopub.status.idle": "2024-12-08T18:46:08.836591Z",
     "shell.execute_reply": "2024-12-08T18:46:08.835764Z"
    },
    "papermill": {
     "duration": 0.045318,
     "end_time": "2024-12-08T18:46:08.838499",
     "exception": false,
     "start_time": "2024-12-08T18:46:08.793181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conf_llm_gen_7b.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conf_llm_gen_7b.yaml\n",
    "run_on_save: true\n",
    "\n",
    "max_new_tokens: 256 # 384\n",
    "stage_one_path: retriever_outputs/stage_one_blended.parquet\n",
    "\n",
    "model:\n",
    "    backbone_path: \"/kaggle/input/eedi-cot-7b-base-dec4/transformers/default/1\"\n",
    "    max_length: 768\n",
    "    num_proc: 2\n",
    "    quantization:\n",
    "    \n",
    "    tokenizer:\n",
    "        padding_side: left\n",
    "        truncation_side: left\n",
    "        use_fast: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e1034c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:46:08.915349Z",
     "iopub.status.busy": "2024-12-08T18:46:08.915102Z",
     "iopub.status.idle": "2024-12-08T18:46:08.919891Z",
     "shell.execute_reply": "2024-12-08T18:46:08.919122Z"
    },
    "papermill": {
     "duration": 0.044904,
     "end_time": "2024-12-08T18:46:08.921510",
     "exception": false,
     "start_time": "2024-12-08T18:46:08.876606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conf_llm_gen_14b.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conf_llm_gen_14b.yaml\n",
    "run_on_save: false\n",
    "\n",
    "max_new_tokens: 256 # 384\n",
    "stage_one_path: retriever_outputs/stage_one_blended.parquet\n",
    "\n",
    "model:\n",
    "    backbone_path: \"/kaggle/input/eedi-cot-14b-dec6-awq/transformers/default/1\"\n",
    "    max_length: 768\n",
    "    num_proc: 2\n",
    "    quantization: awq\n",
    "     \n",
    "    tokenizer:\n",
    "        padding_side: left\n",
    "        truncation_side: left\n",
    "        use_fast: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ad5dc98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:46:08.999687Z",
     "iopub.status.busy": "2024-12-08T18:46:08.999420Z",
     "iopub.status.idle": "2024-12-08T18:46:09.004483Z",
     "shell.execute_reply": "2024-12-08T18:46:09.003507Z"
    },
    "papermill": {
     "duration": 0.04616,
     "end_time": "2024-12-08T18:46:09.006196",
     "exception": false,
     "start_time": "2024-12-08T18:46:08.960036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conf_llm_gen_32b.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conf_llm_gen_32b.yaml\n",
    "run_on_save: false\n",
    "\n",
    "max_new_tokens: 256\n",
    "stage_one_path: retriever_outputs/stage_one_blended.parquet\n",
    "\n",
    "model:\n",
    "    backbone_path: \"/kaggle/input/eedi-cot-32b-dec6-awq/transformers/default/1\"\n",
    "    max_length: 768\n",
    "    num_proc: 2\n",
    "    quantization: awq\n",
    "    \n",
    "    tokenizer:\n",
    "        padding_side: left\n",
    "        truncation_side: left\n",
    "        use_fast: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d98f915",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-08T18:46:09.084916Z",
     "iopub.status.busy": "2024-12-08T18:46:09.084696Z",
     "iopub.status.idle": "2024-12-08T18:48:39.490061Z",
     "shell.execute_reply": "2024-12-08T18:48:39.488842Z"
    },
    "papermill": {
     "duration": 150.446795,
     "end_time": "2024-12-08T18:48:39.491980",
     "exception": false,
     "start_time": "2024-12-08T18:46:09.045185",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm version= 0.6.3.post1\r\n",
      "Number of examples: 12\r\n",
      "====================================================================================================\r\n",
      "Generating for model: /kaggle/input/eedi-cot-7b-base-dec4/transformers/default/1\r\n",
      "====================================================================================================\r\n",
      "WARNING 12-08 18:46:18 config.py:1668] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-08 18:46:27 config.py:905] Defaulting to use mp for distributed inference\r\n",
      "WARNING 12-08 18:46:27 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 12-08 18:46:27 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/eedi-cot-7b-base-dec4/transformers/default/1', speculative_config=None, tokenizer='/kaggle/input/eedi-cot-7b-base-dec4/transformers/default/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1296, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/eedi-cot-7b-base-dec4/transformers/default/1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\r\n",
      "WARNING 12-08 18:46:28 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 12-08 18:46:28 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "INFO 12-08 18:46:28 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-08 18:46:28 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m INFO 12-08 18:46:28 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m INFO 12-08 18:46:28 selector.py:115] Using XFormers backend.\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m INFO 12-08 18:46:29 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "INFO 12-08 18:46:30 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m INFO 12-08 18:46:30 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m INFO 12-08 18:46:30 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-08 18:46:30 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-08 18:46:30 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-08 18:46:48 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m INFO 12-08 18:46:48 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "WARNING 12-08 18:46:48 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m WARNING 12-08 18:46:48 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 12-08 18:46:48 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7965f09a4e20>, local_subscribe_port=45139, remote_subscribe_port=None)\r\n",
      "INFO 12-08 18:46:48 model_runner.py:1056] Starting to load model /kaggle/input/eedi-cot-7b-base-dec4/transformers/default/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m INFO 12-08 18:46:48 model_runner.py:1056] Starting to load model /kaggle/input/eedi-cot-7b-base-dec4/transformers/default/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m INFO 12-08 18:46:49 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m INFO 12-08 18:46:49 selector.py:115] Using XFormers backend.\r\n",
      "INFO 12-08 18:46:49 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-08 18:46:49 selector.py:115] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:26<01:18, 26.10s/it]\r\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:52<00:52, 26.17s/it]\r\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:16, 16.25s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:24<00:00, 20.76s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:24<00:00, 21.11s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m INFO 12-08 18:48:13 model_runner.py:1067] Loading model weights took 7.1145 GB\r\n",
      "INFO 12-08 18:48:14 model_runner.py:1067] Loading model weights took 7.1145 GB\r\n",
      "INFO 12-08 18:48:15 distributed_gpu_executor.py:57] # GPU blocks: 12009, # CPU blocks: 9362\r\n",
      "INFO 12-08 18:48:15 distributed_gpu_executor.py:61] Maximum concurrency for 1296 tokens per request: 148.26x\r\n",
      "Analyze the incorrect answer to detect flaws in the student's reasoning.\r\n",
      "\r\n",
      "Query: Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal \\( 13 \\) ?\r\n",
      "Correct Answer: \\( 3 \\times(2+4)-5 \\)\r\n",
      "Incorrect Answer: \\( 3 \\times 2+(4-5) \\)\r\n",
      "Answer:\r\n",
      "\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "Analyze the incorrect answer to detect flaws in the student's reasoning.\r\n",
      "\r\n",
      "Query: Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal \\( 13 \\) ?\r\n",
      "Correct Answer: \\( 3 \\times(2+4)-5 \\)\r\n",
      "Incorrect Answer: \\( 3 \\times(2+4-5) \\)\r\n",
      "Answer:\r\n",
      "\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "Analyze the incorrect answer to detect flaws in the student's reasoning.\r\n",
      "\r\n",
      "Query: Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal \\( 13 \\) ?\r\n",
      "Correct Answer: \\( 3 \\times(2+4)-5 \\)\r\n",
      "Incorrect Answer: Does not need brackets\r\n",
      "Answer:\r\n",
      "\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "Analyze the incorrect answer to detect flaws in the student's reasoning.\r\n",
      "\r\n",
      "Query: Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\r\n",
      "Correct Answer: Does not simplify\r\n",
      "Incorrect Answer: \\( m+1 \\)\r\n",
      "Answer:\r\n",
      "\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "Analyze the incorrect answer to detect flaws in the student's reasoning.\r\n",
      "\r\n",
      "Query: Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\r\n",
      "Correct Answer: Does not simplify\r\n",
      "Incorrect Answer: \\( m+2 \\)\r\n",
      "Answer:\r\n",
      "\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "Processing chunk 1 of 1\r\n",
      "Processed prompts: 100%|█| 12/12 [00:06<00:00,  1.75it/s, est. speed input: 170.\r\n",
      "The student incorrectly concluded that halving the values would maintain the range because they failed to recognize that when each value is divided by 2, the difference between the maximum and minimum values is also halved. This led them to believe that the range would remain unchanged, when in fact it would be reduced to half its original size.\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "The student incorrectly believed that the diagonals of a rectangle could be at any angle, failing to recognize that diagonals in a rectangle always bisect each other at right angles. This fundamental property of rectangles led them to conclude there wasn't enough information to determine the angle between the diagonals, when in fact the diagonals of any rectangle must intersect at 90°.\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "The student incorrectly attempted to simplify the expression by dividing the numerator \\( m^2 + 2m - 3 \\) by the denominator \\( m - 3 \\), treating it like a division problem rather than recognizing it as a fraction. This led them to incorrectly factor the numerator as \\( (m-3)(m+2) \\) and then cancel out the \\( m-3 \\) terms, resulting in \\( m+2 \\). This specific error shows they don't understand that simplifying a fraction means factoring and canceling common factors, not performing division between the numerator and denominator.\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "The student incorrectly concluded that the angles formed by the diagonals of a rectangle must be obtuse because they assumed that obtuse angles are more common than acute angles in geometric shapes. This flawed reasoning ignores that in a rectangle, the diagonals always intersect at right angles (90°), regardless of the rectangle's dimensions, making it impossible for the angles to be obtuse.\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "The student likely calculated 3 × 2 + 4 - 5 = 6 + 4 - 5 = 5, which equals the desired answer of 13, but failed to recognize that the original expression 3 × 2 + 4 - 5 without brackets would give the wrong answer. This reveals they don't understand that the expression 3 × 2 + 4 - 5 actually evaluates to 5, and therefore don't grasp that brackets are needed to change the order of operations to get 13.\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "INFO 12-08 18:48:31 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=276)\u001b[0;0m INFO 12-08 18:48:31 multiproc_worker_utils.py:240] Worker exiting\r\n",
      "CPU times: user 1.96 s, sys: 453 ms, total: 2.41 s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python run_gen_cot.py --config_path conf_llm_gen_7b.yaml --save_dir ./gen --model_id qwen_7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e03d8fbe",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-08T18:48:39.575387Z",
     "iopub.status.busy": "2024-12-08T18:48:39.575098Z",
     "iopub.status.idle": "2024-12-08T18:48:51.427228Z",
     "shell.execute_reply": "2024-12-08T18:48:51.426100Z"
    },
    "papermill": {
     "duration": 11.896666,
     "end_time": "2024-12-08T18:48:51.429215",
     "exception": false,
     "start_time": "2024-12-08T18:48:39.532549",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm version= 0.6.3.post1\r\n",
      "CPU times: user 137 ms, sys: 33.9 ms, total: 171 ms\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python run_gen_cot.py --config_path conf_llm_gen_14b.yaml --save_dir ./gen --model_id qwen_14b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "048546b8",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-08T18:48:51.512378Z",
     "iopub.status.busy": "2024-12-08T18:48:51.512098Z",
     "iopub.status.idle": "2024-12-08T18:49:01.820962Z",
     "shell.execute_reply": "2024-12-08T18:49:01.819859Z"
    },
    "papermill": {
     "duration": 10.352325,
     "end_time": "2024-12-08T18:49:01.823138",
     "exception": false,
     "start_time": "2024-12-08T18:48:51.470813",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm version= 0.6.3.post1\r\n",
      "CPU times: user 126 ms, sys: 33.1 ms, total: 159 ms\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python run_gen_cot.py --config_path conf_llm_gen_32b.yaml --save_dir ./gen --model_id qwen_32b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47c7480e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:49:01.911909Z",
     "iopub.status.busy": "2024-12-08T18:49:01.911560Z",
     "iopub.status.idle": "2024-12-08T18:49:01.915495Z",
     "shell.execute_reply": "2024-12-08T18:49:01.914684Z"
    },
    "papermill": {
     "duration": 0.04973,
     "end_time": "2024-12-08T18:49:01.917207",
     "exception": false,
     "start_time": "2024-12-08T18:49:01.867477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for 256 inputs\n",
    "# 7b gen time  : ~2 mins\n",
    "# 14b gen time : \n",
    "# 32b gen time : ~12mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cf712d",
   "metadata": {
    "papermill": {
     "duration": 0.040433,
     "end_time": "2024-12-08T18:49:01.998542",
     "exception": false,
     "start_time": "2024-12-08T18:49:01.958109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stage 2: Re-Ranker (Haiku)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4ea11",
   "metadata": {
    "papermill": {
     "duration": 0.039914,
     "end_time": "2024-12-08T18:49:02.079105",
     "exception": false,
     "start_time": "2024-12-08T18:49:02.039191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1 Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4a2aaeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:49:02.162347Z",
     "iopub.status.busy": "2024-12-08T18:49:02.162067Z",
     "iopub.status.idle": "2024-12-08T18:49:02.171632Z",
     "shell.execute_reply": "2024-12-08T18:49:02.170799Z"
    },
    "papermill": {
     "duration": 0.053718,
     "end_time": "2024-12-08T18:49:02.173492",
     "exception": false,
     "start_time": "2024-12-08T18:49:02.119774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_haiku.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_haiku.py\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/kaggle/input/eedi-utils-v11\")\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import os\n",
    "import vllm\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from llm_oracle.ranker_dataset import RankerDataset\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def is_nan(x): return x != x\n",
    "    \n",
    "def stable_softmax(x, temp=1.0):\n",
    "    x = np.array(x) / temp\n",
    "    x_max = np.max(x)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "    \n",
    "def eedi_process_df(df):\n",
    "    df = df.copy()\n",
    "    df = df.rename(columns={\"QuestionId\": \"query_id\"})\n",
    "    grouped = df.groupby(\"query_id\")\n",
    "\n",
    "    question_dict = {}\n",
    "    for question_id, group in grouped:\n",
    "        question_data = group.to_dict(orient=\"records\")[0]\n",
    "        del question_data[\"query_id\"]\n",
    "        question_dict[question_id] = question_data\n",
    "\n",
    "    all_questions = list(question_dict.keys())\n",
    "\n",
    "    queries = []\n",
    "    query2content = defaultdict(list)\n",
    "    content2query = defaultdict(list)\n",
    "\n",
    "    # ---\n",
    "    for qid in all_questions:\n",
    "        info = question_dict[qid]\n",
    "\n",
    "        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if info[\"CorrectAnswer\"] == answer_key:\n",
    "                continue\n",
    "            this_example = dict()\n",
    "            this_key = f\"{qid}_{answer_key}\"\n",
    "            this_example[\"query_id\"] = this_key\n",
    "\n",
    "            if is_nan(info[f\"Misconception{answer_key}Id\"]):\n",
    "                continue\n",
    "\n",
    "            mid = str(int(info[f\"Misconception{answer_key}Id\"]))\n",
    "            query2content[this_key].append(mid)\n",
    "            content2query[mid].append(this_key)\n",
    "\n",
    "            # ---\n",
    "            for col in [\"SubjectId\", \"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n",
    "                this_example[col] = info[col]\n",
    "\n",
    "            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n",
    "            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n",
    "            this_example[\"AllOptionText\"] = \"\\n- \".join([info[f\"Answer{x}Text\"] for x in [\"A\", \"B\", \"C\", \"D\"]])\n",
    "            this_example[\"AllOptionText\"] = f\"\\n- {this_example['AllOptionText']}\"\n",
    "            queries.append(this_example)\n",
    "    # --\n",
    "    query_df = pd.DataFrame(queries)\n",
    "    corr_df = pd.Series(query2content).reset_index().rename(columns={\"index\": \"query_id\", 0: \"content_id\"})\n",
    "    corr_df[\"content_id\"] = corr_df[\"content_id\"].apply(lambda x: x[0])\n",
    "\n",
    "    query_df = query_df.reset_index(drop=True)\n",
    "\n",
    "    return query_df, corr_df, content2query\n",
    "\n",
    "def sort_by_scores(pred_ids, scores):\n",
    "    keep_idxs = np.argsort(-np.array(scores)).tolist()\n",
    "    ret_ids = [pred_ids[idx] for idx in keep_idxs]\n",
    "    ret_scores = [scores[idx] for idx in keep_idxs]\n",
    "    return {\"sorted_ids\": ret_ids, \"sorted_scores\": ret_scores}\n",
    "\n",
    "\n",
    "def format_example(row):\n",
    "    example = f\"Question: {row['QuestionText']}\\nAnswer:{row['CorrectAnswerText']}\\nMisconception Answer: {row['InCorrectAnswerText']}\"\n",
    "    return example\n",
    "\n",
    "\n",
    "def add_fs_examples(df, content2query, query2example, rng, n=2):\n",
    "    cache = {}\n",
    "    def _add_examples(row):\n",
    "        cid = row[\"content_id\"]\n",
    "        if cid in cache:\n",
    "            return cache[cid]\n",
    "        else:\n",
    "            qids = content2query[cid]\n",
    "            qids = [qid for qid in qids if qid != row[\"query_id\"]]\n",
    "            if len(qids) == 0:\n",
    "                cache[cid] = \"\"\n",
    "                return \"\"\n",
    "\n",
    "            qids = rng.sample(qids, k=min(n, len(qids)))\n",
    "            examples = [query2example[qid] for qid in qids]\n",
    "            fs = \"\\n--\\n\".join(examples)\n",
    "            cache[cid] = fs\n",
    "            return fs\n",
    "\n",
    "    df[\"examples\"] = df.apply(_add_examples, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def main(cfg, save_dir, model_id):\n",
    "    test_df = pd.read_parquet(cfg.input_path)\n",
    "    test_df = test_df.rename(columns={\"QuestionId_Answer\": \"query_id\", \"MisconceptionId\": \"content_id\"})\n",
    "    test_df[\"content_id\"] = test_df[\"content_id\"].astype(str)\n",
    "\n",
    "    # comp data examples ---\n",
    "    rng = random.Random(cfg.seed)\n",
    "    comp_df = pd.read_csv(cfg.icl_path).rename(columns={\"QuestionId\": \"query_id\"})\n",
    "\n",
    "    query_df, _, content2query = eedi_process_df(comp_df)\n",
    "    query_df[\"demo\"] = query_df.apply(format_example, axis=1)\n",
    "    query2example = dict(zip(query_df[\"query_id\"], query_df[\"demo\"]))\n",
    "\n",
    "    # add few shot examples --\n",
    "    test_df = add_fs_examples(test_df, content2query, query2example, rng, n=cfg.k_shot)\n",
    "    test_df = test_df.sort_values(by='content_id').reset_index(drop=True)\n",
    "\n",
    "    # cot --\n",
    "    if cfg.use_cot:\n",
    "        print(\"Loading CoT....\")\n",
    "        cot_df = pd.read_parquet(cfg.cot_path)\n",
    "        test_df = test_df.merge(cot_df, on='query_id', how='left')\n",
    "        num_missing = test_df[\"cot\"].isna().sum()\n",
    "        print(f\"# of missing cot: {num_missing}\")\n",
    "        test_df[\"cot\"] = test_df[\"cot\"].fillna(\"\")\n",
    "        print(test_df.sample().T.to_dict())\n",
    "        print(\"--\"*40)\n",
    "\n",
    "    #---\n",
    "    dataset_creator = RankerDataset(cfg)\n",
    "    infer_ds = dataset_creator.get_dataset(test_df)\n",
    "    \n",
    "    tokenizer = dataset_creator.tokenizer    \n",
    "    infer_qa_ids = infer_ds[\"query_id\"]\n",
    "    infer_mc_ids = infer_ds[\"content_id\"]\n",
    "    \n",
    "    infer_ds = infer_ds.map(lambda example: {\"prompt\": tokenizer.decode(example['input_ids'], skip_special_tokens=False)})\n",
    "    prompts = infer_ds['prompt']\n",
    "\n",
    "    print(f\"# of requests: {len(prompts)}\")\n",
    "    print(f\"Example:\\n\\n{prompts[0]}\")\n",
    "    \n",
    "    # -- in\n",
    "    llm = vllm.LLM(\n",
    "        cfg.model.backbone_path,\n",
    "        quantization=\"awq\",\n",
    "        tensor_parallel_size=2,\n",
    "        gpu_memory_utilization=0.99,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2048,\n",
    "        enable_prefix_caching=True,\n",
    "    )\n",
    "    \n",
    "    sampling_params = vllm.SamplingParams(n=1, top_p=0.8, logprobs=20, max_tokens=1, temperature=0.0, skip_special_tokens=False)\n",
    "    responses = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "\n",
    "    # Get Results\n",
    "    print(\"--\"*40)\n",
    "    yes_tok_id = tokenizer(\"Yes\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "    no_tok_id = tokenizer(\"No\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "    \n",
    "    print(f\">> EediRanker: Yes token id: {yes_tok_id} | Expected: 9454\")\n",
    "    print(f\">> EediRanker: No token id: {no_tok_id} | Expected: 2753\")\n",
    "    print(\"--\"*40)\n",
    "    \n",
    "    QuestionId_Answer = []\n",
    "    MisconceptionId =[]\n",
    "    scores = []\n",
    "    \n",
    "    for qid, cid, response in zip(infer_qa_ids, infer_mc_ids, responses):\n",
    "        logprob_dict = response.outputs[0].logprobs[0]\n",
    "\n",
    "        top_tok_ids = set(list(logprob_dict.keys()))\n",
    "        if len(top_tok_ids.intersection(set([yes_tok_id, no_tok_id]))) == 0:\n",
    "            print(f\"Bad Output for {qid} - {cid}\")\n",
    "            continue\n",
    "        \n",
    "        yes_logit, no_logit = -10.0, -10.0\n",
    "        \n",
    "        if yes_tok_id in logprob_dict:\n",
    "            yes_logit = logprob_dict[yes_tok_id].logprob\n",
    "        \n",
    "        if no_tok_id in logprob_dict:\n",
    "            no_logit = logprob_dict[no_tok_id].logprob\n",
    "        \n",
    "        score = yes_logit - no_logit\n",
    "        \n",
    "        QuestionId_Answer.append(qid)\n",
    "        MisconceptionId.append(cid)\n",
    "        scores.append(score)\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"QuestionId_Answer\"] = QuestionId_Answer\n",
    "    result_df[\"MisconceptionId\"] = MisconceptionId\n",
    "    result_df[\"score\"] = scores\n",
    "    \n",
    "    agg_df = result_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n",
    "    score_agg_df = result_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n",
    "    agg_df = pd.merge(agg_df, score_agg_df, on=\"QuestionId_Answer\", how=\"left\")\n",
    "    \n",
    "    agg_df[\"topk_info\"] = agg_df.apply(lambda x: sort_by_scores(x[\"MisconceptionId\"], x[\"score\"]), axis=1)\n",
    "    agg_df[\"MisconceptionId\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_ids\"])\n",
    "    agg_df[\"score\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_scores\"])\n",
    "    \n",
    "    # compute oof dataframe ---\n",
    "    oof_df = agg_df.copy()\n",
    "    oof_df = oof_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n",
    "    oof_df = oof_df.rename(columns={\"score\": \"logit_scores\"})\n",
    "\n",
    "    # normalize ---\n",
    "    oof_df[\"pred_scores\"] = oof_df[\"logit_scores\"].apply(stable_softmax)\n",
    "    oof_df[\"MisconceptionId\"] = oof_df[\"MisconceptionId\"].apply(lambda x: list(map(str, x)))\n",
    "\n",
    "    # print ---\n",
    "    print(\"--\"*40)\n",
    "    row = oof_df.sample()\n",
    "    formatted_scores = [f\"{s:.3f}\" for s in row['pred_scores'].values[0]]\n",
    "    misconceptions = row['MisconceptionId'].values[0]\n",
    "    print(f\"Showing 1 example: {row['QuestionId_Answer'].values[0]}\")\n",
    "    for rank, (m, s) in enumerate(zip(misconceptions, formatted_scores)):\n",
    "        print(f\"MisconceptionId: {m} -> Score: {s}\")\n",
    "    print(\"--\"*40)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"ranker_{model_id}.parquet\")\n",
    "    oof_df.to_parquet(save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('--config_path', type=str, required=True)\n",
    "    ap.add_argument('--save_dir', type=str, required=True)\n",
    "    ap.add_argument('--model_id', type=str, required=True)\n",
    "\n",
    "    args = ap.parse_args()\n",
    "    cfg = OmegaConf.load(args.config_path)\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "    # execution ---\n",
    "    main(cfg, save_dir=args.save_dir, model_id=args.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af8a99",
   "metadata": {
    "papermill": {
     "duration": 0.039746,
     "end_time": "2024-12-08T18:49:02.253902",
     "exception": false,
     "start_time": "2024-12-08T18:49:02.214156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Re-Ranker Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f78f2dc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:49:02.336121Z",
     "iopub.status.busy": "2024-12-08T18:49:02.335866Z",
     "iopub.status.idle": "2024-12-08T18:49:02.341068Z",
     "shell.execute_reply": "2024-12-08T18:49:02.340391Z"
    },
    "papermill": {
     "duration": 0.048292,
     "end_time": "2024-12-08T18:49:02.342681",
     "exception": false,
     "start_time": "2024-12-08T18:49:02.294389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conf_llm_oracle_14b_awq.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conf_llm_oracle_14b_awq.yaml\n",
    "\n",
    "seed: 675\n",
    "input_path: ./retriever_outputs/ranker_input_stage_one.parquet\n",
    "icl_path: /kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\n",
    "\n",
    "k_shot: 2\n",
    "use_cot: false\n",
    "cot_path: ./gen/gen_qwen_14b.parquet\n",
    "\n",
    "model:\n",
    "    backbone_path: \"/kaggle/input/eedi-oracle-14b-dec7-cv646-awq-ff/transformers/default/1\"\n",
    "    max_length: 768\n",
    "    num_proc: 2\n",
    "    \n",
    "    tokenizer:\n",
    "        padding_side: left\n",
    "        truncation_side: left\n",
    "        use_fast: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12b4f5",
   "metadata": {
    "papermill": {
     "duration": 0.041914,
     "end_time": "2024-12-08T18:49:02.425179",
     "exception": false,
     "start_time": "2024-12-08T18:49:02.383265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Re-Ranker Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2838e3ea",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-08T18:49:02.508976Z",
     "iopub.status.busy": "2024-12-08T18:49:02.508699Z",
     "iopub.status.idle": "2024-12-08T18:52:32.945153Z",
     "shell.execute_reply": "2024-12-08T18:52:32.944108Z"
    },
    "papermill": {
     "duration": 210.480603,
     "end_time": "2024-12-08T18:52:32.947136",
     "exception": false,
     "start_time": "2024-12-08T18:49:02.466533",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Llama tokenizer\r\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Map (num_proc=2): 100%|███████████████| 390/390 [00:01<00:00, 315.47 examples/s]\r\n",
      "Map: 100%|████████████████████████████| 390/390 [00:00<00:00, 479.49 examples/s]\r\n",
      "# of requests: 390\r\n",
      "Example:\r\n",
      "\r\n",
      "<|im_start|>system\r\n",
      "You are an expert in detecting grade-school level math misconceptions. Verify if the incorrect answer stems from the provided misconception.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Misconception: Thinks that a number in front of a bracket means divide\r\n",
      "\r\n",
      "Demos for the misconception:\r\n",
      "Question: I think of a number. I take away six and then divide the result by three. The answer I get is \\( 21 \\).\r\n",
      "Which of the following equations represents this?\r\n",
      "Answer:\\( \\frac{n-6}{3}=21 \\)\r\n",
      "Misconception Answer: \\( 3(n-6)=21 \\)\r\n",
      "\r\n",
      "Subject: BIDMAS\r\n",
      "Topic: Use the order of operations to carry out calculations involving powers\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal \\( 13 \\) ?\r\n",
      "Correct Answer: \\( 3 \\times(2+4)-5 \\)\r\n",
      "Incorrect Answer: \\( 3 \\times 2+(4-5) \\)\r\n",
      "Does the misconception (Thinks that a number in front of a bracket means divide) lead to the incorrect answer? (Yes/No)<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "WARNING 12-08 18:49:25 config.py:321] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-08 18:49:25 config.py:905] Defaulting to use mp for distributed inference\r\n",
      "WARNING 12-08 18:49:25 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 12-08 18:49:25 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/eedi-oracle-14b-dec7-cv646-awq-ff/transformers/default/1', speculative_config=None, tokenizer='/kaggle/input/eedi-oracle-14b-dec7-cv646-awq-ff/transformers/default/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/eedi-oracle-14b-dec7-cv646-awq-ff/transformers/default/1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\r\n",
      "WARNING 12-08 18:49:25 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 12-08 18:49:25 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "INFO 12-08 18:49:26 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m INFO 12-08 18:49:26 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m INFO 12-08 18:49:26 selector.py:115] Using XFormers backend.\r\n",
      "INFO 12-08 18:49:26 selector.py:115] Using XFormers backend.\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m INFO 12-08 18:49:26 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m INFO 12-08 18:49:27 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-08 18:49:27 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-08 18:49:27 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m INFO 12-08 18:49:27 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-08 18:49:28 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m INFO 12-08 18:49:28 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "WARNING 12-08 18:49:28 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m WARNING 12-08 18:49:28 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 12-08 18:49:28 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f9064df9540>, local_subscribe_port=41895, remote_subscribe_port=None)\r\n",
      "INFO 12-08 18:49:28 model_runner.py:1056] Starting to load model /kaggle/input/eedi-oracle-14b-dec7-cv646-awq-ff/transformers/default/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m INFO 12-08 18:49:28 model_runner.py:1056] Starting to load model /kaggle/input/eedi-oracle-14b-dec7-cv646-awq-ff/transformers/default/1...\r\n",
      "INFO 12-08 18:49:28 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-08 18:49:28 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m INFO 12-08 18:49:28 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m INFO 12-08 18:49:28 selector.py:115] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:08<00:16,  8.16s/it]\r\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:45<00:25, 25.14s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [01:20<00:00, 29.63s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [01:20<00:00, 26.72s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m INFO 12-08 18:50:49 model_runner.py:1067] Loading model weights took 4.8324 GB\r\n",
      "INFO 12-08 18:50:49 model_runner.py:1067] Loading model weights took 4.8324 GB\r\n",
      "INFO 12-08 18:50:51 distributed_gpu_executor.py:57] # GPU blocks: 4954, # CPU blocks: 2730\r\n",
      "INFO 12-08 18:50:51 distributed_gpu_executor.py:61] Maximum concurrency for 2048 tokens per request: 38.70x\r\n",
      "Processed prompts: 100%|█| 390/390 [01:28<00:00,  4.39it/s, est. speed input: 12\r\n",
      "--------------------------------------------------------------------------------\r\n",
      ">> EediRanker: Yes token id: 9454 | Expected: 9454\r\n",
      ">> EediRanker: No token id: 2753 | Expected: 2753\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Showing 1 example: 2_A\r\n",
      "MisconceptionId: 1287 -> Score: 0.927\r\n",
      "MisconceptionId: 1073 -> Score: 0.023\r\n",
      "MisconceptionId: 1059 -> Score: 0.019\r\n",
      "MisconceptionId: 557 -> Score: 0.006\r\n",
      "MisconceptionId: 1866 -> Score: 0.006\r\n",
      "MisconceptionId: 1677 -> Score: 0.005\r\n",
      "MisconceptionId: 397 -> Score: 0.002\r\n",
      "MisconceptionId: 2319 -> Score: 0.002\r\n",
      "MisconceptionId: 691 -> Score: 0.002\r\n",
      "MisconceptionId: 1521 -> Score: 0.002\r\n",
      "MisconceptionId: 2471 -> Score: 0.002\r\n",
      "MisconceptionId: 2408 -> Score: 0.001\r\n",
      "MisconceptionId: 632 -> Score: 0.001\r\n",
      "MisconceptionId: 1306 -> Score: 0.001\r\n",
      "MisconceptionId: 676 -> Score: 0.001\r\n",
      "MisconceptionId: 1150 -> Score: 0.000\r\n",
      "MisconceptionId: 2551 -> Score: 0.000\r\n",
      "MisconceptionId: 912 -> Score: 0.000\r\n",
      "MisconceptionId: 2188 -> Score: 0.000\r\n",
      "MisconceptionId: 2575 -> Score: 0.000\r\n",
      "MisconceptionId: 2439 -> Score: 0.000\r\n",
      "MisconceptionId: 1923 -> Score: 0.000\r\n",
      "MisconceptionId: 2177 -> Score: 0.000\r\n",
      "MisconceptionId: 129 -> Score: 0.000\r\n",
      "MisconceptionId: 365 -> Score: 0.000\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "INFO 12-08 18:52:26 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=504)\u001b[0;0m INFO 12-08 18:52:26 multiproc_worker_utils.py:240] Worker exiting\r\n",
      "CPU times: user 2.68 s, sys: 657 ms, total: 3.33 s\n",
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python run_haiku.py --config_path \"./conf_llm_oracle_14b_awq.yaml\" --save_dir \"./ranker_outputs\" --model_id \"qwen_14b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db0273",
   "metadata": {
    "papermill": {
     "duration": 0.04496,
     "end_time": "2024-12-08T18:52:33.037896",
     "exception": false,
     "start_time": "2024-12-08T18:52:32.992936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.4 Blend Retriever and Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a740e696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:52:33.128823Z",
     "iopub.status.busy": "2024-12-08T18:52:33.127997Z",
     "iopub.status.idle": "2024-12-08T18:52:33.135540Z",
     "shell.execute_reply": "2024-12-08T18:52:33.134616Z"
    },
    "papermill": {
     "duration": 0.055476,
     "end_time": "2024-12-08T18:52:33.137363",
     "exception": false,
     "start_time": "2024-12-08T18:52:33.081887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing blend_one_two.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile blend_one_two.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def get_sorted_pairs(content_ids, scores):\n",
    "    collection = [(cid, s) for cid, s in zip(content_ids, scores)]\n",
    "    sorted_collection = sorted(collection, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_collection\n",
    "\n",
    "def cut_at_n(sub_df, n=25):\n",
    "    sub_df[\"MisconceptionId\"] = sub_df[\"MisconceptionId\"].apply(lambda x: x[:n])\n",
    "    sub_df[\"score\"] = sub_df[\"score\"].apply(lambda x: x[:n])\n",
    "    return sub_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config-path\", type=str)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    with open(args.config_path, \"r\") as f:\n",
    "        cfg = OmegaConf.load(f)\n",
    "\n",
    "    # read predictions ---\n",
    "    ret_preds = pd.read_parquet(cfg.ret_path)\n",
    "    ranker_preds = pd.read_parquet(cfg.ranker_path)\n",
    "    print(ranker_preds.sample(3))\n",
    "\n",
    "    # flatten ---\n",
    "    ret_preds = ret_preds[[\"QuestionId_Answer\", \"MisconceptionId\", \"pred_scores\"]].explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n",
    "    ret_preds = ret_preds.rename(columns={\"pred_scores\": \"score_ret\"})\n",
    "    ret_preds[\"MisconceptionId\"] = ret_preds[\"MisconceptionId\"].astype(str)\n",
    "\n",
    "    ranker_preds = ranker_preds[[\"QuestionId_Answer\", \"MisconceptionId\", \"pred_scores\"]].explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n",
    "    ranker_preds = ranker_preds.rename(columns={\"pred_scores\": \"score_ranker\"})\n",
    "    ranker_preds[\"MisconceptionId\"] = ranker_preds[\"MisconceptionId\"].astype(str)\n",
    "\n",
    "    # blend ---\n",
    "    w_ret = cfg.ret_weight\n",
    "    w_ranker = cfg.ranker_weight\n",
    "\n",
    "    candidate_df = pd.merge(ret_preds, ranker_preds, on=[\"QuestionId_Answer\", \"MisconceptionId\"])\n",
    "    candidate_df[\"score\"] = candidate_df.apply(lambda x: w_ret*x['score_ret'] + w_ranker*x['score_ranker'], axis=1) # blending\n",
    "    candidate_df = candidate_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n",
    "\n",
    "    cdf = candidate_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n",
    "    sdf = candidate_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n",
    "    candidate_df = pd.merge(cdf, sdf, on=\"QuestionId_Answer\", how=\"left\")\n",
    "\n",
    "    candidate_df[\"sorted\"] = candidate_df.apply(lambda x: get_sorted_pairs(x['MisconceptionId'], x['score']), axis=1)\n",
    "    candidate_df[\"MisconceptionId\"] = candidate_df[\"sorted\"].apply(lambda x: [y[0] for y in x])\n",
    "    candidate_df[\"score\"] = candidate_df[\"sorted\"].apply(lambda x: [y[1] for y in x])\n",
    "    candidate_df = candidate_df.drop(columns=['sorted'])\n",
    "    \n",
    "    print(\"--\"*40)\n",
    "    print(f\"saving retriever+ranker prediction to: {cfg.blended_pred_path}\")\n",
    "    candidate_df.to_parquet(cfg.blended_pred_path)\n",
    "    print(\"Example:\")\n",
    "    print(candidate_df.sample().T)\n",
    "    print(\"--\"*40)\n",
    "\n",
    "    # Cut at N ---\n",
    "    candidate_df = cut_at_n(candidate_df, n=cfg.cutoff_n)\n",
    "    input_df = pd.read_parquet(cfg.ranker_input_file)\n",
    "    print(f\"Shape of ranker input previously: {input_df.shape}\")\n",
    "    keep_df = candidate_df[['QuestionId_Answer', 'MisconceptionId']].explode(\"MisconceptionId\").reset_index(drop=True)\n",
    "    keep_df['MisconceptionId'] = keep_df['MisconceptionId'].astype(input_df[\"MisconceptionId\"].dtype)\n",
    "    input_df = input_df.merge(keep_df, on=[\"QuestionId_Answer\", \"MisconceptionId\"], how=\"inner\")\n",
    "    print(f\"shape of ranker input for next stage: {input_df.shape}\")\n",
    "\n",
    "    # # Prepare further ranking ---\n",
    "    save_path = cfg.reranker_input_path\n",
    "    print(f\"saving output to: {save_path}\")\n",
    "    input_df.to_parquet(save_path)\n",
    "    print(f\"shape of input_df output: {input_df.shape}\")\n",
    "    print(\"--\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aca0a99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:52:33.228430Z",
     "iopub.status.busy": "2024-12-08T18:52:33.228176Z",
     "iopub.status.idle": "2024-12-08T18:52:33.233202Z",
     "shell.execute_reply": "2024-12-08T18:52:33.232465Z"
    },
    "papermill": {
     "duration": 0.052124,
     "end_time": "2024-12-08T18:52:33.234887",
     "exception": false,
     "start_time": "2024-12-08T18:52:33.182763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing one_two_blend.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile one_two_blend.yaml\n",
    "\n",
    "ranker_input_file: ./retriever_outputs/ranker_input_stage_one.parquet\n",
    "\n",
    "ret_path: ./retriever_outputs/stage_one_blended.parquet\n",
    "ranker_path: ./ranker_outputs/ranker_qwen_14b.parquet\n",
    "\n",
    "ret_weight: 0.1\n",
    "ranker_weight: 1.0\n",
    "\n",
    "cutoff_n: 8 # recall ~0.82\n",
    "\n",
    "blended_pred_path: ./ranker_outputs/one_two_blended.parquet\n",
    "reranker_input_path: ./ranker_outputs/ranker_input_stage_two.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d3c34dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:52:33.325649Z",
     "iopub.status.busy": "2024-12-08T18:52:33.325071Z",
     "iopub.status.idle": "2024-12-08T18:52:35.157224Z",
     "shell.execute_reply": "2024-12-08T18:52:35.156386Z"
    },
    "papermill": {
     "duration": 1.879206,
     "end_time": "2024-12-08T18:52:35.159256",
     "exception": false,
     "start_time": "2024-12-08T18:52:33.280050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  QuestionId_Answer  ...                                        pred_scores\r\n",
      "3               1_A  ...  [0.25325364837189845, 0.15122470123885048, 0.0...\r\n",
      "1               0_C  ...  [0.20301143762042834, 0.16059519720300236, 0.0...\r\n",
      "9               3_A  ...  [0.7207034823409108, 0.06199804830291408, 0.05...\r\n",
      "\r\n",
      "[3 rows x 4 columns]\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "saving retriever+ranker prediction to: ./ranker_outputs/one_two_blended.parquet\r\n",
      "Example:\r\n",
      "                                                                   3\r\n",
      "QuestionId_Answer                                                1_A\r\n",
      "MisconceptionId    [1755, 2142, 1535, 418, 891, 2068, 1421, 143, ...\r\n",
      "score              [0.3360335962885651, 0.23275139394718383, 0.15...\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Shape of ranker input previously: (390, 9)\r\n",
      "shape of ranker input for next stage: (96, 9)\r\n",
      "saving output to: ./ranker_outputs/ranker_input_stage_two.parquet\r\n",
      "shape of input_df output: (96, 9)\r\n",
      "--------------------------------------------------------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "!python blend_one_two.py --config-path one_two_blend.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5fb3c",
   "metadata": {
    "papermill": {
     "duration": 0.045446,
     "end_time": "2024-12-08T18:52:35.251737",
     "exception": false,
     "start_time": "2024-12-08T18:52:35.206291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Optional Sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3ca9bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:52:35.345038Z",
     "iopub.status.busy": "2024-12-08T18:52:35.344312Z",
     "iopub.status.idle": "2024-12-08T18:52:35.348516Z",
     "shell.execute_reply": "2024-12-08T18:52:35.347696Z"
    },
    "papermill": {
     "duration": 0.053167,
     "end_time": "2024-12-08T18:52:35.350162",
     "exception": false,
     "start_time": "2024-12-08T18:52:35.296995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# pred_df = pd.read_parquet(\"./ranker_outputs/one_two_blended.parquet\")\n",
    "# pred_df[\"MisconceptionId\"] = pred_df[\"MisconceptionId\"].apply(lambda x: x[:25])\n",
    "# pred_df[\"MisconceptionId\"] = pred_df[\"MisconceptionId\"].apply(lambda x: \" \".join(x))\n",
    "# sub_df = pred_df[[\"QuestionId_Answer\", \"MisconceptionId\"]].copy()\n",
    "# sub_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "# sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f51814",
   "metadata": {
    "papermill": {
     "duration": 0.044873,
     "end_time": "2024-12-08T18:52:35.440155",
     "exception": false,
     "start_time": "2024-12-08T18:52:35.395282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stage 3: Reranker (Sonnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef736ae1",
   "metadata": {
    "papermill": {
     "duration": 0.045505,
     "end_time": "2024-12-08T18:52:35.531886",
     "exception": false,
     "start_time": "2024-12-08T18:52:35.486381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1 Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b522def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:52:35.623440Z",
     "iopub.status.busy": "2024-12-08T18:52:35.623187Z",
     "iopub.status.idle": "2024-12-08T18:52:35.632770Z",
     "shell.execute_reply": "2024-12-08T18:52:35.631844Z"
    },
    "papermill": {
     "duration": 0.057645,
     "end_time": "2024-12-08T18:52:35.634500",
     "exception": false,
     "start_time": "2024-12-08T18:52:35.576855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_sonnet_v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_sonnet_v1.py\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/kaggle/input/eedi-utils-v08\")\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import os\n",
    "import vllm\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from llm_oracle.ranker_dataset import RankerDataset\n",
    "from llm_oracle.ranker_loader import RankerCollator, RankerCollatorTrain, show_batch\n",
    "from llm_oracle.ranker_model import EediRanker\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def is_nan(x): return x != x\n",
    "    \n",
    "def stable_softmax(x, temp=1.0):\n",
    "    x = np.array(x) / temp\n",
    "    x_max = np.max(x)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "    \n",
    "def eedi_process_df(df):\n",
    "    df = df.copy()\n",
    "    df = df.rename(columns={\"QuestionId\": \"query_id\"})\n",
    "    grouped = df.groupby(\"query_id\")\n",
    "\n",
    "    question_dict = {}\n",
    "    for question_id, group in grouped:\n",
    "        question_data = group.to_dict(orient=\"records\")[0]\n",
    "        del question_data[\"query_id\"]\n",
    "        question_dict[question_id] = question_data\n",
    "\n",
    "    all_questions = list(question_dict.keys())\n",
    "\n",
    "    queries = []\n",
    "    query2content = defaultdict(list)\n",
    "    content2query = defaultdict(list)\n",
    "\n",
    "    # ---\n",
    "    for qid in all_questions:\n",
    "        info = question_dict[qid]\n",
    "\n",
    "        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if info[\"CorrectAnswer\"] == answer_key:\n",
    "                continue\n",
    "            this_example = dict()\n",
    "            this_key = f\"{qid}_{answer_key}\"\n",
    "            this_example[\"query_id\"] = this_key\n",
    "\n",
    "            if is_nan(info[f\"Misconception{answer_key}Id\"]):\n",
    "                continue\n",
    "\n",
    "            mid = str(int(info[f\"Misconception{answer_key}Id\"]))\n",
    "            query2content[this_key].append(mid)\n",
    "            content2query[mid].append(this_key)\n",
    "\n",
    "            # ---\n",
    "            for col in [\"SubjectId\", \"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n",
    "                this_example[col] = info[col]\n",
    "\n",
    "            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n",
    "            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n",
    "            this_example[\"AllOptionText\"] = \"\\n- \".join([info[f\"Answer{x}Text\"] for x in [\"A\", \"B\", \"C\", \"D\"]])\n",
    "            this_example[\"AllOptionText\"] = f\"\\n- {this_example['AllOptionText']}\"\n",
    "            queries.append(this_example)\n",
    "    # --\n",
    "    query_df = pd.DataFrame(queries)\n",
    "    corr_df = pd.Series(query2content).reset_index().rename(columns={\"index\": \"query_id\", 0: \"content_id\"})\n",
    "    corr_df[\"content_id\"] = corr_df[\"content_id\"].apply(lambda x: x[0])\n",
    "\n",
    "    query_df = query_df.reset_index(drop=True)\n",
    "\n",
    "    return query_df, corr_df, content2query\n",
    "\n",
    "def sort_by_scores(pred_ids, scores):\n",
    "    keep_idxs = np.argsort(-np.array(scores)).tolist()\n",
    "    ret_ids = [pred_ids[idx] for idx in keep_idxs]\n",
    "    ret_scores = [scores[idx] for idx in keep_idxs]\n",
    "    return {\"sorted_ids\": ret_ids, \"sorted_scores\": ret_scores}\n",
    "\n",
    "\n",
    "def format_example(row):\n",
    "    example = f\"Question: {row['QuestionText']}\\nAnswer:{row['CorrectAnswerText']}\\nMisconception Answer: {row['InCorrectAnswerText']}\"\n",
    "    return example\n",
    "\n",
    "def add_fs_examples(df, content2query, query2example, rng, n=2):\n",
    "    cache = {}\n",
    "    def _add_examples(row):\n",
    "        cid = row[\"content_id\"]\n",
    "        if cid in cache:\n",
    "            return cache[cid]\n",
    "        else:\n",
    "            qids = content2query[cid]\n",
    "            qids = [qid for qid in qids if qid != row[\"query_id\"]]\n",
    "            if len(qids) == 0:\n",
    "                cache[cid] = \"\"\n",
    "                return \"\"\n",
    "\n",
    "            qids = rng.sample(qids, k=min(n, len(qids)))\n",
    "            examples = [query2example[qid] for qid in qids]\n",
    "            fs = \"\\n--\\n\".join(examples)\n",
    "            cache[cid] = fs\n",
    "            return fs\n",
    "\n",
    "    df[\"examples\"] = df.apply(_add_examples, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def main(cfg, save_dir, model_id):\n",
    "    test_df = pd.read_parquet(cfg.input_path)\n",
    "    test_df = test_df.rename(columns={\"QuestionId_Answer\": \"query_id\", \"MisconceptionId\": \"content_id\"})\n",
    "    test_df[\"content_id\"] = test_df[\"content_id\"].astype(str)\n",
    "\n",
    "    # comp data examples ---\n",
    "    rng = random.Random(cfg.seed)\n",
    "    comp_df = pd.read_csv(cfg.icl_path).rename(columns={\"QuestionId\": \"query_id\"})\n",
    "\n",
    "\n",
    "    query_df, _, content2query = eedi_process_df(comp_df)\n",
    "    query_df[\"demo\"] = query_df.apply(format_example, axis=1)\n",
    "    query2example = dict(zip(query_df[\"query_id\"], query_df[\"demo\"]))\n",
    "\n",
    "    # add few shot examples --\n",
    "    test_df = add_fs_examples(test_df, content2query, query2example, rng, n=cfg.k_shot)\n",
    "    test_df = test_df.sort_values(by='content_id').reset_index(drop=True)\n",
    "\n",
    "    # cot --\n",
    "    if cfg.use_cot:\n",
    "        print(\"Loading CoT....\")\n",
    "        cot_df = pd.read_parquet(cfg.cot_path)\n",
    "        test_df = test_df.merge(cot_df, on='query_id', how='left')\n",
    "        num_missing = test_df[\"cot\"].isna().sum()\n",
    "        print(f\"# of missing cot: {num_missing}\")\n",
    "        test_df[\"cot\"] = test_df[\"cot\"].fillna(\"\")\n",
    "        print(test_df.sample().T.to_dict())\n",
    "        print(\"--\"*40)\n",
    "\n",
    "    #---\n",
    "    dataset_creator = RankerDataset(cfg)\n",
    "    infer_ds = dataset_creator.get_dataset(test_df)\n",
    "    \n",
    "    tokenizer = dataset_creator.tokenizer\n",
    "    infer_ds = infer_ds.map(lambda example: {\"prompt\": tokenizer.decode(example['input_ids'], skip_special_tokens=False)})\n",
    "    \n",
    "    infer_qa_ids = infer_ds[\"query_id\"]\n",
    "    infer_mc_ids = infer_ds[\"content_id\"]\n",
    "    \n",
    "    prompts = infer_ds['prompt']\n",
    "\n",
    "    print(f\"# of requests: {len(prompts)}\")\n",
    "    print(f\"Example:\\n\\n{prompts[0]}\")\n",
    "    \n",
    "    # -- in\n",
    "    llm = vllm.LLM(\n",
    "        cfg.model.backbone_path,\n",
    "        quantization=\"awq\",\n",
    "        tensor_parallel_size=2,\n",
    "        gpu_memory_utilization=0.99,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2048,\n",
    "        enable_prefix_caching=True\n",
    "    )\n",
    "    \n",
    "    sampling_params = vllm.SamplingParams(n=1, top_p=0.8, logprobs=20, max_tokens=1, temperature=0.0, skip_special_tokens=False)\n",
    "    responses = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "\n",
    "    # Get Results\n",
    "    print(\"--\"*40)\n",
    "    yes_tok_id = tokenizer(\"Yes\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "    no_tok_id = tokenizer(\"No\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "    \n",
    "    print(f\">> EediRanker: Yes token id: {yes_tok_id} | Expected: 9454\")\n",
    "    print(f\">> EediRanker: No token id: {no_tok_id} | Expected: 2753\")\n",
    "    print(\"--\"*40)\n",
    "    \n",
    "    QuestionId_Answer = []\n",
    "    MisconceptionId =[]\n",
    "    scores = []\n",
    "    \n",
    "    for qid, cid, response in zip(infer_qa_ids, infer_mc_ids, responses):\n",
    "        logprob_dict = response.outputs[0].logprobs[0]\n",
    "\n",
    "        top_tok_ids = set(list(logprob_dict.keys()))\n",
    "        if len(top_tok_ids.intersection(set([yes_tok_id, no_tok_id]))) == 0:\n",
    "            print(f\"Bad Output for {qid} - {cid}\")\n",
    "            continue\n",
    "        \n",
    "        yes_logit, no_logit = -10.0, -10.0\n",
    "        \n",
    "        if yes_tok_id in logprob_dict:\n",
    "            yes_logit = logprob_dict[yes_tok_id].logprob\n",
    "        \n",
    "        if no_tok_id in logprob_dict:\n",
    "            no_logit = logprob_dict[no_tok_id].logprob\n",
    "        \n",
    "        score = yes_logit - no_logit\n",
    "        \n",
    "        QuestionId_Answer.append(qid)\n",
    "        MisconceptionId.append(cid)\n",
    "        scores.append(score)\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"QuestionId_Answer\"] = QuestionId_Answer\n",
    "    result_df[\"MisconceptionId\"] = MisconceptionId\n",
    "    result_df[\"score\"] = scores\n",
    "    \n",
    "    agg_df = result_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n",
    "    score_agg_df = result_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n",
    "    agg_df = pd.merge(agg_df, score_agg_df, on=\"QuestionId_Answer\", how=\"left\")\n",
    "    \n",
    "    agg_df[\"topk_info\"] = agg_df.apply(lambda x: sort_by_scores(x[\"MisconceptionId\"], x[\"score\"]), axis=1)\n",
    "    agg_df[\"MisconceptionId\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_ids\"])\n",
    "    agg_df[\"score\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_scores\"])\n",
    "    \n",
    "    # compute oof dataframe ---\n",
    "    oof_df = agg_df.copy()\n",
    "    oof_df = oof_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n",
    "    oof_df = oof_df.rename(columns={\"score\": \"logit_scores\"})\n",
    "\n",
    "    # normalize ---\n",
    "    oof_df[\"pred_scores\"] = oof_df[\"logit_scores\"].apply(stable_softmax)\n",
    "    oof_df[\"MisconceptionId\"] = oof_df[\"MisconceptionId\"].apply(lambda x: list(map(str, x)))\n",
    "\n",
    "    # print ---\n",
    "    print(\"--\"*40)\n",
    "    row = oof_df.sample()\n",
    "    formatted_scores = [f\"{s:.3f}\" for s in row['pred_scores'].values[0]]\n",
    "    misconceptions = row['MisconceptionId'].values[0]\n",
    "    print(f\"Showing 1 example: {row['QuestionId_Answer'].values[0]}\")\n",
    "    for rank, (m, s) in enumerate(zip(misconceptions, formatted_scores)):\n",
    "        print(f\"MisconceptionId: {m} -> Score: {s}\")\n",
    "    print(\"--\"*40)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"ranker_{model_id}.parquet\")\n",
    "    oof_df.to_parquet(save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('--config_path', type=str, required=True)\n",
    "    ap.add_argument('--save_dir', type=str, required=True)\n",
    "    ap.add_argument('--model_id', type=str, required=True)\n",
    "\n",
    "    args = ap.parse_args()\n",
    "    cfg = OmegaConf.load(args.config_path)\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "    # execution ---\n",
    "    main(cfg, save_dir=args.save_dir, model_id=args.model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "623b6ce8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:52:35.727106Z",
     "iopub.status.busy": "2024-12-08T18:52:35.726870Z",
     "iopub.status.idle": "2024-12-08T18:52:35.734981Z",
     "shell.execute_reply": "2024-12-08T18:52:35.734250Z"
    },
    "papermill": {
     "duration": 0.056557,
     "end_time": "2024-12-08T18:52:35.736826",
     "exception": false,
     "start_time": "2024-12-08T18:52:35.680269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_sonnet_v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_sonnet_v2.py\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/kaggle/input/eedi-utils-v12\")\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import os\n",
    "import vllm\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from llm_oracle.ranker_dataset import RankerDataset\n",
    "from llm_oracle.ranker_loader import RankerCollator, RankerCollatorTrain, show_batch\n",
    "from llm_oracle.ranker_model import EediRanker\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def is_nan(x): return x != x\n",
    "    \n",
    "def stable_softmax(x, temp=1.0):\n",
    "    x = np.array(x) / temp\n",
    "    x_max = np.max(x)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "    \n",
    "def eedi_process_df(df):\n",
    "    df = df.copy()\n",
    "    df = df.rename(columns={\"QuestionId\": \"query_id\"})\n",
    "    grouped = df.groupby(\"query_id\")\n",
    "\n",
    "    question_dict = {}\n",
    "    for question_id, group in grouped:\n",
    "        question_data = group.to_dict(orient=\"records\")[0]\n",
    "        del question_data[\"query_id\"]\n",
    "        question_dict[question_id] = question_data\n",
    "\n",
    "    all_questions = list(question_dict.keys())\n",
    "\n",
    "    queries = []\n",
    "    query2content = defaultdict(list)\n",
    "    content2query = defaultdict(list)\n",
    "\n",
    "    # ---\n",
    "    for qid in all_questions:\n",
    "        info = question_dict[qid]\n",
    "\n",
    "        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if info[\"CorrectAnswer\"] == answer_key:\n",
    "                continue\n",
    "            this_example = dict()\n",
    "            this_key = f\"{qid}_{answer_key}\"\n",
    "            this_example[\"query_id\"] = this_key\n",
    "\n",
    "            if is_nan(info[f\"Misconception{answer_key}Id\"]):\n",
    "                continue\n",
    "\n",
    "            mid = str(int(info[f\"Misconception{answer_key}Id\"]))\n",
    "            query2content[this_key].append(mid)\n",
    "            content2query[mid].append(this_key)\n",
    "\n",
    "            # ---\n",
    "            for col in [\"SubjectId\", \"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n",
    "                this_example[col] = info[col]\n",
    "\n",
    "            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n",
    "            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n",
    "            this_example[\"AllOptionText\"] = \"\\n- \".join([info[f\"Answer{x}Text\"] for x in [\"A\", \"B\", \"C\", \"D\"]])\n",
    "            this_example[\"AllOptionText\"] = f\"\\n- {this_example['AllOptionText']}\"\n",
    "            queries.append(this_example)\n",
    "    # --\n",
    "    query_df = pd.DataFrame(queries)\n",
    "    corr_df = pd.Series(query2content).reset_index().rename(columns={\"index\": \"query_id\", 0: \"content_id\"})\n",
    "    corr_df[\"content_id\"] = corr_df[\"content_id\"].apply(lambda x: x[0])\n",
    "\n",
    "    query_df = query_df.reset_index(drop=True)\n",
    "\n",
    "    return query_df, corr_df, content2query\n",
    "\n",
    "def sort_by_scores(pred_ids, scores):\n",
    "    keep_idxs = np.argsort(-np.array(scores)).tolist()\n",
    "    ret_ids = [pred_ids[idx] for idx in keep_idxs]\n",
    "    ret_scores = [scores[idx] for idx in keep_idxs]\n",
    "    return {\"sorted_ids\": ret_ids, \"sorted_scores\": ret_scores}\n",
    "\n",
    "\n",
    "def format_example(row):\n",
    "    example = f\"Question: {row['QuestionText']}\\nAnswer:{row['CorrectAnswerText']}\\nMisconception Answer: {row['InCorrectAnswerText']}\"\n",
    "    return example\n",
    "\n",
    "def add_fs_examples(df, content2query, query2example, rng, n=2):\n",
    "    cache = {}\n",
    "    def _add_examples(row):\n",
    "        cid = row[\"content_id\"]\n",
    "        if cid in cache:\n",
    "            return cache[cid]\n",
    "        else:\n",
    "            qids = content2query[cid]\n",
    "            qids = [qid for qid in qids if qid != row[\"query_id\"]]\n",
    "            if len(qids) == 0:\n",
    "                cache[cid] = \"\"\n",
    "                return \"\"\n",
    "\n",
    "            qids = rng.sample(qids, k=min(n, len(qids)))\n",
    "            examples = [query2example[qid] for qid in qids]\n",
    "            fs = \"\\n--\\n\".join(examples)\n",
    "            cache[cid] = fs\n",
    "            return fs\n",
    "\n",
    "    df[\"examples\"] = df.apply(_add_examples, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def main(cfg, save_dir, model_id):\n",
    "    test_df = pd.read_parquet(cfg.input_path)\n",
    "    test_df = test_df.rename(columns={\"QuestionId_Answer\": \"query_id\", \"MisconceptionId\": \"content_id\"})\n",
    "    test_df[\"content_id\"] = test_df[\"content_id\"].astype(str)\n",
    "\n",
    "    # comp data examples ---\n",
    "    rng = random.Random(cfg.seed)\n",
    "    comp_df = pd.read_csv(cfg.icl_path).rename(columns={\"QuestionId\": \"query_id\"})\n",
    "\n",
    "\n",
    "    query_df, _, content2query = eedi_process_df(comp_df)\n",
    "    query_df[\"demo\"] = query_df.apply(format_example, axis=1)\n",
    "    query2example = dict(zip(query_df[\"query_id\"], query_df[\"demo\"]))\n",
    "\n",
    "    # add few shot examples --\n",
    "    test_df = add_fs_examples(test_df, content2query, query2example, rng, n=cfg.k_shot)\n",
    "    test_df = test_df.sort_values(by='content_id').reset_index(drop=True)\n",
    "\n",
    "    # cot --\n",
    "    if cfg.use_cot:\n",
    "        print(\"Loading CoT....\")\n",
    "        cot_df = pd.read_parquet(cfg.cot_path)\n",
    "        test_df = test_df.merge(cot_df, on='query_id', how='left')\n",
    "        num_missing = test_df[\"cot\"].isna().sum()\n",
    "        print(f\"# of missing cot: {num_missing}\")\n",
    "        test_df[\"cot\"] = test_df[\"cot\"].fillna(\"\")\n",
    "        print(test_df.sample().T.to_dict())\n",
    "        print(\"--\"*40)\n",
    "\n",
    "    #---\n",
    "    dataset_creator = RankerDataset(cfg)\n",
    "    infer_ds = dataset_creator.get_dataset(test_df)\n",
    "    \n",
    "    tokenizer = dataset_creator.tokenizer\n",
    "    infer_ds = infer_ds.map(lambda example: {\"prompt\": tokenizer.decode(example['input_ids'], skip_special_tokens=False)})\n",
    "    \n",
    "    infer_qa_ids = infer_ds[\"query_id\"]\n",
    "    infer_mc_ids = infer_ds[\"content_id\"]\n",
    "    \n",
    "    prompts = infer_ds['prompt']\n",
    "\n",
    "    print(f\"# of requests: {len(prompts)}\")\n",
    "    print(f\"Example:\\n\\n{prompts[0]}\")\n",
    "    \n",
    "    # -- in\n",
    "    llm = vllm.LLM(\n",
    "        cfg.model.backbone_path,\n",
    "        quantization=\"awq\",\n",
    "        tensor_parallel_size=2,\n",
    "        gpu_memory_utilization=0.99,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2048,\n",
    "        enable_prefix_caching=True\n",
    "    )\n",
    "    \n",
    "    sampling_params = vllm.SamplingParams(n=1, top_p=0.8, logprobs=20, max_tokens=1, temperature=0.0, skip_special_tokens=False)\n",
    "    responses = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "\n",
    "    # Get Results\n",
    "    print(\"--\"*40)\n",
    "    yes_tok_id = tokenizer(\"Yes\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "    no_tok_id = tokenizer(\"No\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "    \n",
    "    print(f\">> EediRanker: Yes token id: {yes_tok_id} | Expected: 9454\")\n",
    "    print(f\">> EediRanker: No token id: {no_tok_id} | Expected: 2753\")\n",
    "    print(\"--\"*40)\n",
    "    \n",
    "    QuestionId_Answer = []\n",
    "    MisconceptionId =[]\n",
    "    scores = []\n",
    "    \n",
    "    for qid, cid, response in zip(infer_qa_ids, infer_mc_ids, responses):\n",
    "        logprob_dict = response.outputs[0].logprobs[0]\n",
    "\n",
    "        top_tok_ids = set(list(logprob_dict.keys()))\n",
    "        if len(top_tok_ids.intersection(set([yes_tok_id, no_tok_id]))) == 0:\n",
    "            print(f\"Bad Output for {qid} - {cid}\")\n",
    "            continue\n",
    "        \n",
    "        yes_logit, no_logit = -10.0, -10.0\n",
    "        \n",
    "        if yes_tok_id in logprob_dict:\n",
    "            yes_logit = logprob_dict[yes_tok_id].logprob\n",
    "        \n",
    "        if no_tok_id in logprob_dict:\n",
    "            no_logit = logprob_dict[no_tok_id].logprob\n",
    "        \n",
    "        score = yes_logit - no_logit\n",
    "        \n",
    "        QuestionId_Answer.append(qid)\n",
    "        MisconceptionId.append(cid)\n",
    "        scores.append(score)\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"QuestionId_Answer\"] = QuestionId_Answer\n",
    "    result_df[\"MisconceptionId\"] = MisconceptionId\n",
    "    result_df[\"score\"] = scores\n",
    "    \n",
    "    agg_df = result_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n",
    "    score_agg_df = result_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n",
    "    agg_df = pd.merge(agg_df, score_agg_df, on=\"QuestionId_Answer\", how=\"left\")\n",
    "    \n",
    "    agg_df[\"topk_info\"] = agg_df.apply(lambda x: sort_by_scores(x[\"MisconceptionId\"], x[\"score\"]), axis=1)\n",
    "    agg_df[\"MisconceptionId\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_ids\"])\n",
    "    agg_df[\"score\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_scores\"])\n",
    "    \n",
    "    # compute oof dataframe ---\n",
    "    oof_df = agg_df.copy()\n",
    "    oof_df = oof_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n",
    "    oof_df = oof_df.rename(columns={\"score\": \"logit_scores\"})\n",
    "\n",
    "    # normalize ---\n",
    "    oof_df[\"pred_scores\"] = oof_df[\"logit_scores\"].apply(stable_softmax)\n",
    "    oof_df[\"MisconceptionId\"] = oof_df[\"MisconceptionId\"].apply(lambda x: list(map(str, x)))\n",
    "\n",
    "    # print ---\n",
    "    print(\"--\"*40)\n",
    "    row = oof_df.sample()\n",
    "    formatted_scores = [f\"{s:.3f}\" for s in row['pred_scores'].values[0]]\n",
    "    misconceptions = row['MisconceptionId'].values[0]\n",
    "    print(f\"Showing 1 example: {row['QuestionId_Answer'].values[0]}\")\n",
    "    for rank, (m, s) in enumerate(zip(misconceptions, formatted_scores)):\n",
    "        print(f\"MisconceptionId: {m} -> Score: {s}\")\n",
    "    print(\"--\"*40)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"ranker_{model_id}.parquet\")\n",
    "    oof_df.to_parquet(save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('--config_path', type=str, required=True)\n",
    "    ap.add_argument('--save_dir', type=str, required=True)\n",
    "    ap.add_argument('--model_id', type=str, required=True)\n",
    "\n",
    "    args = ap.parse_args()\n",
    "    cfg = OmegaConf.load(args.config_path)\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "    # execution ---\n",
    "    main(cfg, save_dir=args.save_dir, model_id=args.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e6592",
   "metadata": {
    "papermill": {
     "duration": 0.044028,
     "end_time": "2024-12-08T18:52:35.825104",
     "exception": false,
     "start_time": "2024-12-08T18:52:35.781076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2 Qwen 32b Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a029b90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:52:35.917001Z",
     "iopub.status.busy": "2024-12-08T18:52:35.916467Z",
     "iopub.status.idle": "2024-12-08T18:52:35.920961Z",
     "shell.execute_reply": "2024-12-08T18:52:35.920197Z"
    },
    "papermill": {
     "duration": 0.05142,
     "end_time": "2024-12-08T18:52:35.922576",
     "exception": false,
     "start_time": "2024-12-08T18:52:35.871156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conf_oracle_32b_cv663_ff.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conf_oracle_32b_cv663_ff.yaml\n",
    "\n",
    "seed: 4562\n",
    "k_shot: 0\n",
    "use_cot: true\n",
    "\n",
    "input_path: ./ranker_outputs/ranker_input_stage_two.parquet\n",
    "icl_path: /kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\n",
    "cot_path: ./gen/gen_qwen_14b.parquet\n",
    "\n",
    "model:\n",
    "    backbone_path: \"/kaggle/input/eedi-oracle-32b-cv663-dec7-awq-ff/transformers/default/1\"\n",
    "    max_length: 768\n",
    "    num_proc: 2\n",
    "    \n",
    "    tokenizer:\n",
    "        padding_side: left\n",
    "        truncation_side: left\n",
    "        use_fast: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95edd8dd",
   "metadata": {
    "papermill": {
     "duration": 0.044463,
     "end_time": "2024-12-08T18:52:36.012642",
     "exception": false,
     "start_time": "2024-12-08T18:52:35.968179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.3 Qwen32b Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bb38c42",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-08T18:52:36.103068Z",
     "iopub.status.busy": "2024-12-08T18:52:36.102801Z",
     "iopub.status.idle": "2024-12-08T18:57:03.814257Z",
     "shell.execute_reply": "2024-12-08T18:57:03.813143Z"
    },
    "papermill": {
     "duration": 267.759131,
     "end_time": "2024-12-08T18:57:03.816227",
     "exception": false,
     "start_time": "2024-12-08T18:52:36.057096",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CoT....\r\n",
      "# of missing cot: 0\r\n",
      "{39: {'query_id': '2_A', 'content_id': '1866', 'SubjectName': 'Range and Interquartile Range from a List of Data', 'ConstructName': 'Calculate the range from a list of data', 'QuestionText': \"Tom and Katie are discussing the \\\\( 5 \\\\) plants with these heights:\\n\\\\( 24 \\\\mathrm{~cm}, 17 \\\\mathrm{~cm}, 42 \\\\mathrm{~cm}, 26 \\\\mathrm{~cm}, 13 \\\\mathrm{~cm} \\\\)\\nTom says if all the plants were cut in half, the range wouldn't change.\\nKatie says if all the plants grew by \\\\( 3 \\\\mathrm{~cm} \\\\) each, the range wouldn't change.\\nWho do you agree with?\", 'CorrectAnswerText': 'Only\\nKatie', 'InCorrectAnswerText': 'Only\\nTom', 'AllOptionText': '\\n- Only\\nTom\\n- Only\\nKatie\\n- Both Tom and Katie\\n- Neither is correct', 'MisconceptionName': 'Confuses additive and multiplicative relationships', 'examples': '', 'prompt': \"Analyze the incorrect answer to detect flaws in the student's reasoning.\\n\\nQuery: Question: Tom and Katie are discussing the \\\\( 5 \\\\) plants with these heights:\\n\\\\( 24 \\\\mathrm{~cm}, 17 \\\\mathrm{~cm}, 42 \\\\mathrm{~cm}, 26 \\\\mathrm{~cm}, 13 \\\\mathrm{~cm} \\\\)\\nTom says if all the plants were cut in half, the range wouldn't change.\\nKatie says if all the plants grew by \\\\( 3 \\\\mathrm{~cm} \\\\) each, the range wouldn't change.\\nWho do you agree with?\\nCorrect Answer: Only\\nKatie\\nIncorrect Answer: Only\\nTom\\nAnswer:\\n\", 'cot': 'The student incorrectly concluded that halving the values would maintain the range because they failed to recognize that when each value is divided by 2, the difference between the maximum and minimum values is also halved. This led them to believe that the range would remain unchanged, when in fact it would be reduced to half its original size.'}}\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Using Llama tokenizer\r\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Map (num_proc=2): 100%|██████████████████| 96/96 [00:01<00:00, 85.48 examples/s]\r\n",
      "Map: 100%|██████████████████████████████| 96/96 [00:00<00:00, 458.77 examples/s]\r\n",
      "# of requests: 96\r\n",
      "Example:\r\n",
      "\r\n",
      "<|im_start|>system\r\n",
      "You are an expert in detecting grade-school level math misconceptions. Verify if the incorrect answer stems from the provided misconception.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Misconception: Carries out operations from left to right regardless of priority order, unless brackets are used\r\n",
      "\r\n",
      "Subject: BIDMAS\r\n",
      "Topic: Use the order of operations to carry out calculations involving powers\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal \\( 13 \\) ?\r\n",
      "Correct Answer: \\( 3 \\times(2+4)-5 \\)\r\n",
      "Incorrect Answer: \\( 3 \\times 2+(4-5) \\)\r\n",
      "Thought: The student incorrectly prioritized the addition operation over multiplication, leading them to place brackets around \\(2 + 4\\) instead of \\(2 + 4\\) being grouped with multiplication. This misapplication caused the expression to evaluate as \\(3 \\times 2 + (-1) = 6 - 1 = 5\\), rather than reaching the correct answer of \\(13\\). By not adhering to the correct order of operations, the student failed to properly manage the precedence between multiplication and addition, resulting in an inaccurate solution.\r\n",
      "\r\n",
      "Does the misconception (Carries out operations from left to right regardless of priority order, unless brackets are used) lead to the incorrect answer? (Yes/No)<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "WARNING 12-08 18:52:58 config.py:321] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-08 18:52:58 config.py:905] Defaulting to use mp for distributed inference\r\n",
      "WARNING 12-08 18:52:58 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 12-08 18:52:58 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/eedi-oracle-32b-cv663-dec7-awq-ff/transformers/default/1', speculative_config=None, tokenizer='/kaggle/input/eedi-oracle-32b-cv663-dec7-awq-ff/transformers/default/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/eedi-oracle-32b-cv663-dec7-awq-ff/transformers/default/1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\r\n",
      "WARNING 12-08 18:52:59 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 12-08 18:52:59 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "INFO 12-08 18:52:59 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m INFO 12-08 18:52:59 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m INFO 12-08 18:52:59 selector.py:115] Using XFormers backend.\r\n",
      "INFO 12-08 18:52:59 selector.py:115] Using XFormers backend.\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m INFO 12-08 18:53:00 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m INFO 12-08 18:53:01 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-08 18:53:01 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m INFO 12-08 18:53:01 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-08 18:53:01 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m INFO 12-08 18:53:01 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-08 18:53:01 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "WARNING 12-08 18:53:01 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m WARNING 12-08 18:53:01 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 12-08 18:53:01 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7dcbc8a71f30>, local_subscribe_port=57737, remote_subscribe_port=None)\r\n",
      "INFO 12-08 18:53:01 model_runner.py:1056] Starting to load model /kaggle/input/eedi-oracle-32b-cv663-dec7-awq-ff/transformers/default/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m INFO 12-08 18:53:01 model_runner.py:1056] Starting to load model /kaggle/input/eedi-oracle-32b-cv663-dec7-awq-ff/transformers/default/1...\r\n",
      "INFO 12-08 18:53:01 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-08 18:53:01 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m INFO 12-08 18:53:01 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m INFO 12-08 18:53:01 selector.py:115] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:43<02:11, 43.92s/it]\r\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [01:20<01:18, 39.39s/it]\r\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [02:04<00:41, 41.87s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:47<00:00, 42.08s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:47<00:00, 41.84s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m INFO 12-08 18:55:49 model_runner.py:1067] Loading model weights took 9.4096 GB\r\n",
      "INFO 12-08 18:55:50 model_runner.py:1067] Loading model weights took 9.4096 GB\r\n",
      "INFO 12-08 18:55:54 distributed_gpu_executor.py:57] # GPU blocks: 1379, # CPU blocks: 2048\r\n",
      "INFO 12-08 18:55:54 distributed_gpu_executor.py:61] Maximum concurrency for 2048 tokens per request: 10.77x\r\n",
      "Processed prompts: 100%|█| 96/96 [00:58<00:00,  1.65it/s, est. speed input: 469.\r\n",
      "--------------------------------------------------------------------------------\r\n",
      ">> EediRanker: Yes token id: 9454 | Expected: 9454\r\n",
      ">> EediRanker: No token id: 2753 | Expected: 2753\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Showing 1 example: 2_A\r\n",
      "MisconceptionId: 1287 -> Score: 0.970\r\n",
      "MisconceptionId: 1059 -> Score: 0.016\r\n",
      "MisconceptionId: 1073 -> Score: 0.005\r\n",
      "MisconceptionId: 557 -> Score: 0.003\r\n",
      "MisconceptionId: 397 -> Score: 0.003\r\n",
      "MisconceptionId: 1677 -> Score: 0.002\r\n",
      "MisconceptionId: 1866 -> Score: 0.000\r\n",
      "MisconceptionId: 2439 -> Score: 0.000\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "INFO 12-08 18:56:57 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=706)\u001b[0;0m INFO 12-08 18:56:57 multiproc_worker_utils.py:240] Worker exiting\r\n",
      "CPU times: user 3.26 s, sys: 799 ms, total: 4.06 s\n",
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python run_sonnet_v2.py --config_path \"./conf_oracle_32b_cv663_ff.yaml\" --save_dir \"./ranker_outputs\" --model_id \"qwen_32b_oracle_main\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f774a10b",
   "metadata": {
    "papermill": {
     "duration": 0.048177,
     "end_time": "2024-12-08T18:57:03.911917",
     "exception": false,
     "start_time": "2024-12-08T18:57:03.863740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.4 Prep for stage 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "912e41d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:57:04.008562Z",
     "iopub.status.busy": "2024-12-08T18:57:04.007731Z",
     "iopub.status.idle": "2024-12-08T18:57:04.015638Z",
     "shell.execute_reply": "2024-12-08T18:57:04.014880Z"
    },
    "papermill": {
     "duration": 0.058217,
     "end_time": "2024-12-08T18:57:04.017330",
     "exception": false,
     "start_time": "2024-12-08T18:57:03.959113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing blend_two_three.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile blend_two_three.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def get_sorted_pairs(content_ids, scores):\n",
    "    collection = [(cid, s) for cid, s in zip(content_ids, scores)]\n",
    "    sorted_collection = sorted(collection, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_collection\n",
    "\n",
    "def cut_at_n(sub_df, n=25):\n",
    "    sub_df[\"MisconceptionId\"] = sub_df[\"MisconceptionId\"].apply(lambda x: x[:n])\n",
    "    sub_df[\"score\"] = sub_df[\"score\"].apply(lambda x: x[:n])\n",
    "    return sub_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config-path\", type=str)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    with open(args.config_path, \"r\") as f:\n",
    "        cfg = OmegaConf.load(f)\n",
    "\n",
    "    # read predictions ---\n",
    "    haiku_preds = pd.read_parquet(cfg.haiku_path)\n",
    "    sonnet_preds = pd.read_parquet(cfg.sonnet_path)\n",
    "    \n",
    "    print(\"Sample Haiku Preds:\")\n",
    "    print(haiku_preds.sample(3))\n",
    "    print(\"--\"*50)\n",
    "    print(\"Sample Sonnet Preds:\")\n",
    "    print(sonnet_preds.sample(3))\n",
    "    print(\"--\"*50)\n",
    "\n",
    "    # flatten ---\n",
    "    haiku_preds = haiku_preds[[\"QuestionId_Answer\", \"MisconceptionId\", \"pred_scores\"]].explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n",
    "    haiku_preds = haiku_preds.rename(columns={\"pred_scores\": \"score_haiku\"})\n",
    "    haiku_preds[\"MisconceptionId\"] = haiku_preds[\"MisconceptionId\"].astype(str)\n",
    "\n",
    "    sonnet_preds = sonnet_preds[[\"QuestionId_Answer\", \"MisconceptionId\", \"pred_scores\"]].explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n",
    "    sonnet_preds = sonnet_preds.rename(columns={\"pred_scores\": \"score_sonnet\"})\n",
    "    sonnet_preds[\"MisconceptionId\"] = sonnet_preds[\"MisconceptionId\"].astype(str)\n",
    "\n",
    "    # blend ---\n",
    "    w_haiku = cfg.haiku_weight\n",
    "    w_sonnet = cfg.sonnet_weight\n",
    "\n",
    "    candidate_df = pd.merge(haiku_preds, sonnet_preds, on=[\"QuestionId_Answer\", \"MisconceptionId\"])\n",
    "    candidate_df[\"score\"] = candidate_df.apply(lambda x: w_haiku*x['score_haiku'] + w_sonnet*x['score_sonnet'], axis=1) # blending\n",
    "    candidate_df = candidate_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n",
    "\n",
    "    cdf = candidate_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n",
    "    sdf = candidate_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n",
    "    candidate_df = pd.merge(cdf, sdf, on=\"QuestionId_Answer\", how=\"left\")\n",
    "\n",
    "    candidate_df[\"sorted\"] = candidate_df.apply(lambda x: get_sorted_pairs(x['MisconceptionId'], x['score']), axis=1)\n",
    "    candidate_df[\"MisconceptionId\"] = candidate_df[\"sorted\"].apply(lambda x: [y[0] for y in x])\n",
    "    candidate_df[\"score\"] = candidate_df[\"sorted\"].apply(lambda x: [y[1] for y in x])\n",
    "    candidate_df = candidate_df.drop(columns=['sorted'])\n",
    "    \n",
    "    print(\"--\"*40)\n",
    "    print(f\"Saving sonnet+haiku prediction to: {cfg.blended_pred_path}\")\n",
    "    candidate_df.to_parquet(cfg.blended_pred_path)\n",
    "    \n",
    "    print(\"Example:\")\n",
    "    print(candidate_df.sample().T)\n",
    "    print(\"--\"*40)\n",
    "    print(\"Distribution:\")\n",
    "    print(candidate_df['MisconceptionId'].apply(len).value_counts())\n",
    "    print(\"--\"*40)\n",
    "\n",
    "    # Cut at N ---\n",
    "    candidate_df = cut_at_n(candidate_df, n=cfg.cutoff_n)\n",
    "    candidate_df.to_parquet(cfg.tutor_base_path)\n",
    "    \n",
    "    input_df = pd.read_parquet(cfg.ranker_input_file)\n",
    "    \n",
    "    print(f\"Shape of ranker input previously: {input_df.shape}\")\n",
    "    keep_df = candidate_df[['QuestionId_Answer', 'MisconceptionId']].explode(\"MisconceptionId\").reset_index(drop=True)\n",
    "    keep_df['MisconceptionId'] = keep_df['MisconceptionId'].astype(input_df[\"MisconceptionId\"].dtype)\n",
    "    input_df = input_df.merge(keep_df, on=[\"QuestionId_Answer\", \"MisconceptionId\"], how=\"inner\")\n",
    "    print(f\"shape of ranker input for stage 4: {input_df.shape}\")\n",
    "\n",
    "    # # Prepare further ranking ---\n",
    "    save_path = cfg.stage4_input_path\n",
    "    print(f\"saving output to: {save_path}\")\n",
    "    input_df.to_parquet(save_path)\n",
    "    print(f\"shape of input_df output: {input_df.shape}\")\n",
    "    print(\"--\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39f85ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:57:04.112951Z",
     "iopub.status.busy": "2024-12-08T18:57:04.112696Z",
     "iopub.status.idle": "2024-12-08T18:57:04.117341Z",
     "shell.execute_reply": "2024-12-08T18:57:04.116577Z"
    },
    "papermill": {
     "duration": 0.054603,
     "end_time": "2024-12-08T18:57:04.119094",
     "exception": false,
     "start_time": "2024-12-08T18:57:04.064491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing two_three_blend.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile two_three_blend.yaml\n",
    "\n",
    "ranker_input_file: ./retriever_outputs/ranker_input_stage_one.parquet\n",
    "\n",
    "haiku_path: ./ranker_outputs/ranker_qwen_14b.parquet\n",
    "sonnet_path: ./ranker_outputs/ranker_qwen_32b_oracle_main.parquet\n",
    "\n",
    "haiku_weight: 1.0\n",
    "sonnet_weight: 4.0\n",
    "\n",
    "cutoff_n: 5\n",
    "\n",
    "blended_pred_path: ./ranker_outputs/two_three_blended.parquet\n",
    "tutor_base_path: ./ranker_outputs/tutor_base.parquet\n",
    "stage4_input_path: ./ranker_outputs/ranker_input_stage_four.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "969b05f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:57:04.217734Z",
     "iopub.status.busy": "2024-12-08T18:57:04.217467Z",
     "iopub.status.idle": "2024-12-08T18:57:06.476732Z",
     "shell.execute_reply": "2024-12-08T18:57:06.475590Z"
    },
    "papermill": {
     "duration": 2.311863,
     "end_time": "2024-12-08T18:57:06.478889",
     "exception": false,
     "start_time": "2024-12-08T18:57:04.167026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Haiku Preds:\r\n",
      "   QuestionId_Answer  ...                                        pred_scores\r\n",
      "10               3_B  ...  [0.6888656626211809, 0.10899436650846599, 0.06...\r\n",
      "11               3_D  ...  [0.8639744070892488, 0.041691330620762815, 0.0...\r\n",
      "6                2_A  ...  [0.9274250497695343, 0.02303694863201152, 0.01...\r\n",
      "\r\n",
      "[3 rows x 4 columns]\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "Sample Sonnet Preds:\r\n",
      "   QuestionId_Answer  ...                                        pred_scores\r\n",
      "4                1_B  ...  [0.39836614192932135, 0.1795576931842323, 0.13...\r\n",
      "10               3_B  ...  [0.6104413841376953, 0.29750540792589314, 0.03...\r\n",
      "5                1_C  ...  [0.21584247544606916, 0.1437821073620254, 0.13...\r\n",
      "\r\n",
      "[3 rows x 4 columns]\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Saving sonnet+haiku prediction to: ./ranker_outputs/two_three_blended.parquet\r\n",
      "Example:\r\n",
      "                                                                   5\r\n",
      "QuestionId_Answer                                                1_C\r\n",
      "MisconceptionId        [1755, 1421, 891, 143, 2142, 2068, 1535, 418]\r\n",
      "score              [1.1322907840463665, 0.6115229152848767, 0.596...\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Distribution:\r\n",
      "MisconceptionId\r\n",
      "8    12\r\n",
      "Name: count, dtype: int64\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Shape of ranker input previously: (390, 9)\r\n",
      "shape of ranker input for stage 4: (60, 9)\r\n",
      "saving output to: ./ranker_outputs/ranker_input_stage_four.parquet\r\n",
      "shape of input_df output: (60, 9)\r\n",
      "--------------------------------------------------------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "!python blend_two_three.py --config-path two_three_blend.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd38539",
   "metadata": {
    "papermill": {
     "duration": 0.047976,
     "end_time": "2024-12-08T18:57:06.576229",
     "exception": false,
     "start_time": "2024-12-08T18:57:06.528253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.5 Tutor Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55a7202a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:57:06.676313Z",
     "iopub.status.busy": "2024-12-08T18:57:06.675813Z",
     "iopub.status.idle": "2024-12-08T18:57:06.683291Z",
     "shell.execute_reply": "2024-12-08T18:57:06.682447Z"
    },
    "papermill": {
     "duration": 0.060603,
     "end_time": "2024-12-08T18:57:06.685094",
     "exception": false,
     "start_time": "2024-12-08T18:57:06.624491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prep_tutor_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prep_tutor_data.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def eedi_process_df(df):\n",
    "    df = deepcopy(df)\n",
    "    df = df.rename(columns={\"QuestionId\": \"query_id\"})\n",
    "    grouped = df.groupby(\"query_id\")\n",
    "\n",
    "    question_dict = {}\n",
    "    for question_id, group in grouped:\n",
    "        question_data = group.to_dict(orient=\"records\")[0]\n",
    "        del question_data[\"query_id\"]\n",
    "        question_dict[question_id] = question_data\n",
    "\n",
    "    all_questions = list(question_dict.keys())\n",
    "\n",
    "    queries = []\n",
    "    for qid in all_questions:\n",
    "        info = question_dict[qid]\n",
    "\n",
    "        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if info[\"CorrectAnswer\"] == answer_key:\n",
    "                continue\n",
    "\n",
    "            this_example = dict()\n",
    "            this_key = f\"{qid}_{answer_key}\"\n",
    "            this_example[\"query_id\"] = this_key\n",
    "\n",
    "            for col in [\"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n",
    "                this_example[col] = info[col]\n",
    "\n",
    "            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n",
    "            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n",
    "            queries.append(this_example)\n",
    "    # --\n",
    "    query_df = pd.DataFrame(queries)\n",
    "    return query_df\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config-path\", type=str)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    with open(args.config_path, \"r\") as f:\n",
    "        cfg = OmegaConf.load(f)\n",
    "\n",
    "\n",
    "    # read data ---\n",
    "    test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "    if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        n_ex = int(os.getenv(\"N_EX\"))\n",
    "        test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").head(n_ex)\n",
    "    content_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "    id2name = dict(zip(content_df['MisconceptionId'], content_df['MisconceptionName']))\n",
    "\n",
    "    query_df = eedi_process_df(test_df)\n",
    "\n",
    "    # read predictions ---\n",
    "    oof_df = pd.read_parquet(cfg.tutor_base_path)\n",
    "    oof_df[\"MisconceptionId\"] = oof_df[\"MisconceptionId\"].apply(lambda x: list(map(int, x)))\n",
    "    oof_df = oof_df.rename(columns={'QuestionId_Answer': 'query_id', 'MisconceptionId': 'content_ids'})\n",
    "    oof_df = oof_df[['query_id', 'content_ids']].copy()\n",
    "\n",
    "    # cots\n",
    "    cot_df_7b = pd.read_parquet(cfg.cot_7b_path)\n",
    "    cot_df_7b = cot_df_7b[['query_id', 'cot']].rename(columns={'cot': 'cot_7b'})\n",
    "\n",
    "    cot_df_14b = pd.read_parquet(cfg.cot_14b_path)\n",
    "    cot_df_14b = cot_df_14b[['query_id', 'cot']].rename(columns={'cot': 'cot_14b'})\n",
    "    \n",
    "\n",
    "    cot_df_32b = pd.read_parquet(cfg.cot_32b_path)\n",
    "    cot_df_32b = cot_df_32b[['query_id', 'cot']].rename(columns={'cot': 'cot_32b'})\n",
    "\n",
    "    # prep\n",
    "    query_df = query_df.merge(oof_df, on=\"query_id\", how=\"left\")\n",
    "    query_df['MisconceptionNameList'] = query_df['content_ids'].apply(lambda x: [id2name[y] for y in x])\n",
    "\n",
    "    query_df = query_df.merge(cot_df_7b, on='query_id', how='left')\n",
    "    query_df = query_df.merge(cot_df_14b, on='query_id', how='left')\n",
    "    query_df = query_df.merge(cot_df_32b, on='query_id', how='left')\n",
    "\n",
    "    print(query_df[\"cot_7b\"].isna().sum())\n",
    "    print(query_df[\"cot_14b\"].isna().sum())\n",
    "    print(query_df[\"cot_32b\"].isna().sum())\n",
    "\n",
    "    query_df[\"cot_7b\"] = query_df[\"cot_7b\"].fillna(\"\")\n",
    "    query_df[\"cot_14b\"] = query_df[\"cot_14b\"].fillna(\"\")\n",
    "    query_df[\"cot_32b\"] = query_df[\"cot_32b\"].fillna(\"\")\n",
    "\n",
    "    final_df = query_df[\n",
    "        [\"query_id\", \"content_ids\", \"SubjectName\", \"ConstructName\", \"QuestionText\", \n",
    "         \"CorrectAnswerText\", \"InCorrectAnswerText\", \"MisconceptionNameList\", \"cot_7b\", \"cot_14b\", \"cot_32b\"]\n",
    "    ].copy()\n",
    "    \n",
    "    print(\"--\"*50)\n",
    "    final_df = final_df.sort_values(by=\"query_id\").reset_index(drop=True)\n",
    "    ex = final_df.sample().to_dict(orient='records')[0]\n",
    "    for k, v in ex.items():\n",
    "        print(f\"{k} -> {v}\")\n",
    "\n",
    "    print(f\"saving tutor data to: {cfg.tutor_input_path}\")\n",
    "    final_df.to_parquet(cfg.tutor_input_path)\n",
    "    print(\"--\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7ecd289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:57:06.783299Z",
     "iopub.status.busy": "2024-12-08T18:57:06.783029Z",
     "iopub.status.idle": "2024-12-08T18:57:06.788147Z",
     "shell.execute_reply": "2024-12-08T18:57:06.787293Z"
    },
    "papermill": {
     "duration": 0.055715,
     "end_time": "2024-12-08T18:57:06.789821",
     "exception": false,
     "start_time": "2024-12-08T18:57:06.734106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tutor_data_prep.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile tutor_data_prep.yaml\n",
    "\n",
    "tutor_base_path: ./ranker_outputs/tutor_base.parquet\n",
    "\n",
    "cot_7b_path: ./gen/gen_qwen_7b.parquet\n",
    "cot_14b_path: ./gen/gen_qwen_14b.parquet\n",
    "cot_32b_path: ./gen/gen_qwen_32b.parquet\n",
    "\n",
    "tutor_input_path: ./ranker_outputs/tutor_input.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96c38728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:57:06.886982Z",
     "iopub.status.busy": "2024-12-08T18:57:06.886712Z",
     "iopub.status.idle": "2024-12-08T18:57:08.764480Z",
     "shell.execute_reply": "2024-12-08T18:57:08.763639Z"
    },
    "papermill": {
     "duration": 1.92848,
     "end_time": "2024-12-08T18:57:08.766507",
     "exception": false,
     "start_time": "2024-12-08T18:57:06.838027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "0\r\n",
      "0\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "query_id -> 0_D\r\n",
      "content_ids -> [1507, 2532, 1672, 1005, 328]\r\n",
      "SubjectName -> BIDMAS\r\n",
      "ConstructName -> Use the order of operations to carry out calculations involving powers\r\n",
      "QuestionText -> \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal \\( 13 \\) ?\r\n",
      "CorrectAnswerText -> \\( 3 \\times(2+4)-5 \\)\r\n",
      "InCorrectAnswerText -> Does not need brackets\r\n",
      "MisconceptionNameList -> ['Carries out operations from left to right regardless of priority order', 'Believes order of operations does not affect the answer to a calculation', 'Confuses the order of operations, believes addition comes before multiplication ', 'Carries out operations from left to right regardless of priority order, unless brackets are used', 'Performs addition ahead of multiplication']\r\n",
      "cot_7b -> The student likely calculated 3 × 2 + 4 - 5 = 6 + 4 - 5 = 5, which equals the desired answer of 13, but failed to recognize that the original expression 3 × 2 + 4 - 5 without brackets would give the wrong answer. This reveals they don't understand that the expression 3 × 2 + 4 - 5 actually evaluates to 5, and therefore don't grasp that brackets are needed to change the order of operations to get 13.\r\n",
      "cot_14b -> The student likely calculated 3 × 2 + 4 - 5 = 6 + 4 - 5 = 5, which equals the desired answer of 13, but failed to recognize that the original expression 3 × 2 + 4 - 5 without brackets would give the wrong answer. This reveals they don't understand that the expression 3 × 2 + 4 - 5 actually evaluates to 5, and therefore don't grasp that brackets are needed to change the order of operations to get 13.\r\n",
      "cot_32b -> The student likely calculated 3 × 2 + 4 - 5 = 6 + 4 - 5 = 5, which equals the desired answer of 13, but failed to recognize that the original expression 3 × 2 + 4 - 5 without brackets would give the wrong answer. This reveals they don't understand that the expression 3 × 2 + 4 - 5 actually evaluates to 5, and therefore don't grasp that brackets are needed to change the order of operations to get 13.\r\n",
      "saving tutor data to: ./ranker_outputs/tutor_input.parquet\r\n",
      "----------------------------------------------------------------------------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "!python prep_tutor_data.py --config-path tutor_data_prep.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0738cd2",
   "metadata": {
    "papermill": {
     "duration": 0.04929,
     "end_time": "2024-12-08T18:57:08.865392",
     "exception": false,
     "start_time": "2024-12-08T18:57:08.816102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stage 4: Ranker (Opus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ea18d26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:57:08.962918Z",
     "iopub.status.busy": "2024-12-08T18:57:08.962543Z",
     "iopub.status.idle": "2024-12-08T18:57:08.973471Z",
     "shell.execute_reply": "2024-12-08T18:57:08.972670Z"
    },
    "papermill": {
     "duration": 0.06199,
     "end_time": "2024-12-08T18:57:08.975333",
     "exception": false,
     "start_time": "2024-12-08T18:57:08.913343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_expert_tutor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_expert_tutor.py\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/kaggle/input/eedi-utils-v12\")\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vllm\n",
    "from llm_tutor.ranker_dataset import RankerDataset\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "def is_nan(x):\n",
    "    return x != x\n",
    "\n",
    "\n",
    "def eedi_process_df(df):\n",
    "    df = df.copy()\n",
    "    df = df.rename(columns={\"QuestionId\": \"query_id\"})\n",
    "    grouped = df.groupby(\"query_id\")\n",
    "\n",
    "    question_dict = {}\n",
    "    for question_id, group in grouped:\n",
    "        question_data = group.to_dict(orient=\"records\")[0]\n",
    "        del question_data[\"query_id\"]\n",
    "        question_dict[question_id] = question_data\n",
    "\n",
    "    all_questions = list(question_dict.keys())\n",
    "\n",
    "    queries = []\n",
    "    query2content = defaultdict(list)\n",
    "    content2query = defaultdict(list)\n",
    "\n",
    "    # ---\n",
    "    for qid in all_questions:\n",
    "        info = question_dict[qid]\n",
    "\n",
    "        for answer_key in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if info[\"CorrectAnswer\"] == answer_key:\n",
    "                continue\n",
    "            this_example = dict()\n",
    "            this_key = f\"{qid}_{answer_key}\"\n",
    "            this_example[\"query_id\"] = this_key\n",
    "\n",
    "            if is_nan(info[f\"Misconception{answer_key}Id\"]):\n",
    "                continue\n",
    "\n",
    "            mid = str(int(info[f\"Misconception{answer_key}Id\"]))\n",
    "            query2content[this_key].append(mid)\n",
    "            content2query[mid].append(this_key)\n",
    "\n",
    "            # ---\n",
    "            for col in [\"SubjectId\", \"SubjectName\", \"ConstructName\", \"QuestionText\"]:\n",
    "                this_example[col] = info[col]\n",
    "\n",
    "            this_example[\"CorrectAnswerText\"] = info[f\"Answer{info['CorrectAnswer']}Text\"]\n",
    "            this_example[\"InCorrectAnswerText\"] = info[f\"Answer{answer_key}Text\"]\n",
    "            this_example[\"AllOptionText\"] = \"\\n- \".join([info[f\"Answer{x}Text\"] for x in [\"A\", \"B\", \"C\", \"D\"]])\n",
    "            this_example[\"AllOptionText\"] = f\"\\n- {this_example['AllOptionText']}\"\n",
    "            queries.append(this_example)\n",
    "    # --\n",
    "    query_df = pd.DataFrame(queries)\n",
    "    corr_df = pd.Series(query2content).reset_index().rename(columns={\"index\": \"query_id\", 0: \"content_id\"})\n",
    "    corr_df[\"content_id\"] = corr_df[\"content_id\"].apply(lambda x: x[0])\n",
    "\n",
    "    query_df = query_df.reset_index(drop=True)\n",
    "\n",
    "    return query_df, corr_df, content2query\n",
    "\n",
    "\n",
    "def stable_softmax(x, temp=1.0):\n",
    "    x = np.array(x) / temp\n",
    "    x_max = np.max(x)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "\n",
    "def sort_by_scores(pred_ids, scores):\n",
    "    keep_idxs = np.argsort(-np.array(scores)).tolist()\n",
    "    ret_ids = [pred_ids[idx] for idx in keep_idxs]\n",
    "    ret_scores = [scores[idx] for idx in keep_idxs]\n",
    "    return {\"sorted_ids\": ret_ids, \"sorted_scores\": ret_scores}\n",
    "\n",
    "\n",
    "def format_example(row, id2name, query2content):\n",
    "    cid = int(query2content[row[\"query_id\"]])\n",
    "    misconception_name = id2name[cid]\n",
    "    example = f\"Question: {row['QuestionText']}\\nAnswer:{row['CorrectAnswerText']}\\nIncorrect Answer: {row['InCorrectAnswerText']}\\nMisconception: {misconception_name}\"\n",
    "    return example\n",
    "\n",
    "\n",
    "def add_fs_examples_tutor_for_eval(df, content2query, query2example, rng):\n",
    "    cache = {}\n",
    "\n",
    "    def _add_examples(row):\n",
    "        cids = row[\"content_ids\"]\n",
    "\n",
    "        selected_qids = []\n",
    "        for cid in cids:\n",
    "            cid = int(cid)\n",
    "            qids = content2query[cid]  # content2query is a defaultdict(list)\n",
    "            qids = [qid for qid in qids if qid != row[\"query_id\"]]\n",
    "            if len(qids) > 0:\n",
    "                if cid not in cache:\n",
    "                    selected = rng.choice(qids)\n",
    "                    cache[cid] = selected\n",
    "                    selected_qids.append(cache[cid])\n",
    "                else:\n",
    "                    selected_qids.append(cache[cid])\n",
    "\n",
    "        # print(f\"selected_qids: {selected_qids}\")\n",
    "        if len(selected_qids) == 0:\n",
    "            return \"\"\n",
    "\n",
    "        # keep max of 3 examples\n",
    "        selected_qids = rng.sample(selected_qids, k=min(3, len(selected_qids)))\n",
    "        selected_qids = sorted(selected_qids)  # for prefix cache\n",
    "        examples = [query2example[qid] for qid in selected_qids]\n",
    "        fs = \"\\n--\\n\".join(examples)\n",
    "        # print(f\"fs: {fs}\")\n",
    "        return fs\n",
    "\n",
    "    df[\"examples\"] = df.apply(_add_examples, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def main(cfg, save_dir, model_id):\n",
    "    test_df = pd.read_parquet(cfg.input_path)\n",
    "\n",
    "    if cfg.use_tta:\n",
    "        test_df_tta = test_df.copy()\n",
    "        test_df_tta[\"content_ids\"] = test_df_tta[\"content_ids\"].apply(lambda x: x[::-1])\n",
    "        test_df_tta[\"MisconceptionNameList\"] = test_df_tta[\"MisconceptionNameList\"].apply(lambda x: x[::-1])\n",
    "        test_df = pd.concat([test_df, test_df_tta]).reset_index(drop=True)\n",
    "        test_df = test_df.sort_values(by=\"query_id\").reset_index(drop=True)\n",
    "\n",
    "    # comp few shot examples ---\n",
    "    rng = random.Random(cfg.seed)\n",
    "\n",
    "    fs_df = pd.read_csv(cfg.icl_path).rename(columns={\"QuestionId\": \"query_id\"})\n",
    "    content_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "    id2name = dict(zip(content_df[\"MisconceptionId\"], content_df[\"MisconceptionName\"]))\n",
    "\n",
    "    query_df, fs_corr_df, content2query = eedi_process_df(fs_df)\n",
    "    fs_query2content = dict(zip(fs_corr_df[\"query_id\"], fs_corr_df[\"content_id\"]))\n",
    "    query_df[\"demo\"] = query_df.apply(lambda x: format_example(x, id2name, fs_query2content), axis=1)\n",
    "    query2example = dict(zip(query_df[\"query_id\"], query_df[\"demo\"]))\n",
    "\n",
    "    test_df = add_fs_examples_tutor_for_eval(test_df, content2query, query2example, rng)\n",
    "    print(f\"shape of test data: {test_df.shape}\")\n",
    "    print(\"--\" * 40)\n",
    "\n",
    "    # ---\n",
    "    dataset_creator = RankerDataset(cfg)\n",
    "    infer_ds = dataset_creator.get_dataset(test_df)\n",
    "    tokenizer = dataset_creator.tokenizer\n",
    "\n",
    "    a_tok_id = tokenizer(\"A\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "    b_tok_id = tokenizer(\"B\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "    c_tok_id = tokenizer(\"C\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "    d_tok_id = tokenizer(\"D\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "    e_tok_id = tokenizer(\"E\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "\n",
    "    print(f\">> EediRanker: A token id: {a_tok_id}\")\n",
    "    print(f\">> EediRanker: B token id: {b_tok_id}\")\n",
    "    print(f\">> EediRanker: C token id: {c_tok_id}\")\n",
    "    print(f\">> EediRanker: D token id: {d_tok_id}\")\n",
    "    print(f\">> EediRanker: E token id: {e_tok_id}\")\n",
    "\n",
    "    infer_ds = infer_ds.map(lambda example: {\"prompt\": tokenizer.decode(example[\"input_ids\"], skip_special_tokens=False)})\n",
    "\n",
    "    infer_qa_ids = infer_ds[\"query_id\"]\n",
    "    infer_mc_ids = infer_ds[\"content_ids\"]\n",
    "    prompts = infer_ds[\"prompt\"]\n",
    "\n",
    "    print(f\"# of requests: {len(prompts)}\")\n",
    "    print(f\"Example:\\n\\n{prompts[0]}\")\n",
    "    print(\"data preparation done...\")\n",
    "\n",
    "    # -- create the llm ----#\n",
    "    llm = vllm.LLM(\n",
    "        cfg.model.backbone_path,\n",
    "        # quantization=\"awq\",\n",
    "        tensor_parallel_size=2,\n",
    "        gpu_memory_utilization=0.99,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2048,\n",
    "        disable_log_stats=True,\n",
    "        cpu_offload_gb=8,\n",
    "        swap_space=1,\n",
    "        device=\"cuda\",\n",
    "        max_num_seqs=20,\n",
    "        enable_prefix_caching=True\n",
    "    )\n",
    "\n",
    "    sampling_params = vllm.SamplingParams(n=1, top_p=0.8, logprobs=20, max_tokens=1, temperature=0.0, skip_special_tokens=False)\n",
    "    responses = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "    print(\"inference done...\")\n",
    "\n",
    "    # get results ---\n",
    "    print(\"--\" * 40)\n",
    "\n",
    "    QuestionId_Answer = []\n",
    "    MisconceptionId = []\n",
    "    scores = []\n",
    "\n",
    "    for qid, cids, response in zip(infer_qa_ids, infer_mc_ids, responses):\n",
    "        logprob_dict = response.outputs[0].logprobs[0]\n",
    "\n",
    "        top_tok_ids = set(list(logprob_dict.keys()))\n",
    "        if len(top_tok_ids.intersection(set([a_tok_id, b_tok_id, c_tok_id, d_tok_id, e_tok_id]))) == 0:\n",
    "            print(f\"Bad Output for {qid}\")\n",
    "            continue\n",
    "\n",
    "        a_logit, b_logit, c_logit, d_logit, e_logit = -10.0, -10.0, -10.0, -10.0, -10.0\n",
    "\n",
    "        if a_tok_id in logprob_dict:\n",
    "            a_logit = logprob_dict[a_tok_id].logprob\n",
    "\n",
    "        if b_tok_id in logprob_dict:\n",
    "            b_logit = logprob_dict[b_tok_id].logprob\n",
    "\n",
    "        if c_tok_id in logprob_dict:\n",
    "            c_logit = logprob_dict[c_tok_id].logprob\n",
    "\n",
    "        if d_tok_id in logprob_dict:\n",
    "            d_logit = logprob_dict[d_tok_id].logprob\n",
    "\n",
    "        if e_tok_id in logprob_dict:\n",
    "            e_logit = logprob_dict[e_tok_id].logprob\n",
    "\n",
    "        logits = np.array([a_logit, b_logit, c_logit, d_logit, e_logit])\n",
    "        logits_max = np.max(logits)\n",
    "        exp_logits = np.exp(logits - logits_max)\n",
    "        normalized_scores = exp_logits / np.sum(exp_logits)\n",
    "\n",
    "        QuestionId_Answer.append(qid)\n",
    "        MisconceptionId.append(cids)\n",
    "        scores.append(normalized_scores)\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"QuestionId_Answer\"] = QuestionId_Answer\n",
    "    result_df[\"MisconceptionId\"] = MisconceptionId\n",
    "    result_df[\"MisconceptionId\"] = result_df[\"MisconceptionId\"].apply(lambda x: [str(y) for y in x])\n",
    "    result_df[\"score\"] = scores\n",
    "\n",
    "    # ----\n",
    "    if cfg.use_tta:\n",
    "        result_df = result_df.explode([\"MisconceptionId\", \"score\"]).reset_index(drop=True)\n",
    "        result_df = result_df.groupby([\"QuestionId_Answer\", \"MisconceptionId\"]).agg({\"score\": \"mean\"}).reset_index()\n",
    "\n",
    "        # regroup --\n",
    "        agg_df = result_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n",
    "        score_agg_df = result_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n",
    "        agg_df = pd.merge(agg_df, score_agg_df, on=\"QuestionId_Answer\", how=\"left\")\n",
    "        result_df = agg_df.copy()\n",
    "\n",
    "    # --------\n",
    "    agg_df = result_df.copy()\n",
    "    agg_df[\"topk_info\"] = agg_df.apply(lambda x: sort_by_scores(x[\"MisconceptionId\"], x[\"score\"]), axis=1)\n",
    "    agg_df[\"MisconceptionId\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_ids\"])\n",
    "    agg_df[\"score\"] = agg_df[\"topk_info\"].apply(lambda x: x[\"sorted_scores\"])\n",
    "\n",
    "    # compute oof dataframe ---\n",
    "    oof_df = agg_df.copy()\n",
    "    oof_df = oof_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n",
    "    oof_df = oof_df.rename(columns={\"score\": \"logit_scores\"})\n",
    "\n",
    "    # normalize ---\n",
    "    oof_df[\"pred_scores\"] = oof_df[\"logit_scores\"].apply(stable_softmax)\n",
    "    oof_df[\"MisconceptionId\"] = oof_df[\"MisconceptionId\"].apply(lambda x: list(map(str, x)))\n",
    "\n",
    "    # print ---\n",
    "    print(\"--\" * 40)\n",
    "    row = oof_df.sample()\n",
    "    formatted_scores = [f\"{s:.3f}\" for s in row[\"pred_scores\"].values[0]]\n",
    "    misconceptions = row[\"MisconceptionId\"].values[0]\n",
    "    print(f\"Showing 1 example: {row['QuestionId_Answer'].values[0]}\")\n",
    "    for rank, (m, s) in enumerate(zip(misconceptions, formatted_scores)):\n",
    "        print(f\"MisconceptionId: {m} -> Score: {s}\")\n",
    "    print(\"--\" * 40)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"ranker_{model_id}.parquet\")\n",
    "    oof_df.to_parquet(save_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--config_path\", type=str, required=True)\n",
    "    ap.add_argument(\"--save_dir\", type=str, required=True)\n",
    "    ap.add_argument(\"--model_id\", type=str, required=True)\n",
    "\n",
    "    args = ap.parse_args()\n",
    "    cfg = OmegaConf.load(args.config_path)\n",
    "\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "    # execution ---\n",
    "    main(cfg, save_dir=args.save_dir, model_id=args.model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9356cb93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T18:57:09.070998Z",
     "iopub.status.busy": "2024-12-08T18:57:09.070710Z",
     "iopub.status.idle": "2024-12-08T18:57:09.075777Z",
     "shell.execute_reply": "2024-12-08T18:57:09.074953Z"
    },
    "papermill": {
     "duration": 0.054644,
     "end_time": "2024-12-08T18:57:09.077440",
     "exception": false,
     "start_time": "2024-12-08T18:57:09.022796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conf_expert_tutor.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conf_expert_tutor.yaml\n",
    "\n",
    "seed: 8798\n",
    "k_shot: 0\n",
    "\n",
    "use_tta: false\n",
    "\n",
    "input_path: ./ranker_outputs/tutor_input.parquet\n",
    "icl_path: /kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\n",
    "\n",
    "model:\n",
    "    backbone_path: \"/kaggle/input/eedi-tutor-72b-cv661-dec7-awq/transformers/default/1\"\n",
    "    max_length: 2048\n",
    "    num_proc: 2\n",
    "    \n",
    "    tokenizer:\n",
    "        padding_side: left\n",
    "        truncation_side: left\n",
    "        use_fast: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5af67fd",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-08T18:57:09.174833Z",
     "iopub.status.busy": "2024-12-08T18:57:09.174548Z",
     "iopub.status.idle": "2024-12-08T19:04:36.581682Z",
     "shell.execute_reply": "2024-12-08T19:04:36.580309Z"
    },
    "papermill": {
     "duration": 447.457577,
     "end_time": "2024-12-08T19:04:36.583558",
     "exception": false,
     "start_time": "2024-12-08T18:57:09.125981",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test data: (12, 12)\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Map (num_proc=2): 100%|██████████████████| 12/12 [00:01<00:00, 11.31 examples/s]\r\n",
      ">> EediRanker: A token id: 32\r\n",
      ">> EediRanker: B token id: 33\r\n",
      ">> EediRanker: C token id: 34\r\n",
      ">> EediRanker: D token id: 35\r\n",
      ">> EediRanker: E token id: 36\r\n",
      "Map: 100%|██████████████████████████████| 12/12 [00:00<00:00, 212.99 examples/s]\r\n",
      "# of requests: 12\r\n",
      "Example:\r\n",
      "\r\n",
      "<|im_start|>system\r\n",
      "Pick the misconception that explains the incorrect answer most specifically.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "A math problem is provided below together with the correct answer and an incorrect answer.\r\n",
      "\r\n",
      "Topic: BIDMAS - Use the order of operations to carry out calculations involving powers\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal \\( 13 \\) ?\r\n",
      "\r\n",
      "Correct Answer: \\( 3 \\times(2+4)-5 \\)\r\n",
      "Incorrect Answer: \\( 3 \\times 2+(4-5) \\)\r\n",
      "\r\n",
      "Thought: The student incorrectly prioritized the addition operation over multiplication, leading them to place brackets around \\(2 + 4\\) instead of \\(2 + 4\\) being grouped with multiplication. This misapplication caused the expression to evaluate as \\(3 \\times 2 + (-1) = 6 - 1 = 5\\), rather than reaching the correct answer of \\(13\\). By not adhering to the correct order of operations, the student failed to properly manage the precedence between multiplication and addition, resulting in an inaccurate solution.\r\n",
      "\r\n",
      "Thought: The student incorrectly prioritized the addition operation over multiplication, leading them to place brackets around \\(2 + 4\\) instead of \\(2 + 4\\) being grouped with multiplication. This misapplication caused the expression to evaluate as \\(3 \\times 2 + (-1) = 6 - 1 = 5\\), rather than reaching the correct answer of \\(13\\). By not adhering to the correct order of operations, the student failed to properly manage the precedence between multiplication and addition, resulting in an inaccurate solution.\r\n",
      "\r\n",
      "Thought: The student incorrectly prioritized the addition operation over multiplication, leading them to place brackets around \\(2 + 4\\) instead of \\(2 + 4\\) being grouped with multiplication. This misapplication caused the expression to evaluate as \\(3 \\times 2 + (-1) = 6 - 1 = 5\\), rather than reaching the correct answer of \\(13\\). By not adhering to the correct order of operations, the student failed to properly manage the precedence between multiplication and addition, resulting in an inaccurate solution.\r\n",
      "\r\n",
      "--\r\n",
      "A list of 5 candidate misconceptions that may explain the incorrect answer:\r\n",
      "A. Carries out operations from left to right regardless of priority order\r\n",
      "B. Carries out operations from right to left regardless of priority order\r\n",
      "C. Answers order of operations questions with brackets as if the brackets are not there\r\n",
      "D. Applies BIDMAS in strict order (does not realize addition and subtraction, and multiplication and division, are of equal priority)\r\n",
      "E. Carries out operations from left to right regardless of priority order, unless brackets are used\r\n",
      "Which misconception explains the incorrect answer most specifically? (A, B, C, D, or E)<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "data preparation done...\r\n",
      "WARNING 12-08 18:57:34 config.py:321] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-08 18:57:34 config.py:905] Defaulting to use mp for distributed inference\r\n",
      "WARNING 12-08 18:57:34 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 12-08 18:57:34 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/eedi-tutor-72b-cv661-dec7-awq/transformers/default/1', speculative_config=None, tokenizer='/kaggle/input/eedi-tutor-72b-cv661-dec7-awq/transformers/default/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/eedi-tutor-72b-cv661-dec7-awq/transformers/default/1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\r\n",
      "WARNING 12-08 18:57:35 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 12-08 18:57:35 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "INFO 12-08 18:57:35 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m INFO 12-08 18:57:35 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m INFO 12-08 18:57:35 selector.py:115] Using XFormers backend.\r\n",
      "INFO 12-08 18:57:35 selector.py:115] Using XFormers backend.\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m INFO 12-08 18:57:36 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "INFO 12-08 18:57:37 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m INFO 12-08 18:57:37 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m INFO 12-08 18:57:37 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-08 18:57:37 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-08 18:57:37 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m INFO 12-08 18:57:37 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "WARNING 12-08 18:57:37 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m WARNING 12-08 18:57:37 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 12-08 18:57:37 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f3445f89f90>, local_subscribe_port=37721, remote_subscribe_port=None)\r\n",
      "INFO 12-08 18:57:37 model_runner.py:1056] Starting to load model /kaggle/input/eedi-tutor-72b-cv661-dec7-awq/transformers/default/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m INFO 12-08 18:57:37 model_runner.py:1056] Starting to load model /kaggle/input/eedi-tutor-72b-cv661-dec7-awq/transformers/default/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m INFO 12-08 18:57:37 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-08 18:57:37 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-08 18:57:37 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m INFO 12-08 18:57:37 selector.py:115] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:45<06:04, 45.61s/it]\r\n",
      "Loading safetensors checkpoint shards:  22% Completed | 2/9 [01:27<05:04, 43.48s/it]\r\n",
      "Loading safetensors checkpoint shards:  33% Completed | 3/9 [01:56<03:41, 36.85s/it]\r\n",
      "Loading safetensors checkpoint shards:  44% Completed | 4/9 [02:39<03:16, 39.25s/it]\r\n",
      "Loading safetensors checkpoint shards:  56% Completed | 5/9 [02:54<02:01, 30.35s/it]\r\n",
      "Loading safetensors checkpoint shards:  67% Completed | 6/9 [03:37<01:44, 34.84s/it]\r\n",
      "Loading safetensors checkpoint shards:  78% Completed | 7/9 [04:18<01:13, 36.79s/it]\r\n",
      "Loading safetensors checkpoint shards:  89% Completed | 8/9 [05:00<00:38, 38.46s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [05:41<00:00, 39.42s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [05:42<00:00, 38.00s/it]\r\n",
      "\r\n",
      "INFO 12-08 19:03:30 model_runner.py:1067] Loading model weights took 11.9423 GB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m INFO 12-08 19:03:30 model_runner.py:1067] Loading model weights took 11.9423 GB\r\n",
      "INFO 12-08 19:03:41 distributed_gpu_executor.py:57] # GPU blocks: 325, # CPU blocks: 409\r\n",
      "INFO 12-08 19:03:41 distributed_gpu_executor.py:61] Maximum concurrency for 2048 tokens per request: 2.54x\r\n",
      "Processed prompts: 100%|█| 12/12 [00:40<00:00,  3.34s/it, est. speed input: 165.\r\n",
      "inference done...\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Showing 1 example: 2_A\r\n",
      "MisconceptionId: 1287 -> Score: 0.381\r\n",
      "MisconceptionId: 1059 -> Score: 0.156\r\n",
      "MisconceptionId: 1073 -> Score: 0.156\r\n",
      "MisconceptionId: 397 -> Score: 0.154\r\n",
      "MisconceptionId: 557 -> Score: 0.153\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "INFO 12-08 19:04:24 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=917)\u001b[0;0m INFO 12-08 19:04:24 multiproc_worker_utils.py:240] Worker exiting\r\n",
      "CPU times: user 5.52 s, sys: 1.36 s, total: 6.88 s\n",
      "Wall time: 7min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python run_expert_tutor.py --config_path \"./conf_expert_tutor.yaml\" --save_dir \"./ranker_outputs\" --model_id \"qwen_72b_tutor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3caecc4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T19:04:36.690760Z",
     "iopub.status.busy": "2024-12-08T19:04:36.690396Z",
     "iopub.status.idle": "2024-12-08T19:04:36.694680Z",
     "shell.execute_reply": "2024-12-08T19:04:36.693953Z"
    },
    "papermill": {
     "duration": 0.06076,
     "end_time": "2024-12-08T19:04:36.696621",
     "exception": false,
     "start_time": "2024-12-08T19:04:36.635861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# time for 16 examples (tta, no prompt caching): Processed prompts: 100%|█| 96/96 [04:45<00:00,  2.97s/it, est. speed input: 177.\n",
    "# time for 16 examples (tta, prompt caching)   : 100%|█| 96/96 [03:54<00:00,  2.44s/it, est. speed input: 215."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73c5eb",
   "metadata": {
    "papermill": {
     "duration": 0.051525,
     "end_time": "2024-12-08T19:04:36.798485",
     "exception": false,
     "start_time": "2024-12-08T19:04:36.746960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.2 Additional 32b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6101770d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T19:04:36.903787Z",
     "iopub.status.busy": "2024-12-08T19:04:36.903076Z",
     "iopub.status.idle": "2024-12-08T19:04:36.908874Z",
     "shell.execute_reply": "2024-12-08T19:04:36.908017Z"
    },
    "papermill": {
     "duration": 0.058697,
     "end_time": "2024-12-08T19:04:36.910543",
     "exception": false,
     "start_time": "2024-12-08T19:04:36.851846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conf_oracle_32b_cv652.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conf_oracle_32b_cv652.yaml\n",
    "\n",
    "seed: 9461\n",
    "k_shot: 0\n",
    "use_cot: false\n",
    "\n",
    "input_path: ./ranker_outputs/ranker_input_stage_four.parquet\n",
    "icl_path: /kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\n",
    "cot_path: ./gen/gen_qwen_14b.parquet\n",
    "\n",
    "model:\n",
    "    backbone_path: \"/kaggle/input/eedi-ranker-32b-cv655-nov24-custom-awq/transformers/default/1\"\n",
    "    max_length: 640\n",
    "    num_proc: 2\n",
    "    \n",
    "    tokenizer:\n",
    "        padding_side: left\n",
    "        truncation_side: left\n",
    "        use_fast: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6499b28",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-08T19:04:37.011167Z",
     "iopub.status.busy": "2024-12-08T19:04:37.010894Z",
     "iopub.status.idle": "2024-12-08T19:04:37.014295Z",
     "shell.execute_reply": "2024-12-08T19:04:37.013545Z"
    },
    "papermill": {
     "duration": 0.055693,
     "end_time": "2024-12-08T19:04:37.016080",
     "exception": false,
     "start_time": "2024-12-08T19:04:36.960387",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !python run_sonnet_v1.py --config_path \"./conf_oracle_32b_cv652.yaml\" --save_dir \"./ranker_outputs\" --model_id \"qwen_32b_oracle_support\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b421b",
   "metadata": {
    "papermill": {
     "duration": 0.050583,
     "end_time": "2024-12-08T19:04:37.115289",
     "exception": false,
     "start_time": "2024-12-08T19:04:37.064706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.3 Ensemble Rankers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1e03144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T19:04:37.221567Z",
     "iopub.status.busy": "2024-12-08T19:04:37.221291Z",
     "iopub.status.idle": "2024-12-08T19:04:37.228570Z",
     "shell.execute_reply": "2024-12-08T19:04:37.227725Z"
    },
    "papermill": {
     "duration": 0.062854,
     "end_time": "2024-12-08T19:04:37.230409",
     "exception": false,
     "start_time": "2024-12-08T19:04:37.167555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ensemble_rankers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ensemble_rankers.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def get_sorted_pairs(content_ids, scores):\n",
    "    collection = [(cid, s) for cid, s in zip(content_ids, scores)]\n",
    "    sorted_collection = sorted(collection, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_collection\n",
    "\n",
    "def stable_softmax(x, temp=1.0):\n",
    "    x = np.array(x) / temp\n",
    "    x_max = np.max(x)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "    \n",
    "def _load_df(pth, cutoff):\n",
    "    df = pd.read_parquet(pth)\n",
    "    # df[\"MisconceptionId\"] = df[\"MisconceptionId\"].apply(lambda x: x[:cutoff])\n",
    "    \n",
    "    print(f\"recomputing softmax for {pth}...\")\n",
    "    # df[\"logit_scores\"] = df[\"logit_scores\"].apply(lambda x: x[:cutoff])\n",
    "    df[\"pred_scores\"] = df[\"logit_scores\"].apply(stable_softmax) # recompute ---\n",
    "    print(df.sample().T)\n",
    "    print(\"--\"*40)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config-path\", type=str)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    with open(args.config_path, \"r\") as f:\n",
    "        cfg = OmegaConf.load(f)\n",
    "\n",
    "    # read predictions ---\n",
    "    one_preds = _load_df(cfg.ranker_1, cutoff=cfg.cutoff_n)\n",
    "    two_preds = _load_df(cfg.ranker_2, cutoff=cfg.cutoff_n)\n",
    "\n",
    "\n",
    "    # flatten\n",
    "    one_preds = one_preds[[\"QuestionId_Answer\", \"MisconceptionId\", \"pred_scores\"]].explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n",
    "    one_preds = one_preds.rename(columns={\"pred_scores\": \"score_one\"})\n",
    "    one_preds[\"MisconceptionId\"] = one_preds[\"MisconceptionId\"].astype(str)\n",
    "    \n",
    "    two_preds = two_preds[[\"QuestionId_Answer\", \"MisconceptionId\", \"pred_scores\"]].explode([\"MisconceptionId\", \"pred_scores\"]).reset_index(drop=True)\n",
    "    two_preds = two_preds.rename(columns={\"pred_scores\": \"score_two\"})\n",
    "    two_preds[\"MisconceptionId\"] = two_preds[\"MisconceptionId\"].astype(str)\n",
    "\n",
    "\n",
    "    # blend ---\n",
    "    w1, w2 = cfg.r1_weight, cfg.r2_weight\n",
    "\n",
    "    candidate_df = pd.merge(one_preds, two_preds, on=[\"QuestionId_Answer\", \"MisconceptionId\"])\n",
    "    print(candidate_df.sample().T)\n",
    "\n",
    "    candidate_df[\"score\"] = candidate_df.apply(lambda x: w1*x['score_one'] + w2*x['score_two'], axis=1) # blending\n",
    "    candidate_df = candidate_df[[\"QuestionId_Answer\", \"MisconceptionId\", \"score\"]].copy()\n",
    "\n",
    "    cdf = candidate_df.groupby(\"QuestionId_Answer\")[\"MisconceptionId\"].agg(list).reset_index()\n",
    "    sdf = candidate_df.groupby(\"QuestionId_Answer\")[\"score\"].agg(list).reset_index()\n",
    "    candidate_df = pd.merge(cdf, sdf, on=\"QuestionId_Answer\", how=\"left\")\n",
    "\n",
    "    candidate_df[\"sorted\"] = candidate_df.apply(lambda x: get_sorted_pairs(x['MisconceptionId'], x['score']), axis=1)\n",
    "    candidate_df[\"MisconceptionId\"] = candidate_df[\"sorted\"].apply(lambda x: [y[0] for y in x])\n",
    "    candidate_df[\"score\"] = candidate_df[\"sorted\"].apply(lambda x: [y[1] for y in x])\n",
    "\n",
    "    # Cut at N ---\n",
    "    print(\"Sample:\")\n",
    "    candidate_df = candidate_df.drop(columns=['sorted'])\n",
    "    print(candidate_df.sample().T)\n",
    "\n",
    "    # Updated input df for re-ranking ---\n",
    "    ranked_df = candidate_df[[\"QuestionId_Answer\", \"MisconceptionId\"]].copy()\n",
    "    ranked_df = ranked_df.reset_index(drop=True)\n",
    "    \n",
    "    print(\"--\"*40)\n",
    "    ranked_df.to_parquet(cfg.outfile_path)\n",
    "    print(ranked_df.sample(3))\n",
    "    print(\"--\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8fafe3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T19:04:37.336003Z",
     "iopub.status.busy": "2024-12-08T19:04:37.335419Z",
     "iopub.status.idle": "2024-12-08T19:04:37.340522Z",
     "shell.execute_reply": "2024-12-08T19:04:37.339651Z"
    },
    "papermill": {
     "duration": 0.058833,
     "end_time": "2024-12-08T19:04:37.342115",
     "exception": false,
     "start_time": "2024-12-08T19:04:37.283282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ensemble_rankers.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ensemble_rankers.yaml\n",
    "\n",
    "cutoff_n: 5 # 25 # TODO <- FIX\n",
    "\n",
    "ranker_1: ./ranker_outputs/ranker_qwen_72b_tutor.parquet\n",
    "ranker_2: ./ranker_outputs/ranker_qwen_32b_oracle_main.parquet\n",
    "\n",
    "r1_weight: 4.0\n",
    "r2_weight: 1.0\n",
    "\n",
    "outfile_path: ./ranker_outputs/ranker_blend_stage4.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc976789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T19:04:37.444030Z",
     "iopub.status.busy": "2024-12-08T19:04:37.443784Z",
     "iopub.status.idle": "2024-12-08T19:04:39.893364Z",
     "shell.execute_reply": "2024-12-08T19:04:39.892179Z"
    },
    "papermill": {
     "duration": 2.502611,
     "end_time": "2024-12-08T19:04:39.895674",
     "exception": false,
     "start_time": "2024-12-08T19:04:37.393063",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recomputing softmax for ./ranker_outputs/ranker_qwen_72b_tutor.parquet...\r\n",
      "                                                                  11\r\n",
      "QuestionId_Answer                                                3_D\r\n",
      "MisconceptionId                           [1180, 219, 451, 465, 515]\r\n",
      "logit_scores       [0.9543449829234928, 0.01350706416758239, 0.01...\r\n",
      "pred_scores        [0.3909395056074849, 0.152583946300969, 0.1524...\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "recomputing softmax for ./ranker_outputs/ranker_qwen_32b_oracle_main.parquet...\r\n",
      "                                                                   5\r\n",
      "QuestionId_Answer                                                1_C\r\n",
      "MisconceptionId        [1755, 1421, 891, 143, 2142, 2068, 1535, 418]\r\n",
      "logit_scores       [2.3281249403953552, 1.9218750894069672, 1.843...\r\n",
      "pred_scores        [0.21584247544606916, 0.1437821073620254, 0.13...\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "                         24\r\n",
      "QuestionId_Answer       1_B\r\n",
      "MisconceptionId         143\r\n",
      "score_one          0.185245\r\n",
      "score_two          0.398366\r\n",
      "Sample:\r\n",
      "                                                                   9\r\n",
      "QuestionId_Answer                                                3_A\r\n",
      "MisconceptionId                         [1180, 1202, 1071, 226, 515]\r\n",
      "score              [1.7738662123662872, 1.0266660560505263, 0.836...\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "   QuestionId_Answer                MisconceptionId\r\n",
      "7                2_C  [1287, 1059, 1073, 557, 1677]\r\n",
      "10               3_B     [1180, 599, 219, 226, 515]\r\n",
      "0                0_B  [1507, 2306, 2488, 1005, 706]\r\n",
      "--------------------------------------------------------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "!python ensemble_rankers.py --config-path ensemble_rankers.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596796f3",
   "metadata": {
    "papermill": {
     "duration": 0.050771,
     "end_time": "2024-12-08T19:04:40.001771",
     "exception": false,
     "start_time": "2024-12-08T19:04:39.951000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7d5d80a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T19:04:40.103018Z",
     "iopub.status.busy": "2024-12-08T19:04:40.102693Z",
     "iopub.status.idle": "2024-12-08T19:04:40.508483Z",
     "shell.execute_reply": "2024-12-08T19:04:40.507820Z"
    },
    "papermill": {
     "duration": 0.459348,
     "end_time": "2024-12-08T19:04:40.510485",
     "exception": false,
     "start_time": "2024-12-08T19:04:40.051137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "top_df = pd.read_parquet(\"./ranker_outputs/ranker_blend_stage4.parquet\").rename(columns={\"MisconceptionId\": \"top_ids\"})\n",
    "mid_df = pd.read_parquet(\"./ranker_outputs/two_three_blended.parquet\").rename(columns={\"MisconceptionId\": \"mid_ids\"}).drop(columns=['score'])\n",
    "low_df = pd.read_parquet(\"./ranker_outputs/one_two_blended.parquet\").rename(columns={\"MisconceptionId\": \"low_ids\"}).drop(columns=['score'])\n",
    "\n",
    "pred_df = pd.merge(top_df, mid_df, on=[\"QuestionId_Answer\"])\n",
    "pred_df = pd.merge(pred_df, low_df, on=[\"QuestionId_Answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ea0abda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T19:04:40.613000Z",
     "iopub.status.busy": "2024-12-08T19:04:40.612307Z",
     "iopub.status.idle": "2024-12-08T19:04:40.617650Z",
     "shell.execute_reply": "2024-12-08T19:04:40.616840Z"
    },
    "papermill": {
     "duration": 0.057577,
     "end_time": "2024-12-08T19:04:40.619378",
     "exception": false,
     "start_time": "2024-12-08T19:04:40.561801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_final_ids(row):\n",
    "    def _cast_to_str(x): return [str(y) for y in x]\n",
    "        \n",
    "    top_ids = _cast_to_str(row['top_ids'])\n",
    "    mid_ids = _cast_to_str(row['mid_ids'])\n",
    "    low_ids = _cast_to_str(row['low_ids'])\n",
    "\n",
    "    ret = list(top_ids)\n",
    "    \n",
    "    for this_id in mid_ids: # add mid ids\n",
    "        if this_id not in ret: ret.append(this_id)\n",
    "\n",
    "    for this_id in low_ids: # add mid ids\n",
    "        if this_id not in ret: ret.append(this_id)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc1b4cdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T19:04:40.720378Z",
     "iopub.status.busy": "2024-12-08T19:04:40.720115Z",
     "iopub.status.idle": "2024-12-08T19:04:40.736331Z",
     "shell.execute_reply": "2024-12-08T19:04:40.735641Z"
    },
    "papermill": {
     "duration": 0.068003,
     "end_time": "2024-12-08T19:04:40.737830",
     "exception": false,
     "start_time": "2024-12-08T19:04:40.669827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df[\"MisconceptionId\"] = pred_df.apply(get_final_ids, axis=1)\n",
    "pred_df[\"MisconceptionId\"] = pred_df[\"MisconceptionId\"].apply(lambda x: x[:25])\n",
    "pred_df[\"MisconceptionId\"] = pred_df[\"MisconceptionId\"].apply(lambda x: \" \".join(x))\n",
    "sub_df = pred_df[[\"QuestionId_Answer\", \"MisconceptionId\"]].copy()\n",
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "326cf6c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T19:04:40.838382Z",
     "iopub.status.busy": "2024-12-08T19:04:40.838134Z",
     "iopub.status.idle": "2024-12-08T19:04:40.851188Z",
     "shell.execute_reply": "2024-12-08T19:04:40.850367Z"
    },
    "papermill": {
     "duration": 0.066275,
     "end_time": "2024-12-08T19:04:40.852806",
     "exception": false,
     "start_time": "2024-12-08T19:04:40.786531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_B</td>\n",
       "      <td>1507 2306 2488 1005 706 1510 1963 1672 328 1345 2532 2181 1941 315 1054 1516 2518 2135 1338 1597 1392 1958 657 2586 234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_C</td>\n",
       "      <td>2488 1507 2306 1005 706 315 1941 1672 1345 2532 2181 1338 2518 1597 1963 220 328 1516 1392 160 1054 2135 373 969 2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_D</td>\n",
       "      <td>1507 2532 1005 328 1672 706 2306 1392 1516 315 1941 2488 2181 2518 1856 158 2586 1963 15 1890 1345 1416 871 1319 987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_A</td>\n",
       "      <td>1755 1535 891 2068 2142 1421 418 143 2398 167 2277 2256 1766 320 1606 1904 363 2078 1540 2131 1610 1432 1871 1256 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_B</td>\n",
       "      <td>143 891 363 2078 2398 1755 2142 418 167 2068 1991 1871 1540 1535 848 1766 2256 1610 885 1593 59 979 80 2307 113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer  \\\n",
       "0               0_B   \n",
       "1               0_C   \n",
       "2               0_D   \n",
       "3               1_A   \n",
       "4               1_B   \n",
       "\n",
       "                                                                                                           MisconceptionId  \n",
       "0  1507 2306 2488 1005 706 1510 1963 1672 328 1345 2532 2181 1941 315 1054 1516 2518 2135 1338 1597 1392 1958 657 2586 234  \n",
       "1    2488 1507 2306 1005 706 315 1941 1672 1345 2532 2181 1338 2518 1597 1963 220 328 1516 1392 160 1054 2135 373 969 2586  \n",
       "2     1507 2532 1005 328 1672 706 2306 1392 1516 315 1941 2488 2181 2518 1856 158 2586 1963 15 1890 1345 1416 871 1319 987  \n",
       "3     1755 1535 891 2068 2142 1421 418 143 2398 167 2277 2256 1766 320 1606 1904 363 2078 1540 2131 1610 1432 1871 1256 59  \n",
       "4          143 891 363 2078 2398 1755 2142 418 167 2068 1991 1871 1540 1535 848 1766 2256 1610 885 1593 59 979 80 2307 113  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.read_csv(\"submission.csv\")\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb10d933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T19:04:40.955773Z",
     "iopub.status.busy": "2024-12-08T19:04:40.955036Z",
     "iopub.status.idle": "2024-12-08T19:04:40.958638Z",
     "shell.execute_reply": "2024-12-08T19:04:40.957898Z"
    },
    "papermill": {
     "duration": 0.056078,
     "end_time": "2024-12-08T19:04:40.960276",
     "exception": false,
     "start_time": "2024-12-08T19:04:40.904198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# End ---#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0552c",
   "metadata": {
    "papermill": {
     "duration": 0.050259,
     "end_time": "2024-12-08T19:04:41.060827",
     "exception": false,
     "start_time": "2024-12-08T19:04:41.010568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b0cfc7",
   "metadata": {
    "papermill": {
     "duration": 0.049798,
     "end_time": "2024-12-08T19:04:41.160966",
     "exception": false,
     "start_time": "2024-12-08T19:04:41.111168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66404e61",
   "metadata": {
    "papermill": {
     "duration": 0.049543,
     "end_time": "2024-12-08T19:04:41.261246",
     "exception": false,
     "start_time": "2024-12-08T19:04:41.211703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6044627,
     "sourceId": 9851004,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6093908,
     "sourceId": 9916435,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6138580,
     "sourceId": 9976695,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6141925,
     "sourceId": 9981326,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6150200,
     "sourceId": 9992860,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6246161,
     "sourceId": 10122395,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6251458,
     "sourceId": 10129723,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10130031,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6143574,
     "sourceId": 9983569,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 202903897,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 210357319,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 176386,
     "modelInstanceId": 153926,
     "sourceId": 180631,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 182502,
     "modelInstanceId": 160135,
     "sourceId": 187838,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 183906,
     "modelInstanceId": 161545,
     "sourceId": 189477,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 183919,
     "modelInstanceId": 161818,
     "sourceId": 189794,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 184989,
     "modelInstanceId": 162643,
     "sourceId": 190807,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 185622,
     "modelInstanceId": 163276,
     "sourceId": 191552,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 185687,
     "modelInstanceId": 163332,
     "sourceId": 191622,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1588.021623,
   "end_time": "2024-12-08T19:04:41.630064",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-08T18:38:13.608441",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
