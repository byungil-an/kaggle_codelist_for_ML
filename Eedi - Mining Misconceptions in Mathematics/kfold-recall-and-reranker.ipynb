{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14bf7f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:18:29.938902Z",
     "iopub.status.busy": "2024-12-12T11:18:29.938546Z",
     "iopub.status.idle": "2024-12-12T11:21:59.203609Z",
     "shell.execute_reply": "2024-12-12T11:21:59.202522Z"
    },
    "papermill": {
     "duration": 209.284533,
     "end_time": "2024-12-12T11:21:59.214021",
     "exception": false,
     "start_time": "2024-12-12T11:18:29.929488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n",
      "CPU times: user 2.06 s, sys: 555 ms, total: 2.61 s\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip uninstall -y torch\n",
    "!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n",
    "!pip install -q -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d429670",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:21:59.230932Z",
     "iopub.status.busy": "2024-12-12T11:21:59.229945Z",
     "iopub.status.idle": "2024-12-12T11:22:07.608050Z",
     "shell.execute_reply": "2024-12-12T11:22:07.606842Z"
    },
    "papermill": {
     "duration": 8.388524,
     "end_time": "2024-12-12T11:22:07.610208",
     "exception": false,
     "start_time": "2024-12-12T11:21:59.221684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !pip install --no-index  /kaggle/input/trans-pip/transformers-4.46.2-py3-none-any.whl --find-links=/kaggle/input/trans-pip\n",
    "# !pip install --no-index /kaggle/input/trans-pip/tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --find-links=/kaggle/input/trans-pip\n",
    "\n",
    "!pip install transformers peft accelerate \\\n",
    "    -q -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80c5603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:22:07.627774Z",
     "iopub.status.busy": "2024-12-12T11:22:07.626910Z",
     "iopub.status.idle": "2024-12-12T11:22:48.728288Z",
     "shell.execute_reply": "2024-12-12T11:22:48.727061Z"
    },
    "papermill": {
     "duration": 41.112325,
     "end_time": "2024-12-12T11:22:48.730403",
     "exception": false,
     "start_time": "2024-12-12T11:22:07.618078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --no-index /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/optimum-1.21.2-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --find-links=/kaggle/input/bitsandbytes0-42-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42b67f69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:22:48.746748Z",
     "iopub.status.busy": "2024-12-12T11:22:48.746436Z",
     "iopub.status.idle": "2024-12-12T11:24:16.129795Z",
     "shell.execute_reply": "2024-12-12T11:24:16.128755Z"
    },
    "papermill": {
     "duration": 87.393945,
     "end_time": "2024-12-12T11:24:16.132020",
     "exception": false,
     "start_time": "2024-12-12T11:22:48.738075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/trans-pip/tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers==0.20.3) (0.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (3.15.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (2024.6.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (4.66.4)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.20.3) (2024.8.30)\r\n",
      "Installing collected packages: tokenizers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.19.1\r\n",
      "    Uninstalling tokenizers-0.19.1:\r\n",
      "      Successfully uninstalled tokenizers-0.19.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "transformers 4.42.4 requires tokenizers<0.20,>=0.19, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed tokenizers-0.20.3\r\n",
      "Processing /kaggle/input/trans-pip/transformers-4.46.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (3.15.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (0.25.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (2.32.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (0.4.5)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (0.20.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (4.66.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (2024.6.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.46.2) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.2) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.2) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.2) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.2) (2024.8.30)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.42.4\r\n",
      "    Uninstalling transformers-4.42.4:\r\n",
      "      Successfully uninstalled transformers-4.42.4\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "optimum 1.21.2 requires transformers[sentencepiece]<4.43.0,>=4.26.0, but you have transformers 4.46.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed transformers-4.46.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/trans-pip/tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install /kaggle/input/trans-pip/transformers-4.46.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "581cea67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:24:16.151269Z",
     "iopub.status.busy": "2024-12-12T11:24:16.151004Z",
     "iopub.status.idle": "2024-12-12T11:24:32.948902Z",
     "shell.execute_reply": "2024-12-12T11:24:32.948017Z"
    },
    "papermill": {
     "duration": 16.810108,
     "end_time": "2024-12-12T11:24:32.950956",
     "exception": false,
     "start_time": "2024-12-12T11:24:16.140848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# from bs4 import BeautifulSoup\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "from peft import PeftModel\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def apk(actual, predicted, k=25):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    \n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    \n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        # first condition checks whether it is valid prediction\n",
    "        # second condition checks if prediction is not repeated\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=25):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    \n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n",
    "\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 2\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "    pids = list(df['order_index'].values)\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    sentence_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    result = {pids[i]: em for i, em in enumerate(sentence_embeddings)}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cdf7bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:24:32.968851Z",
     "iopub.status.busy": "2024-12-12T11:24:32.968337Z",
     "iopub.status.idle": "2024-12-12T11:24:33.881930Z",
     "shell.execute_reply": "2024-12-12T11:24:33.880924Z"
    },
    "papermill": {
     "duration": 0.924257,
     "end_time": "2024-12-12T11:24:33.883619",
     "exception": false,
     "start_time": "2024-12-12T11:24:32.959362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "# task_description = 'Given a math question and a misconcepte incorrect answer, retrieve the most accurate explanation for the misconception.'\n",
    "\n",
    "\n",
    "\n",
    "task_description = 'Given a math question with correct answer and a misconcepted incorrect answer, retrieve the most accurate misconception for the incorrect answer.'\n",
    "\n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "\n",
    "\n",
    "tra = pd.read_csv(f\"{path_prefix}/test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "misconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\n",
    "\n",
    "\n",
    "first_model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(first_model_path)\n",
    "\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "PROMPT  = \"\"\"Here is a question about {ConstructName} ({SubjectName}).\n",
    "Question: {Question}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "\n",
    "Generate a brief rationale for the Incorrect Answer in a sentences, within 50 tokens. Describe the reasoning process that might lead to this choice, including any logical errors or misconceptions.\n",
    "\n",
    "Keep the explanation concise and focused on the key points, and ensure it does not exceed 50 tokens\n",
    "***Important***\n",
    "1.There is no need to analyze the relationship between the wrong answer and the correct answer. Simply describe the rationale of the incorrect answer in a few words.\n",
    "2.There is no need to have fields such as \"Incorrect Answer:\" and \"Incorrect Answer is\". Just give the rationale directly.\n",
    "3.Output results in English.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_template(row, tokenizer, targetCol):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": preprocess_text(\n",
    "                PROMPT.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[f\"Answer{targetCol}Text\"],\n",
    "                    CorrectAnswer=row[f\"Answer{row.CorrectAnswer}Text\"],\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "train_data = []\n",
    "for _,row in tra.iterrows():\n",
    "    for c in ['A','B','C','D']:\n",
    "        if c ==row['CorrectAnswer']:\n",
    "            continue\n",
    "        if f'Answer{c}Text' not in row:\n",
    "            continue\n",
    "        real_answer_id = row['CorrectAnswer']\n",
    "        real_text = row[f'Answer{real_answer_id}Text']\n",
    "        # query_text =f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{real_text}\\n###Misconcepte Incorrect answer###:{c}.{row[f'Answer{c}Text']}\"\n",
    "        query_text = f\"### SubjectName: {row['SubjectName']}\\n### ConstructName: {row['ConstructName']}\\n### Question: {row['QuestionText']}\\n### Correct Answer: {real_text}\\n### Misconcepte Incorrect answer: {row[f'Answer{c}Text']}\"\n",
    "        row['query_text'] = get_detailed_instruct(task_description,query_text)\n",
    "        row['answer_name'] = c\n",
    "        # row['input_llm_text'] = apply_template(row, tokenizer, c)\n",
    "        # row['input_llm_text'] = apply_template(tokenizer, query_text)\n",
    "        row['input_llm_text'] = apply_template(row, tokenizer, c)\n",
    "        row['answer_name'] = c ### add\n",
    "        train_data.append(copy.deepcopy(row))\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df['order_index'] = list(range(len(train_df)))\n",
    "\n",
    "\n",
    "train_df.to_parquet(\"train_df.parquet\", index=False)\n",
    "\n",
    "import gc\n",
    "del train_df, tra, tokenizer\n",
    "gc.collect()\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf1198d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:24:33.901765Z",
     "iopub.status.busy": "2024-12-12T11:24:33.901091Z",
     "iopub.status.idle": "2024-12-12T11:24:33.907112Z",
     "shell.execute_reply": "2024-12-12T11:24:33.906208Z"
    },
    "papermill": {
     "duration": 0.016883,
     "end_time": "2024-12-12T11:24:33.908787",
     "exception": false,
     "start_time": "2024-12-12T11:24:33.891904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm_for_rationale.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm_for_rationale.py\n",
    "\n",
    "import re\n",
    "import vllm\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_parquet(\"train_df.parquet\")\n",
    "first_model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    first_model_path,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.90, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "\n",
    "responses = llm.generate(\n",
    "    train_df[\"input_llm_text\"].values,\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  # Number of output sequences to return for each prompt.\n",
    "        top_p=0.8,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "        temperature=0,  # randomness of the sampling\n",
    "        seed=777, # Seed for reprodicibility\n",
    "        skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "        max_tokens=256,  # Maximum number of tokens to generate per output sequence.\n",
    "    ),\n",
    "    use_tqdm=True\n",
    ")\n",
    "\n",
    "responses = [x.outputs[0].text for x in responses]\n",
    "train_df[\"rationale\"] = responses\n",
    "train_df['query_text'] = train_df['query_text'] + \"\\n### Rationale: \" + train_df['rationale']\n",
    "\n",
    "\n",
    "train_df.to_parquet(\"tmp.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e5df74e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:24:33.926283Z",
     "iopub.status.busy": "2024-12-12T11:24:33.926046Z",
     "iopub.status.idle": "2024-12-12T11:26:45.990120Z",
     "shell.execute_reply": "2024-12-12T11:26:45.989185Z"
    },
    "papermill": {
     "duration": 132.075538,
     "end_time": "2024-12-12T11:26:45.992353",
     "exception": false,
     "start_time": "2024-12-12T11:24:33.916815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-12 11:24:38 config.py:246] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-12 11:24:38 config.py:715] Defaulting to use mp for distributed inference\r\n",
      "INFO 12-12 11:24:38 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, use_v2_block_manager=False, enable_prefix_caching=False)\r\n",
      "INFO 12-12 11:24:38 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "INFO 12-12 11:24:38 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 12-12 11:24:38 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 11:24:38 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 12-12 11:24:38 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 12-12 11:24:39 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "INFO 12-12 11:24:40 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 12-12 11:24:40 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 12-12 11:24:40 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-12 11:24:40 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-12 11:24:40 custom_all_reduce_utils.py:202] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-12 11:24:48 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 12-12 11:24:48 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-12 11:24:48 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f383c586050>, local_subscribe_port=43381, local_sync_port=41521, remote_subscribe_port=None, remote_sync_port=None)\r\n",
      "INFO 12-12 11:24:48 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 12-12 11:24:48 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 12-12 11:24:48 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 12-12 11:24:48 selector.py:54] Using XFormers backend.\r\n",
      "INFO 12-12 11:24:48 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 11:24:48 selector.py:54] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:15<01:02, 15.63s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:33<00:50, 16.94s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:51<00:35, 17.55s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [01:10<00:18, 18.15s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:30<00:00, 18.76s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:30<00:00, 18.14s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 12-12 11:26:19 model_runner.py:692] Loading model weights took 9.0934 GB\r\n",
      "INFO 12-12 11:26:19 model_runner.py:692] Loading model weights took 9.0934 GB\r\n",
      "INFO 12-12 11:26:28 distributed_gpu_executor.py:56] # GPU blocks: 795, # CPU blocks: 2048\r\n",
      "Processed prompts: 100%|â–ˆ| 9/9 [00:09<00:00,  1.10s/it, est. speed input: 229.29\r\n",
      "[rank0]:[W CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\r\n"
     ]
    }
   ],
   "source": [
    "!python run_vllm_for_rationale.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c3c9a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.014352Z",
     "iopub.status.busy": "2024-12-12T11:26:46.013999Z",
     "iopub.status.idle": "2024-12-12T11:26:46.075925Z",
     "shell.execute_reply": "2024-12-12T11:26:46.075335Z"
    },
    "papermill": {
     "duration": 0.074622,
     "end_time": "2024-12-12T11:26:46.077482",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.002860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"tmp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1680f44a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.097733Z",
     "iopub.status.busy": "2024-12-12T11:26:46.097497Z",
     "iopub.status.idle": "2024-12-12T11:26:46.115255Z",
     "shell.execute_reply": "2024-12-12T11:26:46.114473Z"
    },
    "papermill": {
     "duration": 0.029634,
     "end_time": "2024-12-12T11:26:46.116884",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.087250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>ConstructId</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectId</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>CorrectAnswer</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>AnswerAText</th>\n",
       "      <th>AnswerBText</th>\n",
       "      <th>AnswerCText</th>\n",
       "      <th>AnswerDText</th>\n",
       "      <th>query_text</th>\n",
       "      <th>answer_name</th>\n",
       "      <th>input_llm_text</th>\n",
       "      <th>order_index</th>\n",
       "      <th>rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>B</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>0</td>\n",
       "      <td>Adding brackets around 4-5 keeps subtraction i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>C</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mistakenly grouping all numbers in brackets, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>D</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>2</td>\n",
       "      <td>Believes no brackets are needed as operations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>D</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>A</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>3</td>\n",
       "      <td>One might factor the numerator as (m+3)(m-1) a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>D</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>B</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>4</td>\n",
       "      <td>One might incorrectly factor the numerator as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>D</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>C</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>5</td>\n",
       "      <td>One might factor the numerator as (m-1)(m+3) a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>B</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nTom</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Both Tom and Katie</td>\n",
       "      <td>Neither is correct</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>A</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>6</td>\n",
       "      <td>Cutting plants in half would halve the range, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>B</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nTom</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Both Tom and Katie</td>\n",
       "      <td>Neither is correct</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>C</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>7</td>\n",
       "      <td>One might think both are correct if they misun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>B</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nTom</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Both Tom and Katie</td>\n",
       "      <td>Neither is correct</td>\n",
       "      <td>Instruct: Given a math question with correct a...</td>\n",
       "      <td>D</td>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are Qwen, created by A...</td>\n",
       "      <td>8</td>\n",
       "      <td>Both actions change the data set. Halving the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QuestionId  ConstructId                                      ConstructName  \\\n",
       "0        1869          856  Use the order of operations to carry out calcu...   \n",
       "1        1869          856  Use the order of operations to carry out calcu...   \n",
       "2        1869          856  Use the order of operations to carry out calcu...   \n",
       "3        1870         1612  Simplify an algebraic fraction by factorising ...   \n",
       "4        1870         1612  Simplify an algebraic fraction by factorising ...   \n",
       "5        1870         1612  Simplify an algebraic fraction by factorising ...   \n",
       "6        1871         2774            Calculate the range from a list of data   \n",
       "7        1871         2774            Calculate the range from a list of data   \n",
       "8        1871         2774            Calculate the range from a list of data   \n",
       "\n",
       "   SubjectId                                        SubjectName CorrectAnswer  \\\n",
       "0         33                                             BIDMAS             A   \n",
       "1         33                                             BIDMAS             A   \n",
       "2         33                                             BIDMAS             A   \n",
       "3       1077                    Simplifying Algebraic Fractions             D   \n",
       "4       1077                    Simplifying Algebraic Fractions             D   \n",
       "5       1077                    Simplifying Algebraic Fractions             D   \n",
       "6        339  Range and Interquartile Range from a List of Data             B   \n",
       "7        339  Range and Interquartile Range from a List of Data             B   \n",
       "8        339  Range and Interquartile Range from a List of Data             B   \n",
       "\n",
       "                                        QuestionText            AnswerAText  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "2  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "3  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n",
       "4  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n",
       "5  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n",
       "6  Tom and Katie are discussing the \\( 5 \\) plant...              Only\\nTom   \n",
       "7  Tom and Katie are discussing the \\( 5 \\) plant...              Only\\nTom   \n",
       "8  Tom and Katie are discussing the \\( 5 \\) plant...              Only\\nTom   \n",
       "\n",
       "              AnswerBText            AnswerCText             AnswerDText  \\\n",
       "0  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  Does not need brackets   \n",
       "1  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  Does not need brackets   \n",
       "2  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  Does not need brackets   \n",
       "3               \\( m+2 \\)              \\( m-1 \\)       Does not simplify   \n",
       "4               \\( m+2 \\)              \\( m-1 \\)       Does not simplify   \n",
       "5               \\( m+2 \\)              \\( m-1 \\)       Does not simplify   \n",
       "6             Only\\nKatie     Both Tom and Katie      Neither is correct   \n",
       "7             Only\\nKatie     Both Tom and Katie      Neither is correct   \n",
       "8             Only\\nKatie     Both Tom and Katie      Neither is correct   \n",
       "\n",
       "                                          query_text answer_name  \\\n",
       "0  Instruct: Given a math question with correct a...           B   \n",
       "1  Instruct: Given a math question with correct a...           C   \n",
       "2  Instruct: Given a math question with correct a...           D   \n",
       "3  Instruct: Given a math question with correct a...           A   \n",
       "4  Instruct: Given a math question with correct a...           B   \n",
       "5  Instruct: Given a math question with correct a...           C   \n",
       "6  Instruct: Given a math question with correct a...           A   \n",
       "7  Instruct: Given a math question with correct a...           C   \n",
       "8  Instruct: Given a math question with correct a...           D   \n",
       "\n",
       "                                      input_llm_text  order_index  \\\n",
       "0  <|im_start|>system\\nYou are Qwen, created by A...            0   \n",
       "1  <|im_start|>system\\nYou are Qwen, created by A...            1   \n",
       "2  <|im_start|>system\\nYou are Qwen, created by A...            2   \n",
       "3  <|im_start|>system\\nYou are Qwen, created by A...            3   \n",
       "4  <|im_start|>system\\nYou are Qwen, created by A...            4   \n",
       "5  <|im_start|>system\\nYou are Qwen, created by A...            5   \n",
       "6  <|im_start|>system\\nYou are Qwen, created by A...            6   \n",
       "7  <|im_start|>system\\nYou are Qwen, created by A...            7   \n",
       "8  <|im_start|>system\\nYou are Qwen, created by A...            8   \n",
       "\n",
       "                                           rationale  \n",
       "0  Adding brackets around 4-5 keeps subtraction i...  \n",
       "1  Mistakenly grouping all numbers in brackets, i...  \n",
       "2  Believes no brackets are needed as operations ...  \n",
       "3  One might factor the numerator as (m+3)(m-1) a...  \n",
       "4  One might incorrectly factor the numerator as ...  \n",
       "5  One might factor the numerator as (m-1)(m+3) a...  \n",
       "6  Cutting plants in half would halve the range, ...  \n",
       "7  One might think both are correct if they misun...  \n",
       "8  Both actions change the data set. Halving the ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686efada",
   "metadata": {
    "papermill": {
     "duration": 0.009606,
     "end_time": "2024-12-12T11:26:46.136686",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.127080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd0b5e",
   "metadata": {
    "papermill": {
     "duration": 0.009585,
     "end_time": "2024-12-12T11:26:46.156030",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.146445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a331c",
   "metadata": {
    "papermill": {
     "duration": 0.009601,
     "end_time": "2024-12-12T11:26:46.175321",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.165720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b305ea",
   "metadata": {
    "papermill": {
     "duration": 0.009649,
     "end_time": "2024-12-12T11:26:46.194608",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.184959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eafdf1d",
   "metadata": {
    "papermill": {
     "duration": 0.01006,
     "end_time": "2024-12-12T11:26:46.214354",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.204294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f54391",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.235552Z",
     "iopub.status.busy": "2024-12-12T11:26:46.235306Z",
     "iopub.status.idle": "2024-12-12T11:26:46.243314Z",
     "shell.execute_reply": "2024-12-12T11:26:46.242365Z"
    },
    "papermill": {
     "duration": 0.020596,
     "end_time": "2024-12-12T11:26:46.244920",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.224324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fold1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fold1.py\n",
    "from tqdm.auto import tqdm\n",
    "# from bs4 import BeautifulSoup\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "from peft import PeftModel\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "\n",
    "fold = 1\n",
    "train_df = pd.read_parquet(\"tmp.parquet\")\n",
    "\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 2\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "    pids = list(df['order_index'].values)\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    sentence_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    result = {int(pids[i]): em for i, em in enumerate(sentence_embeddings)}\n",
    "    return result\n",
    "\n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "model_path = \"/kaggle/input/qwen-instruct-14b\"\n",
    "lora_path=\"/kaggle/input/aa1201-1815/hp_code_64_128_epoch5_tmp002/epoch_4_model\" #529\n",
    "# lora_path = \"/kaggle/input/v8v13v14-19/v8v13v14_19/lora_merge.bin\"\n",
    "# device='cuda'\n",
    "VALID = False\n",
    "\n",
    "misconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#         )\n",
    "model = AutoModel.from_pretrained(model_path, \n",
    "                                  torch_dtype=torch.float16,\n",
    "                                  # quantization_config=bnb_config,\n",
    "                                  device_map='auto',\n",
    "                                 )\n",
    "if lora_path!=\"none\":\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, lora_path)#.eval().to(args.device)\n",
    "    # model = model.merge_and_unload()\n",
    "model = model.eval()\n",
    "\n",
    "train_embeddings = inference(train_df, model, tokenizer, 'cuda')\n",
    "\n",
    "misconception_mapping['query_text'] = misconception_mapping['MisconceptionName']\n",
    "misconception_mapping['order_index'] = misconception_mapping['MisconceptionId']\n",
    "doc_embeddings = inference(misconception_mapping, model, tokenizer, 'cuda')\n",
    "\n",
    "sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "index_text_embeddings_index = {index: paper_id for index, paper_id in\n",
    "                                         enumerate(list(doc_embeddings.keys()))}\n",
    "\n",
    "\n",
    "cosine_similarities = []\n",
    "for _, row in tqdm(train_df.iterrows()):\n",
    "    query_id = row['order_index']\n",
    "    query_em = train_embeddings[query_id].reshape(1, -1)\n",
    "    \n",
    "    cosine_similarity = np.dot(query_em, sentence_embeddings.T).flatten()\n",
    "    \n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "cosine_similarities = np.vstack(cosine_similarities)\n",
    "# sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "# cosine_similarities = np.dot(train_embeddings.values(), sentence_embeddings.T)\n",
    "np.save(f\"cosine_similarities_model_{fold}.npy\", cosine_similarities)\n",
    "\n",
    "np.save(f\"sentence_embeddings_{fold}.npy\", sentence_embeddings)\n",
    "\n",
    "train_embeddings = {key: train_embeddings[key] for key in sorted(train_embeddings)}\n",
    "train_embeddings = np.concatenate([e.reshape(1, -1) for e in list(train_embeddings.values())])\n",
    "np.save(f\"train_embeddings_{fold}.npy\", train_embeddings)\n",
    "\n",
    "with open(\"index_text_embeddings_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(index_text_embeddings_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "220abf59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.265738Z",
     "iopub.status.busy": "2024-12-12T11:26:46.265489Z",
     "iopub.status.idle": "2024-12-12T11:26:46.268677Z",
     "shell.execute_reply": "2024-12-12T11:26:46.268012Z"
    },
    "papermill": {
     "duration": 0.015499,
     "end_time": "2024-12-12T11:26:46.270268",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.254769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python fold1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d20667",
   "metadata": {
    "papermill": {
     "duration": 0.009977,
     "end_time": "2024-12-12T11:26:46.290320",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.280343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7355fe30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.311691Z",
     "iopub.status.busy": "2024-12-12T11:26:46.311444Z",
     "iopub.status.idle": "2024-12-12T11:26:46.318675Z",
     "shell.execute_reply": "2024-12-12T11:26:46.317698Z"
    },
    "papermill": {
     "duration": 0.019812,
     "end_time": "2024-12-12T11:26:46.320280",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.300468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fold2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fold2.py\n",
    "from tqdm.auto import tqdm\n",
    "# from bs4 import BeautifulSoup\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "from peft import PeftModel\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "\n",
    "fold = 2\n",
    "train_df = pd.read_parquet(\"tmp.parquet\")\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 2\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "    pids = list(df['order_index'].values)\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    sentence_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    result = {int(pids[i]): em for i, em in enumerate(sentence_embeddings)}\n",
    "    return result\n",
    "\n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "model_path = \"/kaggle/input/qwen-instruct-14b\"\n",
    "# lora_path=\"/kaggle/input/aa1203-0254/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd001/epoch_4_model\" #527\n",
    "lora_path=\"/kaggle/input/zsq-other-fourfold-529/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd000_6ra_fold2/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd000_6ra_fold2/epoch_4_model\"\n",
    "# lora_path = \"/kaggle/input/zsq-other-fourfold-529-wd001/rhp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd001_6ra_fold2/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd001_6ra_fold2/epoch_4_model\"\n",
    "# device='cuda'\n",
    "VALID = False\n",
    "\n",
    "misconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#         )\n",
    "model = AutoModel.from_pretrained(model_path, \n",
    "                                  torch_dtype=torch.float16,\n",
    "                                  # quantization_config=bnb_config,\n",
    "                                  device_map='auto',\n",
    "                                 )\n",
    "if lora_path!=\"none\":\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, lora_path)#.eval().to(args.device)\n",
    "    # model = model.merge_and_unload()\n",
    "model = model.eval()\n",
    "\n",
    "train_embeddings = inference(train_df, model, tokenizer, 'cuda')\n",
    "\n",
    "misconception_mapping['query_text'] = misconception_mapping['MisconceptionName']\n",
    "misconception_mapping['order_index'] = misconception_mapping['MisconceptionId']\n",
    "doc_embeddings = inference(misconception_mapping, model, tokenizer, 'cuda')\n",
    "\n",
    "sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "index_text_embeddings_index = {index: paper_id for index, paper_id in\n",
    "                                         enumerate(list(doc_embeddings.keys()))}\n",
    "\n",
    "\n",
    "cosine_similarities = []\n",
    "for _, row in tqdm(train_df.iterrows()):\n",
    "    query_id = row['order_index']\n",
    "    query_em = train_embeddings[query_id].reshape(1, -1)\n",
    "    \n",
    "    cosine_similarity = np.dot(query_em, sentence_embeddings.T).flatten()\n",
    "    \n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "cosine_similarities = np.vstack(cosine_similarities)\n",
    "# sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "# cosine_similarities = np.dot(train_embeddings.values(), sentence_embeddings.T)\n",
    "np.save(f\"cosine_similarities_model_{fold}.npy\", cosine_similarities)\n",
    "\n",
    "np.save(f\"sentence_embeddings_{fold}.npy\", sentence_embeddings)\n",
    "\n",
    "train_embeddings = {key: train_embeddings[key] for key in sorted(train_embeddings)}\n",
    "train_embeddings = np.concatenate([e.reshape(1, -1) for e in list(train_embeddings.values())])\n",
    "np.save(f\"train_embeddings_{fold}.npy\", train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84f6610d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.341329Z",
     "iopub.status.busy": "2024-12-12T11:26:46.341103Z",
     "iopub.status.idle": "2024-12-12T11:26:46.344417Z",
     "shell.execute_reply": "2024-12-12T11:26:46.343667Z"
    },
    "papermill": {
     "duration": 0.015771,
     "end_time": "2024-12-12T11:26:46.346197",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.330426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python fold2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "819c8fb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.367721Z",
     "iopub.status.busy": "2024-12-12T11:26:46.367477Z",
     "iopub.status.idle": "2024-12-12T11:26:46.374484Z",
     "shell.execute_reply": "2024-12-12T11:26:46.373607Z"
    },
    "papermill": {
     "duration": 0.019437,
     "end_time": "2024-12-12T11:26:46.376042",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.356605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fold3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fold3.py\n",
    "from tqdm.auto import tqdm\n",
    "# from bs4 import BeautifulSoup\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "from peft import PeftModel\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "\n",
    "fold = 3\n",
    "train_df = pd.read_parquet(\"tmp.parquet\")\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 2\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "    pids = list(df['order_index'].values)\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    sentence_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    result = {int(pids[i]): em for i, em in enumerate(sentence_embeddings)}\n",
    "    return result\n",
    "\n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "model_path = \"/kaggle/input/qwen-instruct-14b\"\n",
    "# lora_path=\"/kaggle/input/aa1203-2305/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd000_5ra/epoch_4_model\" #527\n",
    "lora_path = \"/kaggle/input/zsq-other-fourfold-529/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd000_6ra_fold3/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd000_6ra_fold3/epoch_4_model\"\n",
    "# lora_path = \"/kaggle/input/zsq-other-fourfold-529-wd001/rhp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd001_6ra_fold3/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd001_6ra_fold3/epoch_4_model\"\n",
    "# device='cuda'\n",
    "VALID = False\n",
    "\n",
    "misconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#         )\n",
    "model = AutoModel.from_pretrained(model_path, \n",
    "                                  torch_dtype=torch.float16,\n",
    "                                  # quantization_config=bnb_config,\n",
    "                                  device_map='auto',\n",
    "                                 )\n",
    "if lora_path!=\"none\":\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, lora_path)#.eval().to(args.device)\n",
    "    # model = model.merge_and_unload()\n",
    "model = model.eval()\n",
    "\n",
    "train_embeddings = inference(train_df, model, tokenizer, 'cuda')\n",
    "\n",
    "misconception_mapping['query_text'] = misconception_mapping['MisconceptionName']\n",
    "misconception_mapping['order_index'] = misconception_mapping['MisconceptionId']\n",
    "doc_embeddings = inference(misconception_mapping, model, tokenizer, 'cuda')\n",
    "\n",
    "sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "index_text_embeddings_index = {index: paper_id for index, paper_id in\n",
    "                                         enumerate(list(doc_embeddings.keys()))}\n",
    "\n",
    "\n",
    "cosine_similarities = []\n",
    "for _, row in tqdm(train_df.iterrows()):\n",
    "    query_id = row['order_index']\n",
    "    query_em = train_embeddings[query_id].reshape(1, -1)\n",
    "    \n",
    "    cosine_similarity = np.dot(query_em, sentence_embeddings.T).flatten()\n",
    "    \n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "cosine_similarities = np.vstack(cosine_similarities)\n",
    "# sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "# cosine_similarities = np.dot(train_embeddings.values(), sentence_embeddings.T)\n",
    "np.save(f\"cosine_similarities_model_{fold}.npy\", cosine_similarities)\n",
    "np.save(f\"sentence_embeddings_{fold}.npy\", sentence_embeddings)\n",
    "train_embeddings = {key: train_embeddings[key] for key in sorted(train_embeddings)}\n",
    "train_embeddings = np.concatenate([e.reshape(1, -1) for e in list(train_embeddings.values())])\n",
    "np.save(f\"train_embeddings_{fold}.npy\", train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14a6e5f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.397501Z",
     "iopub.status.busy": "2024-12-12T11:26:46.397231Z",
     "iopub.status.idle": "2024-12-12T11:26:46.400471Z",
     "shell.execute_reply": "2024-12-12T11:26:46.399701Z"
    },
    "papermill": {
     "duration": 0.015724,
     "end_time": "2024-12-12T11:26:46.402041",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.386317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python fold3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d31b093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.423914Z",
     "iopub.status.busy": "2024-12-12T11:26:46.423670Z",
     "iopub.status.idle": "2024-12-12T11:26:46.430924Z",
     "shell.execute_reply": "2024-12-12T11:26:46.430196Z"
    },
    "papermill": {
     "duration": 0.02026,
     "end_time": "2024-12-12T11:26:46.432571",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.412311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fold4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fold4.py\n",
    "from tqdm.auto import tqdm\n",
    "# from bs4 import BeautifulSoup\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "from peft import PeftModel\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "\n",
    "fold = 4\n",
    "train_df = pd.read_parquet(\"tmp.parquet\")\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 2\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "    pids = list(df['order_index'].values)\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    sentence_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    result = {int(pids[i]): em for i, em in enumerate(sentence_embeddings)}\n",
    "    return result\n",
    "\n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "model_path = \"/kaggle/input/qwen-instruct-14b\"\n",
    "# lora_path=\"/kaggle/input/aa1204-2132/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd001_6ra/epoch_4_model\" #526\n",
    "lora_path = \"/kaggle/input/zsq-other-fourfold-529/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd000_6ra_fold4/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd000_6ra_fold4/epoch_4_model\"\n",
    "# lora_path = \"/kaggle/input/zsq-other-fourfold-529-wd001/rhp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd001_6ra_fold4/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd001_6ra_fold4/epoch_4_model\"\n",
    "# device='cuda'\n",
    "VALID = False\n",
    "\n",
    "misconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#         )\n",
    "model = AutoModel.from_pretrained(model_path, \n",
    "                                  torch_dtype=torch.float16,\n",
    "                                  # quantization_config=bnb_config,\n",
    "                                  device_map='auto',\n",
    "                                 )\n",
    "if lora_path!=\"none\":\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, lora_path)#.eval().to(args.device)\n",
    "    # model = model.merge_and_unload()\n",
    "model = model.eval()\n",
    "\n",
    "train_embeddings = inference(train_df, model, tokenizer, 'cuda')\n",
    "\n",
    "misconception_mapping['query_text'] = misconception_mapping['MisconceptionName']\n",
    "misconception_mapping['order_index'] = misconception_mapping['MisconceptionId']\n",
    "doc_embeddings = inference(misconception_mapping, model, tokenizer, 'cuda')\n",
    "\n",
    "sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "index_text_embeddings_index = {index: paper_id for index, paper_id in\n",
    "                                         enumerate(list(doc_embeddings.keys()))}\n",
    "\n",
    "\n",
    "cosine_similarities = []\n",
    "for _, row in tqdm(train_df.iterrows()):\n",
    "    query_id = row['order_index']\n",
    "    query_em = train_embeddings[query_id].reshape(1, -1)\n",
    "    \n",
    "    cosine_similarity = np.dot(query_em, sentence_embeddings.T).flatten()\n",
    "    \n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "cosine_similarities = np.vstack(cosine_similarities)\n",
    "# sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "# cosine_similarities = np.dot(train_embeddings.values(), sentence_embeddings.T)\n",
    "np.save(f\"cosine_similarities_model_{fold}.npy\", cosine_similarities)\n",
    "np.save(f\"sentence_embeddings_{fold}.npy\", sentence_embeddings)\n",
    "train_embeddings = {key: train_embeddings[key] for key in sorted(train_embeddings)}\n",
    "train_embeddings = np.concatenate([e.reshape(1, -1) for e in list(train_embeddings.values())])\n",
    "np.save(f\"train_embeddings_{fold}.npy\", train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4961382",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.457036Z",
     "iopub.status.busy": "2024-12-12T11:26:46.456729Z",
     "iopub.status.idle": "2024-12-12T11:26:46.460282Z",
     "shell.execute_reply": "2024-12-12T11:26:46.459431Z"
    },
    "papermill": {
     "duration": 0.01746,
     "end_time": "2024-12-12T11:26:46.461862",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.444402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python fold4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5cc32a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.485625Z",
     "iopub.status.busy": "2024-12-12T11:26:46.485341Z",
     "iopub.status.idle": "2024-12-12T11:26:46.492896Z",
     "shell.execute_reply": "2024-12-12T11:26:46.492109Z"
    },
    "papermill": {
     "duration": 0.021887,
     "end_time": "2024-12-12T11:26:46.494678",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.472791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fold5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fold5.py\n",
    "from tqdm.auto import tqdm\n",
    "# from bs4 import BeautifulSoup\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "from peft import PeftModel\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "\n",
    "fold = 5\n",
    "train_df = pd.read_parquet(\"tmp.parquet\")\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 2\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "    pids = list(df['order_index'].values)\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    sentence_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    result = {int(pids[i]): em for i, em in enumerate(sentence_embeddings)}\n",
    "    return result\n",
    "\n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "model_path = \"/kaggle/input/qwen-instruct-14b\"\n",
    "# lora_path=\"/kaggle/input/v36-filterv3-rationale/epoch_19_model\" #503\n",
    "lora_path = \"/kaggle/input/zsq-other-fourfold-529/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd000_6ra_fold5/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd000_6ra_fold5/epoch_4_model\"\n",
    "# lora_path = \"/kaggle/input/zsq-other-fourfold-529-wd001/rhp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd001_6ra_fold5/hp_code_64_128_epoch5_tmp002_lr1e4_gd1_wd001_6ra_fold5/epoch_4_model\"\n",
    "# device='cuda'\n",
    "VALID = False\n",
    "\n",
    "misconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#         )\n",
    "model = AutoModel.from_pretrained(model_path, \n",
    "                                  torch_dtype=torch.float16,\n",
    "                                  # quantization_config=bnb_config,\n",
    "                                  device_map='auto',\n",
    "                                 )\n",
    "if lora_path!=\"none\":\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, lora_path)#.eval().to(args.device)\n",
    "    # model = model.merge_and_unload()\n",
    "model = model.eval()\n",
    "\n",
    "train_embeddings = inference(train_df, model, tokenizer, 'cuda')\n",
    "\n",
    "misconception_mapping['query_text'] = misconception_mapping['MisconceptionName']\n",
    "misconception_mapping['order_index'] = misconception_mapping['MisconceptionId']\n",
    "doc_embeddings = inference(misconception_mapping, model, tokenizer, 'cuda')\n",
    "\n",
    "sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "index_text_embeddings_index = {index: paper_id for index, paper_id in\n",
    "                                         enumerate(list(doc_embeddings.keys()))}\n",
    "\n",
    "\n",
    "cosine_similarities = []\n",
    "for _, row in tqdm(train_df.iterrows()):\n",
    "    query_id = row['order_index']\n",
    "    query_em = train_embeddings[query_id].reshape(1, -1)\n",
    "    \n",
    "    cosine_similarity = np.dot(query_em, sentence_embeddings.T).flatten()\n",
    "    \n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "cosine_similarities = np.vstack(cosine_similarities)\n",
    "# sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "# cosine_similarities = np.dot(train_embeddings.values(), sentence_embeddings.T)\n",
    "np.save(f\"cosine_similarities_model_{fold}.npy\", cosine_similarities)\n",
    "np.save(f\"sentence_embeddings_{fold}.npy\", sentence_embeddings)\n",
    "train_embeddings = {key: train_embeddings[key] for key in sorted(train_embeddings)}\n",
    "train_embeddings = np.concatenate([e.reshape(1, -1) for e in list(train_embeddings.values())])\n",
    "np.save(f\"train_embeddings_{fold}.npy\", train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f65019b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.516469Z",
     "iopub.status.busy": "2024-12-12T11:26:46.516240Z",
     "iopub.status.idle": "2024-12-12T11:26:46.519547Z",
     "shell.execute_reply": "2024-12-12T11:26:46.518888Z"
    },
    "papermill": {
     "duration": 0.016235,
     "end_time": "2024-12-12T11:26:46.521404",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.505169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python fold5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6607e762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.543413Z",
     "iopub.status.busy": "2024-12-12T11:26:46.543123Z",
     "iopub.status.idle": "2024-12-12T11:26:46.549925Z",
     "shell.execute_reply": "2024-12-12T11:26:46.549130Z"
    },
    "papermill": {
     "duration": 0.019664,
     "end_time": "2024-12-12T11:26:46.551435",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.531771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fold6.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fold6.py\n",
    "from tqdm.auto import tqdm\n",
    "# from bs4 import BeautifulSoup\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "from peft import PeftModel\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "\n",
    "fold = 6\n",
    "train_df = pd.read_parquet(\"tmp.parquet\")\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 2\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "    pids = list(df['order_index'].values)\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    sentence_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    result = {int(pids[i]): em for i, em in enumerate(sentence_embeddings)}\n",
    "    return result\n",
    "\n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "model_path = \"/kaggle/input/qwen-instruct-14b\"\n",
    "# lora_path=\"/kaggle/input/v36-filterv3-rationale/epoch_19_model\" #503\n",
    "lora_path = \"/kaggle/input/hp-all-lora-ensemble/hp_all_lora_ensemble\"\n",
    "# lora_path = \"/kaggle/input/v8v13v14-19/v8v13v14_19/lora_merge.bin\"\n",
    "# device='cuda'\n",
    "VALID = False\n",
    "\n",
    "misconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#         )\n",
    "model = AutoModel.from_pretrained(model_path, \n",
    "                                  torch_dtype=torch.float16,\n",
    "                                  # quantization_config=bnb_config,\n",
    "                                  device_map='auto',\n",
    "                                 )\n",
    "if lora_path!=\"none\":\n",
    "\n",
    "    model = PeftModel.from_pretrained(model, lora_path)#.eval().to(args.device)\n",
    "    # model = model.merge_and_unload()\n",
    "model = model.eval()\n",
    "\n",
    "train_embeddings = inference(train_df, model, tokenizer, 'cuda')\n",
    "\n",
    "misconception_mapping['query_text'] = misconception_mapping['MisconceptionName']\n",
    "misconception_mapping['order_index'] = misconception_mapping['MisconceptionId']\n",
    "doc_embeddings = inference(misconception_mapping, model, tokenizer, 'cuda')\n",
    "\n",
    "sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "index_text_embeddings_index = {index: paper_id for index, paper_id in\n",
    "                                         enumerate(list(doc_embeddings.keys()))}\n",
    "\n",
    "\n",
    "cosine_similarities = []\n",
    "for _, row in tqdm(train_df.iterrows()):\n",
    "    query_id = row['order_index']\n",
    "    query_em = train_embeddings[query_id].reshape(1, -1)\n",
    "    \n",
    "    cosine_similarity = np.dot(query_em, sentence_embeddings.T).flatten()\n",
    "    \n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "cosine_similarities = np.vstack(cosine_similarities)\n",
    "# sentence_embeddings = np.concatenate([e.reshape(1, -1) for e in list(doc_embeddings.values())])\n",
    "# cosine_similarities = np.dot(train_embeddings.values(), sentence_embeddings.T)\n",
    "np.save(f\"cosine_similarities_model_{fold}.npy\", cosine_similarities)\n",
    "np.save(f\"sentence_embeddings_{fold}.npy\", sentence_embeddings)\n",
    "train_embeddings = {key: train_embeddings[key] for key in sorted(train_embeddings)}\n",
    "train_embeddings = np.concatenate([e.reshape(1, -1) for e in list(train_embeddings.values())])\n",
    "np.save(f\"train_embeddings_{fold}.npy\", train_embeddings)\n",
    "\n",
    "with open(\"index_text_embeddings_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(index_text_embeddings_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16b81408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:26:46.573217Z",
     "iopub.status.busy": "2024-12-12T11:26:46.572926Z",
     "iopub.status.idle": "2024-12-12T11:34:00.722141Z",
     "shell.execute_reply": "2024-12-12T11:34:00.720905Z"
    },
    "papermill": {
     "duration": 434.162646,
     "end_time": "2024-12-12T11:34:00.724455",
     "exception": false,
     "start_time": "2024-12-12T11:26:46.561809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [03:07<00:00, 23.39s/it]\r\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.37it/s]\r\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1294/1294 [03:39<00:00,  5.89it/s]\r\n",
      "9it [00:00, 223.68it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python fold6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9e100f",
   "metadata": {
    "papermill": {
     "duration": 0.065015,
     "end_time": "2024-12-12T11:34:00.855080",
     "exception": false,
     "start_time": "2024-12-12T11:34:00.790065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8850335",
   "metadata": {
    "papermill": {
     "duration": 0.064418,
     "end_time": "2024-12-12T11:34:00.985038",
     "exception": false,
     "start_time": "2024-12-12T11:34:00.920620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83094e96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:34:01.117764Z",
     "iopub.status.busy": "2024-12-12T11:34:01.117416Z",
     "iopub.status.idle": "2024-12-12T11:34:01.121832Z",
     "shell.execute_reply": "2024-12-12T11:34:01.121040Z"
    },
    "papermill": {
     "duration": 0.072546,
     "end_time": "2024-12-12T11:34:01.123344",
     "exception": false,
     "start_time": "2024-12-12T11:34:01.050798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #result 1\n",
    "# with open(\"index_text_embeddings_index.pkl\", \"rb\") as f:\n",
    "#     index_text_embeddings_index = pickle.load(f)\n",
    "\n",
    "# all_cosine_similarities = []\n",
    "# for i in range(1,6):\n",
    "#     cosine_similarities = np.load(f\"cosine_similarities_model_{i}.npy\")\n",
    "#     all_cosine_similarities.append(cosine_similarities)\n",
    "\n",
    "# # èžåˆç›¸ä¼¼åº¦\n",
    "# combined_cosine_similarities = np.mean(all_cosine_similarities, axis=0)\n",
    "\n",
    "# predicts_test = []\n",
    "# for row in tqdm(combined_cosine_similarities):\n",
    "    \n",
    "#     sort_index = np.argsort(-row)[:25]\n",
    "#     pids = [index_text_embeddings_index[index] for index in sort_index]\n",
    "#     predicts_test.append(pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3473d5bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:34:01.253743Z",
     "iopub.status.busy": "2024-12-12T11:34:01.253501Z",
     "iopub.status.idle": "2024-12-12T11:34:01.525895Z",
     "shell.execute_reply": "2024-12-12T11:34:01.523767Z"
    },
    "papermill": {
     "duration": 0.347323,
     "end_time": "2024-12-12T11:34:01.534765",
     "exception": false,
     "start_time": "2024-12-12T11:34:01.187442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c1c89b842b41198c6d47899d402f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#result 2\n",
    "with open(\"index_text_embeddings_index.pkl\", \"rb\") as f:\n",
    "    index_text_embeddings_index = pickle.load(f)\n",
    "\n",
    "sentence_embeddings = []\n",
    "train_embeddings = []\n",
    "\n",
    "for i in range(6,7):\n",
    "    sentence_embedding = np.load(f\"sentence_embeddings_{i}.npy\")\n",
    "    train_embedding = np.load(f\"train_embeddings_{i}.npy\")\n",
    "    \n",
    "    sentence_embeddings.append(sentence_embedding)\n",
    "    train_embeddings.append(train_embedding)\n",
    "\n",
    "sentence_embeddings = np.mean(sentence_embeddings, axis=0)\n",
    "train_embeddings = np.mean(train_embeddings, axis=0)\n",
    "\n",
    "combined_cosine_similarities = np.dot(train_embeddings,sentence_embeddings.T)\n",
    "predicts_test_2 = []\n",
    "for row in tqdm(combined_cosine_similarities):\n",
    "    \n",
    "    sort_index = np.argsort(-row)[:25]\n",
    "    pids = [index_text_embeddings_index[index] for index in sort_index]\n",
    "    predicts_test_2.append(pids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e94bf69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:34:01.735836Z",
     "iopub.status.busy": "2024-12-12T11:34:01.735069Z",
     "iopub.status.idle": "2024-12-12T11:34:01.739085Z",
     "shell.execute_reply": "2024-12-12T11:34:01.738284Z"
    },
    "papermill": {
     "duration": 0.071363,
     "end_time": "2024-12-12T11:34:01.740552",
     "exception": false,
     "start_time": "2024-12-12T11:34:01.669189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # rank èžåˆ\n",
    "\n",
    "# rank_predict = []\n",
    "# alpha = 1\n",
    "\n",
    "# for p in zip(predicts_test,predicts_test_2):\n",
    "#     score = {}\n",
    "#     for x in p:\n",
    "#         for i,k in enumerate(x):\n",
    "#             if k not in score:\n",
    "#                 score[k] = 1/(i+alpha)\n",
    "#             else:\n",
    "#                 score[k] += 1/(i+alpha)\n",
    "#     print(p[0])\n",
    "#     print(p[1])\n",
    "#     score = sorted(score, key=score.get, reverse=True)\n",
    "#     rank_predict.append(score)\n",
    "#     print(score)\n",
    "#     print(\"====\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26e07cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:34:01.870649Z",
     "iopub.status.busy": "2024-12-12T11:34:01.870413Z",
     "iopub.status.idle": "2024-12-12T11:34:01.882368Z",
     "shell.execute_reply": "2024-12-12T11:34:01.881734Z"
    },
    "papermill": {
     "duration": 0.078931,
     "end_time": "2024-12-12T11:34:01.883866",
     "exception": false,
     "start_time": "2024-12-12T11:34:01.804935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VALID = False\n",
    "\n",
    "if VALID:\n",
    "    train_df['recall_ids'] = predicts_test_2\n",
    "    print(mapk([[data] for data in train_df['answer_id'].values],train_df['recall_ids'].values))\n",
    "else:\n",
    "    train_df['MisconceptionId'] = [' '.join(map(str,c)) for c in predicts_test_2]\n",
    "    # sub = []\n",
    "    # for _,row in train_df.iterrows():\n",
    "    #     sub.append(\n",
    "    #         {\n",
    "    #             \"QuestionId_Answer\":f\"{row['QuestionId']}_{row['answer_name']}\",\n",
    "    #             \"MisconceptionId\":row['MisconceptionId']\n",
    "    #         }\n",
    "    #     )\n",
    "    # submission_df = pd.DataFrame(sub)\n",
    "    # submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    # print(\"Submission file created successfully!\")\n",
    "    train_df.to_parquet(\"recall.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8f49ea3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:34:02.026021Z",
     "iopub.status.busy": "2024-12-12T11:34:02.025316Z",
     "iopub.status.idle": "2024-12-12T11:34:02.921885Z",
     "shell.execute_reply": "2024-12-12T11:34:02.921086Z"
    },
    "papermill": {
     "duration": 0.973567,
     "end_time": "2024-12-12T11:34:02.923707",
     "exception": false,
     "start_time": "2024-12-12T11:34:01.950140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef532514",
   "metadata": {
    "papermill": {
     "duration": 0.070115,
     "end_time": "2024-12-12T11:34:03.065532",
     "exception": false,
     "start_time": "2024-12-12T11:34:02.995417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5265e866",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:34:03.197778Z",
     "iopub.status.busy": "2024-12-12T11:34:03.197493Z",
     "iopub.status.idle": "2024-12-12T11:34:03.204469Z",
     "shell.execute_reply": "2024-12-12T11:34:03.203773Z"
    },
    "papermill": {
     "duration": 0.075063,
     "end_time": "2024-12-12T11:34:03.206008",
     "exception": false,
     "start_time": "2024-12-12T11:34:03.130945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # rank æŽ¨ç†\n",
    "# from tqdm.auto import tqdm\n",
    "# # from bs4 import BeautifulSoup\n",
    "# import gc\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "# import sys\n",
    "# import numpy as np\n",
    "# from tqdm.autonotebook import trange\n",
    "# from sklearn.model_selection import GroupKFold\n",
    "# import json\n",
    "# import torch\n",
    "# from numpy.linalg import norm\n",
    "# import torch.nn.functional as F\n",
    "# from torch import Tensor\n",
    "# from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "# from peft import (\n",
    "#     LoraConfig,\n",
    "#     get_peft_model,\n",
    "# )\n",
    "\n",
    "# from peft import PeftModel\n",
    "# import json\n",
    "# import copy\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# import os\n",
    "# from transformers import AutoModelForCausalLM,AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# def batch_to_device(batch, target_device):\n",
    "#     \"\"\"\n",
    "#     send a pytorch batch to a device (CPU/GPU)\n",
    "#     \"\"\"\n",
    "#     for key in batch:\n",
    "#         if isinstance(batch[key], Tensor):\n",
    "#             batch[key] = batch[key].to(target_device)\n",
    "#     return batch\n",
    "    \n",
    "# def create_one_example(tokenier,query, doc):\n",
    "#     query_max_len = 512\n",
    "    \n",
    "#     qry_inputs = tokenizer.encode(query, truncation=True, max_length = query_max_len, add_special_tokens=False)\n",
    "#     doc_inputs = tokenizer.encode(doc, add_special_tokens=False)\n",
    "#     item = tokenizer.prepare_for_model(\n",
    "#         qry_inputs,\n",
    "#         doc_inputs\n",
    "#     )\n",
    "#     # print(item)\n",
    "#     # print(tokenizer.decode(item['input_ids']))\n",
    "#     return item\n",
    "\n",
    "# def inference(df,model,tokenizer,prompt,device):\n",
    "    \n",
    "    \n",
    "#     #yes_loc = tokenizer('Yes', add_special_tokens=False)['input_ids'][-1]\n",
    "#     batch_size = 2\n",
    "\n",
    "#     pids = list(range(len(df)))\n",
    "#     query = [prompt + q + '\\n### Explanlation:' for q in df['query'].values]\n",
    "#     doc = list(df['MisconceptionName'])\n",
    "#     sentences = [create_one_example(tokenizer,q,d) for q,d in zip(query,doc)]\n",
    "    \n",
    "#     all_scores = []\n",
    "#     length_sorted_idx = np.argsort([-len(sen['input_ids']) for sen in sentences])\n",
    "#     sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "#     for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "#         sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "#         features = tokenizer.pad(sentences_batch,\n",
    "#                                 padding=True,\n",
    "#                                 max_length= 576, # 512+64\n",
    "#                                 return_tensors='pt')\n",
    "        \n",
    "#         print(features['input_ids'].shape)\n",
    "#         features = batch_to_device(features, device)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**features)\n",
    "#             scores = outputs.logits[:, -1]\n",
    "#             scores = scores.detach().cpu().numpy().tolist()\n",
    "#         all_scores.extend(scores)   \n",
    "        \n",
    "#     all_scores = [all_scores[idx] for idx in np.argsort(length_sorted_idx)]\n",
    "#     df['score'] = all_scores\n",
    "#     return df\n",
    "\n",
    "\n",
    "# path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "# model_path = \"/kaggle/input/qwen-instruct-14b\"\n",
    "# lora_path=\"/kaggle/input/rank-all-lora-ensemble-v2/rank-all-lora-ensemble-v2\"\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# tokenizer.padding_side = 'left'\n",
    "# # bnb_config = BitsAndBytesConfig(\n",
    "# #             load_in_4bit=True,\n",
    "# #             bnb_4bit_use_double_quant=True,\n",
    "# #             bnb_4bit_quant_type=\"nf4\",\n",
    "# #             bnb_4bit_compute_dtype=torch.float16\n",
    "# #         )\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_path, \n",
    "#                                   #quantization_config=bnb_config,\n",
    "#                                   torch_dtype=torch.float16,\n",
    "#                                   device_map='auto',\n",
    "#                                   num_labels = 1\n",
    "#                                  )\n",
    "# if lora_path!=\"none\":\n",
    "#     #model = get_peft_model(model, config)\n",
    "#     #d = torch.load(lora_path, map_location=model.device)\n",
    "#     #model.load_state_dict(d, strict=False)\n",
    "#     model = PeftModel.from_pretrained(model, lora_path)#.eval().to(args.device)\n",
    "#     model = model.merge_and_unload()\n",
    "    \n",
    "# model = model.eval()\n",
    "\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prompt = \"Given a query containing a math problem and an incorrect answer, along with an explanation of the mistake made in the answer, determine whether the explanation accurately summarizes the error in the answer by providing a prediction of either 'Yes' or 'No'. \\n Query:\"\n",
    "# def get_query(row):\n",
    "#     real_answer_id = row['CorrectAnswer']\n",
    "#     real_text = row[f'Answer{real_answer_id}Text']\n",
    "#     c = row['answer_name']\n",
    "#     query_text = f\"### SubjectName: {row['SubjectName']}\\n### ConstructName: {row['ConstructName']}\\n### Question: {row['QuestionText']}\\n### Correct Answer: {real_text}\\n### Misconcepte Incorrect answer: {row[f'Answer{c}Text']}\"\n",
    "#     return query_text\n",
    "    \n",
    "# train_df = pd.read_parquet('recall.parquet')\n",
    "# misconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\n",
    "\n",
    "# train_df['order_index'] = list(range(len(train_df)))\n",
    "# train_df['query'] = train_df.apply(get_query,axis = 1)\n",
    "# # add\n",
    "# train_df['query'] = train_df['query'] + \"\\n### Rationale: \" + train_df['rationale']\n",
    "\n",
    "# train_df['MisconceptionId'] = train_df['MisconceptionId'].apply(lambda x:[int(i) for i in x.split(' ')])\n",
    "\n",
    "\n",
    "# ## åªå–å‰15\n",
    "# train_df['MisconceptionId_15_other'] = train_df['MisconceptionId'].apply(lambda x: x[15:])\n",
    "# train_df['MisconceptionId'] = train_df['MisconceptionId'].apply(lambda x: x[:15])\n",
    "\n",
    "# train_explode = train_df.explode('MisconceptionId')\n",
    "# train_explode = pd.merge(train_explode,misconception_mapping,on = 'MisconceptionId',how = 'left')\n",
    "\n",
    "# train_explode = inference(train_explode,model,tokenizer,prompt,'cuda')\n",
    "\n",
    "# train_explode = train_explode.groupby('order_index',group_keys=False).apply(lambda x: x.sort_values('score',ascending=False))\n",
    "# tmp_res = train_explode.groupby('order_index').agg({'MisconceptionId': list}).reset_index()\n",
    "# train_df = pd.merge(train_df.drop('MisconceptionId',axis = 1),tmp_res,on = 'order_index',how = 'left')\n",
    "\n",
    "\n",
    "# ## åˆå¹¶\n",
    "# train_df['MisconceptionId'] = train_df.apply(lambda row: row['MisconceptionId']+row['MisconceptionId_15_other'], axis=1)\n",
    "\n",
    "# train_df['MisconceptionId'] = train_df['MisconceptionId'].apply(lambda x:' '.join([str(i) for i in x]))\n",
    "# train_df[\"QuestionId_Answer\"] = train_df.apply(lambda x:f'{x[\"QuestionId\"]}_{x[\"answer_name\"]}',axis = 1)\n",
    "# train_df[['QuestionId_Answer','MisconceptionId']].to_csv(\"submission.csv\", index=False)\n",
    "# print(\"Submission file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0638fdfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:34:03.340574Z",
     "iopub.status.busy": "2024-12-12T11:34:03.339611Z",
     "iopub.status.idle": "2024-12-12T11:34:03.343512Z",
     "shell.execute_reply": "2024-12-12T11:34:03.342836Z"
    },
    "papermill": {
     "duration": 0.073227,
     "end_time": "2024-12-12T11:34:03.345070",
     "exception": false,
     "start_time": "2024-12-12T11:34:03.271843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f753a",
   "metadata": {
    "papermill": {
     "duration": 0.064862,
     "end_time": "2024-12-12T11:34:03.475446",
     "exception": false,
     "start_time": "2024-12-12T11:34:03.410584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 32b vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5271b06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:34:03.606615Z",
     "iopub.status.busy": "2024-12-12T11:34:03.606356Z",
     "iopub.status.idle": "2024-12-12T11:34:03.614302Z",
     "shell.execute_reply": "2024-12-12T11:34:03.613299Z"
    },
    "papermill": {
     "duration": 0.075635,
     "end_time": "2024-12-12T11:34:03.615917",
     "exception": false,
     "start_time": "2024-12-12T11:34:03.540282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "# rank æŽ¨ç†\n",
    "from tqdm.auto import tqdm\n",
    "# from bs4 import BeautifulSoup\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "from peft import PeftModel\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM,AutoModelForSequenceClassification\n",
    "\n",
    "import vllm\n",
    "from typing import Any, Dict, List\n",
    "from transformers import LogitsProcessor\n",
    "import torch\n",
    "from time import time\n",
    "#os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"ray\"\n",
    "#os.environ['']\n",
    "\n",
    "\n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "model_path = '/kaggle/input/32b-new-last-66-58-awq1212/32b-new-last-66-58-AWQâ€”1212'\n",
    "\n",
    "prompt = \"Given a query containing a math problem and an incorrect answer, along with an explanation of the mistake made in the answer, determine whether the explanation accurately summarizes the error in the answer by providing a prediction of either 'Yes' or 'No'. \\n Query:\"\n",
    "\n",
    "\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.90, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True,\n",
    "    distributed_executor_backend=\"ray\"\n",
    ")\n",
    "\n",
    "def vllm_infer(df,model):\n",
    "\n",
    "    pids = list(range(len(df)))\n",
    "    query = [prompt + q + '\\n### Explanlation:' for q in df['query'].values]\n",
    "    doc = list(df['MisconceptionName'])\n",
    "    sentences = [q+d+'\\n' for q,d in zip(query,doc)]\n",
    "    #print(sentences[0])\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    \n",
    "    choices = [\"Yes\"]\n",
    "\n",
    "    KEEP = []\n",
    "    for x in choices:\n",
    "        c = tokenizer.encode(x,add_special_tokens=False)[0]\n",
    "        KEEP.append(c)\n",
    "\n",
    "    class DigitLogitsProcessor(LogitsProcessor):\n",
    "        def __init__(self, tokenizer):\n",
    "            self.allowed_ids = KEEP\n",
    "\n",
    "        def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "            scores[self.allowed_ids] += 1\n",
    "            return scores\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "    responses = llm.generate(\n",
    "        sentences,\n",
    "        vllm.SamplingParams(\n",
    "            n=1,  # Number of output sequences to return for each prompt.\n",
    "            top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "            temperature=0,  # randomness of the sampling\n",
    "            seed=777, # Seed for reprodicibility\n",
    "            skip_special_tokens=True,  # Whether to skip special tokens in the output.\n",
    "            max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "            logits_processors=logits_processors,\n",
    "            logprobs = 5\n",
    "        ),\n",
    "        use_tqdm = True\n",
    "    )\n",
    "\n",
    "    end = time()\n",
    "    elapsed = (end-start)/60. #minutes\n",
    "    print(f\"Inference took {elapsed} minutes!\")\n",
    "    \n",
    "    results = []\n",
    "    errors = 0\n",
    "    score = []\n",
    "    for i,response in enumerate(responses):\n",
    "        try:\n",
    "            x = response.outputs[0].logprobs[0]\n",
    "            logprobs = []\n",
    "            for k in KEEP:\n",
    "                if k in x:\n",
    "                    score.append(x[k].logprob)\n",
    "                else:\n",
    "                    score.append(-100)\n",
    "                    #print(f\"bad logits {i}\")\n",
    "        except:\n",
    "            #print(f\"error {i}\")\n",
    "            results.append(-100)\n",
    "            errors += 1\n",
    "\n",
    "    print(f\"There were {errors} inference errors out of {i+1} inferences\")\n",
    "    df['score'] = score\n",
    "    return df\n",
    "\n",
    "def get_query(row):\n",
    "    real_answer_id = row['CorrectAnswer']\n",
    "    real_text = row[f'Answer{real_answer_id}Text']\n",
    "    c = row['answer_name']\n",
    "    query_text = f\"### SubjectName: {row['SubjectName']}\\n### ConstructName: {row['ConstructName']}\\n### Question: {row['QuestionText']}\\n### Correct Answer: {real_text}\\n### Misconcepte Incorrect answer: {row[f'Answer{c}Text']}\"\n",
    "    return query_text\n",
    "    \n",
    "train_df = pd.read_parquet('recall.parquet')\n",
    "misconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\n",
    "\n",
    "train_df['order_index'] = list(range(len(train_df)))\n",
    "train_df['query'] = train_df.apply(get_query,axis = 1)\n",
    "# add\n",
    "train_df['query'] = train_df['query'] + \"\\n### Rationale: \" + train_df['rationale']\n",
    "\n",
    "train_df['MisconceptionId'] = train_df['MisconceptionId'].apply(lambda x:[int(i) for i in x.split(' ')])\n",
    "\n",
    "\n",
    "## åªå–å‰15\n",
    "train_df['MisconceptionId_15_other'] = train_df['MisconceptionId'].apply(lambda x: x[20:])\n",
    "train_df['MisconceptionId'] = train_df['MisconceptionId'].apply(lambda x: x[:20])\n",
    "\n",
    "train_explode = train_df.explode('MisconceptionId')\n",
    "train_explode = pd.merge(train_explode,misconception_mapping,on = 'MisconceptionId',how = 'left')\n",
    "\n",
    "train_explode = vllm_infer(train_explode,llm)\n",
    "\n",
    "train_explode = train_explode.groupby('order_index',group_keys=False).apply(lambda x: x.sort_values('score',ascending=False))\n",
    "tmp_res = train_explode.groupby('order_index').agg({'MisconceptionId': list}).reset_index()\n",
    "train_df = pd.merge(train_df.drop('MisconceptionId',axis = 1),tmp_res,on = 'order_index',how = 'left')\n",
    "\n",
    "\n",
    "## åˆå¹¶\n",
    "train_df['MisconceptionId'] = train_df.apply(lambda row: row['MisconceptionId']+row['MisconceptionId_15_other'], axis=1)\n",
    "\n",
    "train_df['MisconceptionId'] = train_df['MisconceptionId'].apply(lambda x:' '.join([str(i) for i in x]))\n",
    "train_df[\"QuestionId_Answer\"] = train_df.apply(lambda x:f'{x[\"QuestionId\"]}_{x[\"answer_name\"]}',axis = 1)\n",
    "train_df[['QuestionId_Answer','MisconceptionId']].to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8deba5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:34:03.746465Z",
     "iopub.status.busy": "2024-12-12T11:34:03.746201Z",
     "iopub.status.idle": "2024-12-12T11:37:34.199750Z",
     "shell.execute_reply": "2024-12-12T11:37:34.198806Z"
    },
    "papermill": {
     "duration": 210.521245,
     "end_time": "2024-12-12T11:37:34.201927",
     "exception": false,
     "start_time": "2024-12-12T11:34:03.680682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-12 11:34:14 config.py:246] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "2024-12-12 11:34:17,147\tINFO worker.py:1749 -- Started a local Ray instance.\r\n",
      "INFO 12-12 11:34:18 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/kaggle/input/32b-new-last-66-58-awq1212/32b-new-last-66-58-AWQâ€”1212', speculative_config=None, tokenizer='/kaggle/input/32b-new-last-66-58-awq1212/32b-new-last-66-58-AWQâ€”1212', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/kaggle/input/32b-new-last-66-58-awq1212/32b-new-last-66-58-AWQâ€”1212, use_v2_block_manager=False, enable_prefix_caching=False)\r\n",
      "INFO 12-12 11:34:25 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 11:34:25 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[36m(RayWorkerWrapper pid=577)\u001b[0m INFO 12-12 11:34:25 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[36m(RayWorkerWrapper pid=577)\u001b[0m INFO 12-12 11:34:25 selector.py:54] Using XFormers backend.\r\n",
      "INFO 12-12 11:34:27 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-12 11:34:27 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "\u001b[36m(RayWorkerWrapper pid=577)\u001b[0m INFO 12-12 11:34:27 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[36m(RayWorkerWrapper pid=577)\u001b[0m INFO 12-12 11:34:27 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-12 11:34:28 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[36m(RayWorkerWrapper pid=577)\u001b[0m INFO 12-12 11:34:28 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-12 11:34:28 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7890104b2f50>, local_subscribe_port=56083, local_sync_port=47605, remote_subscribe_port=None, remote_sync_port=None)\r\n",
      "INFO 12-12 11:34:28 model_runner.py:680] Starting to load model /kaggle/input/32b-new-last-66-58-awq1212/32b-new-last-66-58-AWQâ€”1212...\r\n",
      "\u001b[36m(RayWorkerWrapper pid=577)\u001b[0m INFO 12-12 11:34:28 model_runner.py:680] Starting to load model /kaggle/input/32b-new-last-66-58-awq1212/32b-new-last-66-58-AWQâ€”1212...\r\n",
      "INFO 12-12 11:34:28 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 11:34:28 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[36m(RayWorkerWrapper pid=577)\u001b[0m INFO 12-12 11:34:28 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[36m(RayWorkerWrapper pid=577)\u001b[0m INFO 12-12 11:34:28 selector.py:54] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:22<01:07, 22.34s/it]\r\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:42<00:42, 21.06s/it]\r\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:59<00:19, 19.17s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:19<00:00, 19.66s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:19<00:00, 19.96s/it]\r\n",
      "\r\n",
      "\u001b[36m(RayWorkerWrapper pid=577)\u001b[0m INFO 12-12 11:35:48 model_runner.py:692] Loading model weights took 9.0934 GB\r\n",
      "INFO 12-12 11:35:49 model_runner.py:692] Loading model weights took 9.0934 GB\r\n",
      "INFO 12-12 11:35:59 distributed_gpu_executor.py:56] # GPU blocks: 795, # CPU blocks: 2048\r\n",
      "Processed prompts: 100%|â–ˆ| 180/180 [01:24<00:00,  2.13it/s, est. speed input: 46\r\n",
      "Inference took 1.4090667605400085 minutes!\r\n",
      "There were 0 inference errors out of 180 inferences\r\n",
      "Submission file created successfully!\r\n",
      "\u001b[0m[rank0]:[W CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\r\n"
     ]
    }
   ],
   "source": [
    "!python test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57bdfa53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:37:34.338050Z",
     "iopub.status.busy": "2024-12-12T11:37:34.337438Z",
     "iopub.status.idle": "2024-12-12T11:37:34.351190Z",
     "shell.execute_reply": "2024-12-12T11:37:34.350422Z"
    },
    "papermill": {
     "duration": 0.082622,
     "end_time": "2024-12-12T11:37:34.352808",
     "exception": false,
     "start_time": "2024-12-12T11:37:34.270186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>1345 315 2488 706 2306 1005 1507 2181 1963 251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>1345 2488 2518 315 1507 2306 706 1005 2181 196...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>1507 2532 1672 1005 328 1392 2488 2518 1345 31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>1755 59 2142 2068 167 891 1535 418 1191 1540 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>143 59 891 167 1755 885 2398 1540 363 2078 187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>1755 59 2142 167 2068 891 1535 1191 418 143 68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>1287 1073 2439 1059 2471 365 1665 2551 1306 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>1287 1073 1059 2439 632 365 2551 1665 1521 120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1073 1059 1287 2439 2165 365 2471 2551 2151 39...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  1345 315 2488 706 2306 1005 1507 2181 1963 251...\n",
       "1            1869_C  1345 2488 2518 315 1507 2306 706 1005 2181 196...\n",
       "2            1869_D  1507 2532 1672 1005 328 1392 2488 2518 1345 31...\n",
       "3            1870_A  1755 59 2142 2068 167 891 1535 418 1191 1540 1...\n",
       "4            1870_B  143 59 891 167 1755 885 2398 1540 363 2078 187...\n",
       "5            1870_C  1755 59 2142 167 2068 891 1535 1191 418 143 68...\n",
       "6            1871_A  1287 1073 2439 1059 2471 365 1665 2551 1306 12...\n",
       "7            1871_C  1287 1073 1059 2439 632 365 2551 1665 1521 120...\n",
       "8            1871_D  1073 1059 1287 2439 2165 365 2471 2551 2151 39..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('submission.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d1039",
   "metadata": {
    "papermill": {
     "duration": 0.066468,
     "end_time": "2024-12-12T11:37:34.486300",
     "exception": false,
     "start_time": "2024-12-12T11:37:34.419832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5251603,
     "sourceId": 9094368,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5801830,
     "sourceId": 9527501,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6040767,
     "sourceId": 9846209,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6066007,
     "sourceId": 9879857,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6097690,
     "sourceId": 9921595,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6100002,
     "sourceId": 9924722,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6101531,
     "sourceId": 9926821,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6103506,
     "sourceId": 9929652,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6124323,
     "sourceId": 9957620,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6132224,
     "sourceId": 9968199,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6132253,
     "sourceId": 9968238,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6139612,
     "sourceId": 9978184,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6170518,
     "sourceId": 10020887,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6174898,
     "sourceId": 10026871,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6178603,
     "sourceId": 10031794,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6186864,
     "sourceId": 10043033,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6189834,
     "sourceId": 10047093,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6193351,
     "sourceId": 10051739,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6202038,
     "sourceId": 10063815,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6202585,
     "sourceId": 10064587,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6203095,
     "sourceId": 10065283,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6203515,
     "sourceId": 10065847,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6214763,
     "sourceId": 10080849,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6221468,
     "sourceId": 10089744,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6229089,
     "sourceId": 10099687,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6233656,
     "sourceId": 10105730,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6241842,
     "sourceId": 10116644,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6242091,
     "sourceId": 10117034,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6244684,
     "sourceId": 10120419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6261209,
     "sourceId": 10143833,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6262709,
     "sourceId": 10145834,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6263313,
     "sourceId": 10146624,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6268889,
     "sourceId": 10153935,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6270462,
     "sourceId": 10156011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6279270,
     "sourceId": 10167955,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6280145,
     "sourceId": 10169100,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10171817,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6283772,
     "sourceId": 10173858,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6287181,
     "sourceId": 10178689,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6287862,
     "sourceId": 10179523,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 1902,
     "modelInstanceId": 3899,
     "sourceId": 5111,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1150.048156,
   "end_time": "2024-12-12T11:37:37.552024",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-12T11:18:27.503868",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0d116ea229e4457e85ace313cc87767d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0ffb4ee382654a91b1a814909a86ab31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_899183d7724c4b669972bfe78b209c40",
       "max": 9.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8557b6ece60341ec9b51baa55ade1ccc",
       "value": 9.0
      }
     },
     "26b59cc772c34861b6d09a9fd2810cb9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bb73a04f2b224fac870710541ab1e7f0",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_0d116ea229e4457e85ace313cc87767d",
       "value": "100%"
      }
     },
     "73c1c89b842b41198c6d47899d402f5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_26b59cc772c34861b6d09a9fd2810cb9",
        "IPY_MODEL_0ffb4ee382654a91b1a814909a86ab31",
        "IPY_MODEL_e66f7d94dcdd41d49b9c757849640643"
       ],
       "layout": "IPY_MODEL_ab080e0303cb4aeeaa3ad40add2418f3"
      }
     },
     "8557b6ece60341ec9b51baa55ade1ccc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "899183d7724c4b669972bfe78b209c40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a2341214a6cf4763837e09086289af4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ab080e0303cb4aeeaa3ad40add2418f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bb73a04f2b224fac870710541ab1e7f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d9b50afaa5244424b2cc0e196faa95d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e66f7d94dcdd41d49b9c757849640643": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d9b50afaa5244424b2cc0e196faa95d0",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_a2341214a6cf4763837e09086289af4c",
       "value": "â€‡9/9â€‡[00:00&lt;00:00,â€‡209.68it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
