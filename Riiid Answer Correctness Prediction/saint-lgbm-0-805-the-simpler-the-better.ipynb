{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024007,
     "end_time": "2021-01-08T05:49:16.520999",
     "exception": false,
     "start_time": "2021-01-08T05:49:16.496992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **This is a simple Ensemble of LGBM and SAINT+ like model 0.804 public / 0.806 private**\n",
    "\n",
    "<font size=\"3\">Since this is my first competition, I was clueless on the ensembling techniques that I might use to maximize the improvemement of the model, so I've done a simple SAINT *1.15 + LGBM*0.85 ensemble. Tried stacking, it didn't work well.</font>\n",
    "\n",
    "\n",
    "# **TL;DR** \n",
    "\n",
    "<font size=\"3\">**SAINT**: 128 d_model, 2 att_heads, 100 seq_len no padding, other than timestamp lag, part and user answer, no other feature improved my score. It score 0.803 without LGBM. I will post a notebook that trains and submits the model from scratch.\n",
    "\n",
    "**LGBM** : Basically whole lotta of features that are listed below, 54 feature, LGBM 0.792 , I trained 0.796 LGBM yesterday but I couldn't manage to impelement it before deadline.</font>\n",
    "\n",
    "------------------------\n",
    "\n",
    "My biggest complain in this competition is the obscure submission errors, it really did limit my manoeuvrability, and the ideas I experimented with, however this competition was fun and I learn a lot throughout it. I will post notebooks and discussions explaining everything I have done. Big thanks to the hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021914,
     "end_time": "2021-01-08T05:49:16.565270",
     "exception": false,
     "start_time": "2021-01-08T05:49:16.543356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## This notebook showcases the architecture of my network, the data preparation for LGBM and submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:16.615085Z",
     "iopub.status.busy": "2021-01-08T05:49:16.614467Z",
     "iopub.status.idle": "2021-01-08T05:49:16.645689Z",
     "shell.execute_reply": "2021-01-08T05:49:16.645142Z"
    },
    "papermill": {
     "duration": 0.058541,
     "end_time": "2021-01-08T05:49:16.645800",
     "exception": false,
     "start_time": "2021-01-08T05:49:16.587259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import riiideducation\n",
    "import ast\n",
    "\n",
    "env = riiideducation.make_env()\n",
    "\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:16.695329Z",
     "iopub.status.busy": "2021-01-08T05:49:16.694562Z",
     "iopub.status.idle": "2021-01-08T05:49:16.697360Z",
     "shell.execute_reply": "2021-01-08T05:49:16.696956Z"
    },
    "papermill": {
     "duration": 0.028444,
     "end_time": "2021-01-08T05:49:16.697466",
     "exception": false,
     "start_time": "2021-01-08T05:49:16.669022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:16.748068Z",
     "iopub.status.busy": "2021-01-08T05:49:16.747521Z",
     "iopub.status.idle": "2021-01-08T05:49:18.062332Z",
     "shell.execute_reply": "2021-01-08T05:49:18.060919Z"
    },
    "papermill": {
     "duration": 1.342964,
     "end_time": "2021-01-08T05:49:18.062464",
     "exception": false,
     "start_time": "2021-01-08T05:49:16.719500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sys import getsizeof\n",
    "\n",
    "#supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0223,
     "end_time": "2021-01-08T05:49:18.107393",
     "exception": false,
     "start_time": "2021-01-08T05:49:18.085093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Setting up SAINT** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:18.499702Z",
     "iopub.status.busy": "2021-01-08T05:49:18.498930Z",
     "iopub.status.idle": "2021-01-08T05:49:18.501688Z",
     "shell.execute_reply": "2021-01-08T05:49:18.502081Z"
    },
    "papermill": {
     "duration": 0.37231,
     "end_time": "2021-01-08T05:49:18.502204",
     "exception": false,
     "start_time": "2021-01-08T05:49:18.129894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "q_pad = 13523 #Last value is not used\n",
    "\n",
    "a_pad = 3\n",
    "start_token = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:18.553113Z",
     "iopub.status.busy": "2021-01-08T05:49:18.552525Z",
     "iopub.status.idle": "2021-01-08T05:49:26.638557Z",
     "shell.execute_reply": "2021-01-08T05:49:26.637754Z"
    },
    "papermill": {
     "duration": 8.112745,
     "end_time": "2021-01-08T05:49:26.638667",
     "exception": false,
     "start_time": "2021-01-08T05:49:18.525922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "group = pd.read_pickle(\"../input/groups/group.pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:26.695321Z",
     "iopub.status.busy": "2021-01-08T05:49:26.694758Z",
     "iopub.status.idle": "2021-01-08T05:49:34.786894Z",
     "shell.execute_reply": "2021-01-08T05:49:34.788036Z"
    },
    "papermill": {
     "duration": 8.126387,
     "end_time": "2021-01-08T05:49:34.788251",
     "exception": false,
     "start_time": "2021-01-08T05:49:26.661864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_1_path = '../input/get-features-1/'\n",
    "que_data = pd.read_pickle(features_1_path + \"que_data.pickle\")\n",
    "\n",
    "difficulty = (np.round(que_data.que_correct_per, 1)*10).astype(\"int8\").values #Array of the difficulty of questions 0 -> 11\n",
    "difficulty = torch.Tensor(difficulty).long().to(device)\n",
    "\n",
    "unique_tags = pd.concat([que_data.tags1,que_data.tags2, que_data.tags3, que_data.tags4,que_data.tags5,que_data.tags6]).unique()\n",
    "tags_n = len(unique_tags)\n",
    "\n",
    "unk_tag = tags_n-1  #Unknown tag token\n",
    "que_data = que_data.replace(-1, unk_tag)\n",
    "\n",
    "part_valus = torch.from_numpy(que_data.part.values).long().to(device)\n",
    "que_data = que_data.to_dict(\"index\")\n",
    "\n",
    "\n",
    "que_arr = np.zeros((np.array(list(que_data.keys())).shape[0], 6))\n",
    "\n",
    "for i in que_data:\n",
    "    a = que_data[i]\n",
    "    que_arr[i] = [a['tags1'],a['tags2'],a['tags3'],a['tags4'],a['tags5'],a['tags6']]\n",
    "    \n",
    "    \n",
    "import pickle \n",
    "\n",
    "with open('../input/hashtable/user_info', 'rb') as handle:\n",
    "    user_info = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:34.880127Z",
     "iopub.status.busy": "2021-01-08T05:49:34.879240Z",
     "iopub.status.idle": "2021-01-08T05:49:35.550555Z",
     "shell.execute_reply": "2021-01-08T05:49:35.549831Z"
    },
    "papermill": {
     "duration": 0.722696,
     "end_time": "2021-01-08T05:49:35.550664",
     "exception": false,
     "start_time": "2021-01-08T05:49:34.827968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st_user_info = {}\n",
    "\n",
    "for i in user_info:\n",
    "    st_user_info[i] = {\"timestamp_ms\":user_info[i][\"first_timestamp\"]}\n",
    "    \n",
    "del user_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:35.609012Z",
     "iopub.status.busy": "2021-01-08T05:49:35.607274Z",
     "iopub.status.idle": "2021-01-08T05:49:35.609663Z",
     "shell.execute_reply": "2021-01-08T05:49:35.610065Z"
    },
    "papermill": {
     "duration": 0.036345,
     "end_time": "2021-01-08T05:49:35.610169",
     "exception": false,
     "start_time": "2021-01-08T05:49:35.573824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.scale * self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:35.673508Z",
     "iopub.status.busy": "2021-01-08T05:49:35.672618Z",
     "iopub.status.idle": "2021-01-08T05:49:35.676313Z",
     "shell.execute_reply": "2021-01-08T05:49:35.676741Z"
    },
    "papermill": {
     "duration": 0.043714,
     "end_time": "2021-01-08T05:49:35.676860",
     "exception": false,
     "start_time": "2021-01-08T05:49:35.633146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbedTag(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, que_arr, tags_n):\n",
    "        super(EmbedTag, self).__init__()\n",
    "        self.que_arr = torch.LongTensor(que_arr).to(device)\n",
    "        \n",
    "        self.embedding = nn.Embedding(tags_n, d_model)\n",
    "        \n",
    "    def forward(self, x): #(seq_len, batch)\n",
    "        \n",
    "        x = self.que_arr[x, :]  #(seq_len, batch, 6)\n",
    "        x = self.embedding(x)  #(seq_len, batch, 6, hidden)\n",
    "        \n",
    "        return torch.sum(x, dim=-2)  #(seq_len, batch, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:35.754615Z",
     "iopub.status.busy": "2021-01-08T05:49:35.749000Z",
     "iopub.status.idle": "2021-01-08T05:49:35.757041Z",
     "shell.execute_reply": "2021-01-08T05:49:35.756588Z"
    },
    "papermill": {
     "duration": 0.056818,
     "end_time": "2021-01-08T05:49:35.757163",
     "exception": false,
     "start_time": "2021-01-08T05:49:35.700345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, intoken, outtoken, hidden, que_arr, part_arr, difficulty, enc_layers=4, dec_layers=4, dropout=0.1, ts_unique=70, prior_unique=50):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        nhead = hidden//64\n",
    "        \n",
    "        self.encoder = nn.Embedding(intoken, hidden)\n",
    "        self.pos_encoder = PositionalEncoding(hidden, dropout)\n",
    "\n",
    "        self.decoder = nn.Embedding(outtoken, hidden)\n",
    "        self.pos_decoder = PositionalEncoding(hidden, dropout)\n",
    "        \n",
    "        \n",
    "        self.tagsEmbedder = EmbedTag(hidden, que_arr, tags_n)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers, num_decoder_layers=dec_layers, dim_feedforward=hidden*4, dropout=dropout, activation='relu')\n",
    "        self.fc_out = nn.Linear(hidden, 1)\n",
    "\n",
    "        self.src_mask = None\n",
    "        self.trg_mask = None\n",
    "        self.memory_mask = None\n",
    "      \n",
    "        self.part_embedding = nn.Embedding(7,hidden)\n",
    "        self.part_arr = part_arr\n",
    "        \n",
    "        self.ts_embedding = nn.Embedding(ts_unique, hidden)\n",
    "        self.prior_embedding = nn.Embedding(prior_unique, hidden)\n",
    "        \n",
    "        self.task_container_embedding = nn.Embedding(10000, hidden)\n",
    "        self.user_answer_embedding = nn.Embedding(5, hidden)\n",
    "        \n",
    "        self.difficulty = difficulty\n",
    "        self.difficulty_embedding = nn.Embedding(11, hidden)\n",
    "        self.dropout_7 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        self.dropout_4 = nn.Dropout(dropout)\n",
    "        self.dropout_5 = nn.Dropout(dropout)\n",
    "        self.dropout_6 = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        self.explan_embedding = nn.Embedding(3, hidden)\n",
    "        self.dropout_9 = nn.Dropout(dropout)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz, sz1=None):\n",
    "        \n",
    "        if sz1 == None:\n",
    "            mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        else:\n",
    "            mask = torch.triu(torch.ones(sz, sz1), 1)\n",
    "            \n",
    "        return mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "    def make_len_mask_a(self, inp):\n",
    "        return (inp == a_pad).transpose(0, 1)   #(batch_size, output_seq_len)\n",
    "    \n",
    "    def make_len_mask_q(self, inp):\n",
    "        return (inp == q_pad).transpose(0, 1) #(batch_size, input_seq_len)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, src, trg, ts, prior, task_container, user_answer, explan):\n",
    "\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device)\n",
    "            \n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            self.src_mask = self.generate_square_subsequent_mask(len(src)).to(trg.device)\n",
    "            \n",
    "        if self.memory_mask is None or self.memory_mask.size(0) != len(trg) or self.memory_mask.size(1) != len(src):\n",
    "            self.memory_mask = self.generate_square_subsequent_mask(len(trg),len(src)).to(trg.device)\n",
    "            \n",
    "\n",
    "        #Adding padding mask\n",
    "        src_pad_mask = self.make_len_mask_q(src)\n",
    "        trg_pad_mask = self.make_len_mask_a(trg)\n",
    "\n",
    "        #Get part, prior, timestamp, task_container and user answer embedding\n",
    "        part_emb = self.dropout_1(self.part_embedding(self.part_arr[src]-1))\n",
    "        ts_emb = self.dropout_3(self.ts_embedding(ts))\n",
    "        user_answer_emb = self.dropout_5(self.user_answer_embedding(user_answer))        \n",
    "        \n",
    "        \n",
    "        #Add embeddings Encoder\n",
    "        src = self.encoder(src)  #Embedding\n",
    "        src = torch.add(src, part_emb)\n",
    "        src = torch.add(src, ts_emb)   #Last interaction days \n",
    "        src = self.pos_encoder(src)   #Pos embedding\n",
    "        \n",
    "\n",
    "        #Add embedding decoder\n",
    "        trg = self.decoder(trg)\n",
    "        trg = torch.add(trg, user_answer_emb)\n",
    "        trg = self.pos_decoder(trg)\n",
    "\n",
    "        output = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask, memory_mask=self.memory_mask,\n",
    "                                  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask, memory_key_padding_mask=src_pad_mask)\n",
    "        \n",
    "\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:35.811884Z",
     "iopub.status.busy": "2021-01-08T05:49:35.811131Z",
     "iopub.status.idle": "2021-01-08T05:49:36.423103Z",
     "shell.execute_reply": "2021-01-08T05:49:36.422672Z"
    },
    "papermill": {
     "duration": 0.64267,
     "end_time": "2021-01-08T05:49:36.423200",
     "exception": false,
     "start_time": "2021-01-08T05:49:35.780530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): Embedding(13524, 128)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Embedding(4, 128)\n",
       "  (pos_decoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tagsEmbedder): EmbedTag(\n",
       "    (embedding): Embedding(189, 128)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (part_embedding): Embedding(7, 128)\n",
       "  (ts_embedding): Embedding(70, 128)\n",
       "  (prior_embedding): Embedding(50, 128)\n",
       "  (task_container_embedding): Embedding(10000, 128)\n",
       "  (user_answer_embedding): Embedding(5, 128)\n",
       "  (difficulty_embedding): Embedding(11, 128)\n",
       "  (dropout_7): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_4): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_5): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_6): Dropout(p=0.1, inplace=False)\n",
       "  (explan_embedding): Embedding(3, 128)\n",
       "  (dropout_9): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 128\n",
    "\n",
    "INPUT_DIM = q_pad+1\n",
    "OUTPUT_DIM = 4\n",
    "\n",
    "model_saint = TransformerModel(INPUT_DIM, OUTPUT_DIM, hidden=d_model, que_arr=que_arr,part_arr=part_valus, difficulty=difficulty).to(device)\n",
    "weights = torch.load(\"../input/last-saint/last.torch\", map_location=torch.device(device))\n",
    "model_saint.load_state_dict(weights)\n",
    "\n",
    "model_saint.to(device)\n",
    "\n",
    "model_saint.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:36.494875Z",
     "iopub.status.busy": "2021-01-08T05:49:36.492991Z",
     "iopub.status.idle": "2021-01-08T05:49:36.495490Z",
     "shell.execute_reply": "2021-01-08T05:49:36.495917Z"
    },
    "papermill": {
     "duration": 0.048314,
     "end_time": "2021-01-08T05:49:36.496030",
     "exception": false,
     "start_time": "2021-01-08T05:49:36.447716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred_users(vals): #Input must be (eval_batch, 3): [\"user_id\", \"content_id\", \"content_type_id\"]\n",
    "\n",
    "    eval_batch = vals.shape[0]\n",
    "\n",
    "    tensor_question = np.zeros((eval_batch, 100), dtype=np.long)\n",
    "    tensor_answers = np.zeros((eval_batch, 100), dtype=np.long)\n",
    "    tensor_ts = np.zeros((eval_batch, 100), dtype=np.long)\n",
    "    tensor_user_answer = np.zeros((eval_batch, 100), dtype=np.long)\n",
    "\n",
    "\n",
    "    val_len = []\n",
    "    preds = []\n",
    "    group_index = group.index\n",
    "\n",
    "    for i, line in enumerate(vals):\n",
    "\n",
    "        if line[2] == True:\n",
    "            val_len.append(0)\n",
    "            continue\n",
    "\n",
    "        user_id = line[0]\n",
    "        question_id = line[1]\n",
    "        timestamp = get_timestamp(line[4], user_id) #Compute timestamp difference correctly\n",
    "        prior = get_prior(line[5])\n",
    "        task_container_id = line[3]\n",
    "\n",
    "        que_history = np.array([], dtype=np.int32)\n",
    "        answers_history = np.array([], dtype=np.int32)  \n",
    "        ts_history = np.array([], dtype=np.int32)  \n",
    "        user_answer_history = np.array([], dtype=np.int32)  \n",
    "\n",
    "        if user_id in group_index:\n",
    "\n",
    "            cap = 99\n",
    "            que_history, answers_history, ts_history, user_answer_history = group[user_id]\n",
    "\n",
    "            que_history = que_history[-cap:]\n",
    "            answers_history = answers_history[-cap:]\n",
    "            ts_history = ts_history[-cap:]\n",
    "            user_answer_history = user_answer_history[-cap:]\n",
    "\n",
    "\n",
    "        a_token = 2\n",
    "        user_a_token = 4\n",
    "\n",
    "        #Decoder data, add start token\n",
    "        answers_history = np.concatenate(([a_token],answers_history))\n",
    "        user_answer_history = np.concatenate(([user_a_token],user_answer_history))\n",
    "\n",
    "        #Decoder data\n",
    "        que_history = np.concatenate((que_history, [question_id]))  #Add current question\n",
    "        ts_history = np.concatenate((ts_history, [timestamp]))  \n",
    "\n",
    "        tensor_question[i][:len(que_history)] = que_history\n",
    "        tensor_answers[i][:len(que_history)] = answers_history\n",
    "        tensor_ts[i][:len(que_history)] = ts_history\n",
    "        tensor_user_answer[i][:len(que_history)] = user_answer_history\n",
    "\n",
    "        val_len.append(len(que_history))\n",
    "\n",
    "    tensor_question = torch.from_numpy(tensor_question).long().T.to(device)\n",
    "    tensor_answers = torch.from_numpy(tensor_answers).long().T.to(device)\n",
    "    tensor_ts = torch.from_numpy(tensor_ts).long().T.to(device)\n",
    "    tensor_user_answer = torch.from_numpy(tensor_user_answer).long().T.to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        out = F.sigmoid(model_saint(tensor_question, tensor_answers, tensor_ts, None, None, tensor_user_answer,2)).squeeze(dim=-1).T\n",
    "\n",
    "\n",
    "    for j in range(len(val_len)):\n",
    "        preds.append(out[j][val_len[j]-1].item())\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:36.552097Z",
     "iopub.status.busy": "2021-01-08T05:49:36.550731Z",
     "iopub.status.idle": "2021-01-08T05:49:36.553218Z",
     "shell.execute_reply": "2021-01-08T05:49:36.553709Z"
    },
    "papermill": {
     "duration": 0.032818,
     "end_time": "2021-01-08T05:49:36.553821",
     "exception": false,
     "start_time": "2021-01-08T05:49:36.521003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_preds(preds):\n",
    "    \n",
    "    if preds.shape[0] > 1000:\n",
    "        ret = []\n",
    "        for i in np.array_split(preds, preds.shape[0]//1000):\n",
    "            ret.extend(pred_users(i))\n",
    "        return ret\n",
    "    else:\n",
    "        return pred_users(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:36.655222Z",
     "iopub.status.busy": "2021-01-08T05:49:36.654332Z",
     "iopub.status.idle": "2021-01-08T05:49:36.658796Z",
     "shell.execute_reply": "2021-01-08T05:49:36.658272Z"
    },
    "papermill": {
     "duration": 0.08081,
     "end_time": "2021-01-08T05:49:36.658899",
     "exception": false,
     "start_time": "2021-01-08T05:49:36.578089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_group_var(vals):\n",
    "    \n",
    "    global group\n",
    "    \n",
    "    for i, line in enumerate(vals):\n",
    "        \n",
    "        user_id = line[0]\n",
    "        question_id = line[1]\n",
    "        \n",
    "        content_type_id = line[2]\n",
    "        ts = get_timestamp(line[4], user_id)\n",
    "        \n",
    "        correct = line[6]\n",
    "        user_answer = line[7]\n",
    "        \n",
    "        \n",
    "        if content_type_id == True:\n",
    "            continue\n",
    "\n",
    "        if st_user_info.get(user_id, -1) == -1:\n",
    "            st_user_info[user_id] = {\"timestamp_ms\":0}\n",
    "        else:\n",
    "            st_user_info[user_id][\"timestamp_ms\"] = line[4] #Update user info\n",
    "            \n",
    "        if user_id in group.index:\n",
    "            questions= np.append(group[user_id][0],[question_id])\n",
    "            answers= np.append(group[user_id][1],[correct])\n",
    "            ts= np.append(group[user_id][2],[ts])\n",
    "            user_answer= np.append(group[user_id][3],[user_answer])\n",
    "            \n",
    "            group[user_id] = (questions, answers, ts, user_answer)\n",
    "        else:\n",
    "            group[user_id] = (np.array([question_id], dtype=np.int32), np.array([correct], dtype=np.int32), np.array([ts], dtype=np.int32)\n",
    "                             ,np.array([user_answer], dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:36.734955Z",
     "iopub.status.busy": "2021-01-08T05:49:36.733970Z",
     "iopub.status.idle": "2021-01-08T05:49:36.736824Z",
     "shell.execute_reply": "2021-01-08T05:49:36.737272Z"
    },
    "papermill": {
     "duration": 0.049672,
     "end_time": "2021-01-08T05:49:36.737438",
     "exception": false,
     "start_time": "2021-01-08T05:49:36.687766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ordinal_enc = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 30: 21, 40: 22, 50: 23, 60: 24, 70: 25, 80: 26, 90: 27, 100: 28, 110: 29, 120: 30, 130: 31, 140: 32, 150: 33, 160: 34, 170: 35, 180: 36, 190: 37, 200: 38, 210: 39, 220: 40, 230: 41, 240: 42, 250: 43, 260: 44, 270: 45, 280: 46, 290: 47, 300: 48}\n",
    "boundaries = [120,600,1800,3600,10800,43200,86400,259200,604800]\n",
    "\n",
    "def get_prior(prior):\n",
    "    \n",
    "    if prior != prior:\n",
    "        return 0\n",
    "    \n",
    "    prior /= 1000\n",
    "    if prior > 20:\n",
    "        prior = np.round(prior, decimals=-1)\n",
    "    \n",
    "    return ordinal_enc.get(round(prior),0)\n",
    "\n",
    "\n",
    "def get_timestamp(ts, user_id):\n",
    "    \n",
    "    if st_user_info.get(user_id, -1) == -1:\n",
    "        return 0\n",
    "    \n",
    "    diff = (ts - st_user_info[user_id][\"timestamp_ms\"])/1000\n",
    "    \n",
    "    if diff < 0:\n",
    "        return 0\n",
    "    \n",
    "    if diff <= 60:\n",
    "        return int(diff)\n",
    "    \n",
    "    for i, boundary in enumerate(boundaries):\n",
    "        if boundary > diff:\n",
    "            break\n",
    "            \n",
    "    if i == len(boundaries) - 1:\n",
    "        return 60+i+1\n",
    "    \n",
    "    return 60+i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:36.797013Z",
     "iopub.status.busy": "2021-01-08T05:49:36.796404Z",
     "iopub.status.idle": "2021-01-08T05:49:36.800598Z",
     "shell.execute_reply": "2021-01-08T05:49:36.800158Z"
    },
    "papermill": {
     "duration": 0.03443,
     "end_time": "2021-01-08T05:49:36.800698",
     "exception": false,
     "start_time": "2021-01-08T05:49:36.766268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prior_part_mean_dict = {1: 22166.159642501425,\n",
    " 2: 18714.69673913695,\n",
    " 3: 23620.317746179924,\n",
    " 4: 23762.753651169547,\n",
    " 5: 25094.620302855932,\n",
    " 6: 32417.37918735745,\n",
    " 7: 47444.16407400242}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:49:36.858791Z",
     "iopub.status.busy": "2021-01-08T05:49:36.858148Z",
     "iopub.status.idle": "2021-01-08T05:50:03.787877Z",
     "shell.execute_reply": "2021-01-08T05:50:03.788927Z"
    },
    "papermill": {
     "duration": 26.963575,
     "end_time": "2021-01-08T05:50:03.789108",
     "exception": false,
     "start_time": "2021-01-08T05:49:36.825533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import ast\n",
    "\n",
    "with open('../input/lgbm-test/repeated_que_count', 'rb') as handle:\n",
    "    repeated_que_count = pickle.load(handle)\n",
    "\n",
    "with open('../input/lgbm-test/user_info', 'rb') as handle:\n",
    "    user_info = pickle.load(handle)\n",
    "\n",
    "with open('../input/lgbm-test/watched_tags', 'rb') as handle:\n",
    "    watched_tags = pickle.load(handle)\n",
    "    \n",
    "    \n",
    "with open('../input/lgbm-test/containers_mean', 'rb') as handle:\n",
    "    containers_mean = pickle.load(handle)\n",
    "    \n",
    "with open('../input/lgbm-test/hardest', 'rb') as handle:\n",
    "    hard_questions = pickle.load(handle)\n",
    "    \n",
    "with open('../input/lgbm-test/easiest', 'rb') as handle:\n",
    "    easy_questions = pickle.load(handle)\n",
    "    \n",
    "with open('../input/lgbm-test/que_2_k', 'rb') as handle:\n",
    "    que_2_k = pickle.load(handle)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:03.890190Z",
     "iopub.status.busy": "2021-01-08T05:50:03.889316Z",
     "iopub.status.idle": "2021-01-08T05:50:04.724752Z",
     "shell.execute_reply": "2021-01-08T05:50:04.724250Z"
    },
    "papermill": {
     "duration": 0.889789,
     "end_time": "2021-01-08T05:50:04.724869",
     "exception": false,
     "start_time": "2021-01-08T05:50:03.835080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for u in user_info:\n",
    "    \n",
    "    user_info[u][\"count_2\"] = user_info[u][\"count\"]\n",
    "    user_info[u][\"part_count_2\"] = user_info[u][\"part_count\"].copy()\n",
    "    user_info[u][\"last_part\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:04.782825Z",
     "iopub.status.busy": "2021-01-08T05:50:04.782111Z",
     "iopub.status.idle": "2021-01-08T05:50:06.375246Z",
     "shell.execute_reply": "2021-01-08T05:50:06.374359Z"
    },
    "papermill": {
     "duration": 1.625162,
     "end_time": "2021-01-08T05:50:06.375361",
     "exception": false,
     "start_time": "2021-01-08T05:50:04.750199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "groups = pd.read_pickle(\"../input/lgbm-test/groups\")\n",
    "\n",
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:06.434634Z",
     "iopub.status.busy": "2021-01-08T05:50:06.433924Z",
     "iopub.status.idle": "2021-01-08T05:50:06.603340Z",
     "shell.execute_reply": "2021-01-08T05:50:06.603768Z"
    },
    "papermill": {
     "duration": 0.203103,
     "end_time": "2021-01-08T05:50:06.603902",
     "exception": false,
     "start_time": "2021-01-08T05:50:06.400799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_1_path = '../input/get-features-1/'\n",
    "que_data = pd.read_pickle(features_1_path + \"que_data.pickle\")\n",
    "\n",
    "questions = que_data.drop(columns=[\"options_number\",\"correctness_number\", \"correct_answer\",\"tags6\",\"tags5\", \"tags4\"]).to_dict(\"index\")\n",
    "questions1 = que_data[[\"tags1\", \"tags2\", \"tags3\",\"tags4\",\"tags5\", \"tags6\"]].to_dict(\"index\")\n",
    "\n",
    "parts = que_data.part.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:06.670667Z",
     "iopub.status.busy": "2021-01-08T05:50:06.670072Z",
     "iopub.status.idle": "2021-01-08T05:50:06.682476Z",
     "shell.execute_reply": "2021-01-08T05:50:06.682005Z"
    },
    "papermill": {
     "duration": 0.052846,
     "end_time": "2021-01-08T05:50:06.682572",
     "exception": false,
     "start_time": "2021-01-08T05:50:06.629726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lec_data = pd.read_csv(\"../input/riiid-test-answer-prediction/lectures.csv\")\n",
    "lec_dict = lec_data[[\"lecture_id\", \"tag\"]].set_index(\"lecture_id\").tag.to_dict()\n",
    "\n",
    "#Add dtype for every feature\n",
    "features = [\n",
    "    'task_container_id', \"ts_diff_shifted\", \"watched\",\"ts_diff_shifted_2\",\n",
    "    'content_id', \"k\", \"k_acc\", \"el_avg\", \"wut\",\n",
    "    'prior_question_elapsed_time', \"time_diff2\", \"rolling_mean_5\", \"rolling_mean_10\", \"rolling_mean_15\", \"prior_question_had_explanation_u_part_avg\",\n",
    "    'prior_question_had_explanation', \"hard_ratio_opp\", \"easy_ratio_opp\", \"correct_recency\", \"prior_question_elapsed_time_u_part_avg\", \"ewm_mean_10\", \"rolling_mean_5_prior_question\",\n",
    "    'last_lecture', \"part_mean\", \"opp_mean\", \"mean_pause\", \"timestamp\", \"prior_part_mean\",\n",
    "    \"container_mean\", \"lecs_per\", \"hard_ratio\", \"easy_ratio\",\n",
    "    'que_count_user', 'question_repeated', \"rolling_mean\",\"time_diff3\", \"time_diff4\",\n",
    "    'user_mean', \"time_diff1\", \"time_diff\", \"sessions\", \"session_count\", \"prior_question_had_explanation_ratio\"\n",
    "] + que_data.columns.tolist()[:-1]\n",
    "\n",
    "features.remove(\"options_number\")\n",
    "features.remove(\"correct_answer\")\n",
    "features.remove(\"tags6\")\n",
    "features.remove(\"tags5\")\n",
    "features.remove(\"tags4\")\n",
    "\n",
    "\n",
    "test_cols = ['row_id','timestamp','user_id','content_id','content_type_id','task_container_id','prior_question_elapsed_time',\n",
    "             'prior_question_had_explanation','prior_group_answers_correct','prior_group_responses']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:06.746599Z",
     "iopub.status.busy": "2021-01-08T05:50:06.744795Z",
     "iopub.status.idle": "2021-01-08T05:50:06.747269Z",
     "shell.execute_reply": "2021-01-08T05:50:06.747759Z"
    },
    "papermill": {
     "duration": 0.037593,
     "end_time": "2021-01-08T05:50:06.747879",
     "exception": false,
     "start_time": "2021-01-08T05:50:06.710286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stack_features = [\n",
    "    'task_container_id', \"ts_diff_shifted\", \"watched\",\"ts_diff_shifted_2\",\n",
    "    'content_id', \"k\", \"k_acc\", \"el_avg\", \"wut\", \"lgb_preds\", \"st_preds\",\n",
    "    'prior_question_elapsed_time', \"time_diff2\", \"rolling_mean_5\", \"rolling_mean_10\", \"rolling_mean_15\", \"prior_question_had_explanation_u_part_avg\",\n",
    "    'prior_question_had_explanation', \"hard_ratio_opp\", \"easy_ratio_opp\", \"correct_recency\", \"prior_question_elapsed_time_u_part_avg\", \"ewm_mean_10\", \"rolling_mean_5_prior_question\",\n",
    "    'last_lecture', \"part_mean\", \"opp_mean\", \"mean_pause\", \"timestamp\", \"prior_part_mean\",\n",
    "    \"container_mean\", \"lecs_per\", \"hard_ratio\", \"easy_ratio\",\n",
    "    'que_count_user', 'question_repeated', \"rolling_mean\",\"time_diff3\", \"time_diff4\",\n",
    "    'user_mean', \"time_diff1\", \"time_diff\", \"sessions\", \"session_count\", \"prior_question_had_explanation_ratio\"\n",
    "] + que_data.columns.tolist()[:-1]\n",
    "\n",
    "stack_features.remove(\"options_number\")\n",
    "stack_features.remove(\"correct_answer\")\n",
    "stack_features.remove(\"tags6\")\n",
    "stack_features.remove(\"tags5\")\n",
    "stack_features.remove(\"tags4\")\n",
    "\n",
    "lgb_preds_idx = stack_features.index(\"lgb_preds\")\n",
    "st_preds_idx = stack_features.index(\"st_preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:06.806128Z",
     "iopub.status.busy": "2021-01-08T05:50:06.805346Z",
     "iopub.status.idle": "2021-01-08T05:50:06.808166Z",
     "shell.execute_reply": "2021-01-08T05:50:06.807640Z"
    },
    "papermill": {
     "duration": 0.034923,
     "end_time": "2021-01-08T05:50:06.808269",
     "exception": false,
     "start_time": "2021-01-08T05:50:06.773346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_size = 20\n",
    "\n",
    "cols = {test_cols[k]:k for k in range(len(test_cols))}\n",
    "features_dict = {features[k]:k for k in range(len(features))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:06.871300Z",
     "iopub.status.busy": "2021-01-08T05:50:06.870639Z",
     "iopub.status.idle": "2021-01-08T05:50:06.874254Z",
     "shell.execute_reply": "2021-01-08T05:50:06.873813Z"
    },
    "papermill": {
     "duration": 0.040453,
     "end_time": "2021-01-08T05:50:06.874347",
     "exception": false,
     "start_time": "2021-01-08T05:50:06.833894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "new_user = {'count': 0, 'mean_acc':0.5, 'correct_count': 0, 'last_lec':0, 'tmp':0,\"first_timestamp\":0, \"second_timestamp\":0,\n",
    "            \"third_timestamp\":0, \"fourth_timestamp\":0, \"fifth_timestamp\":0, \"lecs_n\":0,\"interaction_n\":0, \"ts_diff_shifted\":0.,\n",
    "            \"part_corr\":np.zeros((7), dtype=np.uint16), \"part_count\":np.zeros((7), dtype=np.uint16), \"hard_ct\":0, \"hard_cr\":0, \"easy_ct\":0, \"easy_cr\":0,\n",
    "           \"sessions\":0, \"session_count\":0, \"sum_pauses\":0., \"had_exp\":0, \"el_sum\":0, \"part_et\": np.zeros((7), dtype=np.float), \n",
    "            \"part_explan\": np.zeros((7), dtype=np.uint16), \"k_count\": np.zeros((k_size), dtype=np.uint16), \"k_corr\": np.zeros((k_size), dtype=np.uint16), \n",
    "            \"recent_corr\":0, \"priors_5\": [], \"ts_diff_shifted_2\":0., \"count_2\":0, \"part_count_2\":np.zeros((7), dtype=np.uint16), \"last_part\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:06.933245Z",
     "iopub.status.busy": "2021-01-08T05:50:06.932725Z",
     "iopub.status.idle": "2021-01-08T05:50:06.936362Z",
     "shell.execute_reply": "2021-01-08T05:50:06.935849Z"
    },
    "papermill": {
     "duration": 0.035768,
     "end_time": "2021-01-08T05:50:06.936474",
     "exception": false,
     "start_time": "2021-01-08T05:50:06.900706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_meta_data(data_1):\n",
    "    \n",
    "    user_id = data_1[cols['user_id']]\n",
    "    content_type_id = data_1[cols['content_type_id']]\n",
    "    content_id = data_1[cols['content_id']]\n",
    "    prior_group_answers_correct = data_1[cols['prior_group_answers_correct']]\n",
    "    timestamp = data_1[cols['timestamp']]\n",
    "    task_container_id = data_1[cols['task_container_id']]\n",
    "    prior_question_had_explanation = data_1[cols['prior_question_had_explanation']]\n",
    "    elapsed = data_1[cols['prior_question_elapsed_time']]\n",
    "    \n",
    "    return user_id, content_type_id, content_id, prior_group_answers_correct, timestamp,task_container_id,prior_question_had_explanation, elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:06.993199Z",
     "iopub.status.busy": "2021-01-08T05:50:06.992673Z",
     "iopub.status.idle": "2021-01-08T05:50:06.996722Z",
     "shell.execute_reply": "2021-01-08T05:50:06.996211Z"
    },
    "papermill": {
     "duration": 0.034405,
     "end_time": "2021-01-08T05:50:06.996821",
     "exception": false,
     "start_time": "2021-01-08T05:50:06.962416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_user(user_id): \n",
    "    user_info[user_id] = copy.deepcopy(new_user)\n",
    "    repeated_que_count[user_id] = {}\n",
    "    groups[user_id] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:07.055001Z",
     "iopub.status.busy": "2021-01-08T05:50:07.054312Z",
     "iopub.status.idle": "2021-01-08T05:50:07.057409Z",
     "shell.execute_reply": "2021-01-08T05:50:07.057871Z"
    },
    "papermill": {
     "duration": 0.035444,
     "end_time": "2021-01-08T05:50:07.057988",
     "exception": false,
     "start_time": "2021-01-08T05:50:07.022544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_user_part_acc(user_id, question, answered_correctly, elapsed, explan):\n",
    "    \n",
    "    part = parts.get(question, -1)\n",
    "    \n",
    "    user_info[user_id][\"part_count\"][part-1] += 1\n",
    "    user_info[user_id][\"part_corr\"][part-1] += answered_correctly\n",
    "    \n",
    "    if not isinstance(explan, pd._libs.missing.NAType) and explan == explan:\n",
    "        user_info[user_id][\"had_exp\"] += explan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:07.121051Z",
     "iopub.status.busy": "2021-01-08T05:50:07.120307Z",
     "iopub.status.idle": "2021-01-08T05:50:07.123251Z",
     "shell.execute_reply": "2021-01-08T05:50:07.122736Z"
    },
    "papermill": {
     "duration": 0.039418,
     "end_time": "2021-01-08T05:50:07.123335",
     "exception": false,
     "start_time": "2021-01-08T05:50:07.083917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_user(user_id, had_exp, elapsed, content_id ,answered_correctly, timestamp):\n",
    "        \n",
    "    user_info[user_id]['count'] += 1\n",
    "\n",
    "    if repeated_que_count[user_id].get(content_id,  -1) == -1: #If first time question for the user\n",
    "        repeated_que_count[user_id][content_id] = 1\n",
    "    else:\n",
    "        repeated_que_count[user_id][content_id] += 1\n",
    "\n",
    "    if answered_correctly:\n",
    "        user_info[user_id]['correct_count'] += 1\n",
    "\n",
    "    user_info[user_id]['mean_acc'] = user_info[user_id]['correct_count']/user_info[user_id]['count']\n",
    "    \n",
    "    \n",
    "    update_user_part_acc(user_id, content_id, answered_correctly, elapsed, had_exp)\n",
    "    \n",
    "    if hard_questions.get(content_id, False):\n",
    "        user_info[user_id][\"hard_ct\"] += 1\n",
    "        user_info[user_id][\"hard_cr\"] += answered_correctly\n",
    "        \n",
    "    if easy_questions.get(content_id, False):\n",
    "        user_info[user_id][\"easy_ct\"] += 1\n",
    "        user_info[user_id][\"easy_cr\"] += answered_correctly     \n",
    "        \n",
    "    \n",
    "    k = que_2_k[content_id]\n",
    "    user_info[user_id][\"k_count\"][k] += 1\n",
    "    user_info[user_id][\"k_corr\"][k] += answered_correctly\n",
    "    \n",
    "    if answered_correctly:\n",
    "        user_info[user_id][\"recent_corr\"] = timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:07.182836Z",
     "iopub.status.busy": "2021-01-08T05:50:07.182108Z",
     "iopub.status.idle": "2021-01-08T05:50:07.184962Z",
     "shell.execute_reply": "2021-01-08T05:50:07.184558Z"
    },
    "papermill": {
     "duration": 0.035587,
     "end_time": "2021-01-08T05:50:07.185071",
     "exception": false,
     "start_time": "2021-01-08T05:50:07.149484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_lec_data(user_id, content_id):\n",
    "    \n",
    "    if watched_tags.get(str(user_id), -1) == -1:  #If user's first lecture\n",
    "        watched_tags[str(user_id)] = {}\n",
    "        \n",
    "    if user_info.get(user_id, -1) == -1: #In case first action user does is watching a lecture\n",
    "        add_user(user_id)\n",
    "        \n",
    "        \n",
    "    user_info[user_id][\"lecs_n\"] += 1\n",
    "    \n",
    "    lec_tag  = lec_dict[content_id]\n",
    "\n",
    "    watched_tags[str(user_id)][str(lec_tag)] = 1\n",
    "    user_info[user_id]['last_lec'] = content_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:07.251543Z",
     "iopub.status.busy": "2021-01-08T05:50:07.246081Z",
     "iopub.status.idle": "2021-01-08T05:50:07.255604Z",
     "shell.execute_reply": "2021-01-08T05:50:07.255173Z"
    },
    "papermill": {
     "duration": 0.044203,
     "end_time": "2021-01-08T05:50:07.255698",
     "exception": false,
     "start_time": "2021-01-08T05:50:07.211495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def non_lag_update(user_id, timestamp, elapsed, explan, lec):\n",
    "    \n",
    "    \n",
    "    timestamp = timestamp/8.64e+7\n",
    "    \n",
    "    diff_timestamp_1 = timestamp - user_info[user_id][\"first_timestamp\"]\n",
    "    diff_timestamp_2 = timestamp - user_info[user_id][\"second_timestamp\"]\n",
    "    diff_timestamp_3 = timestamp - user_info[user_id][\"third_timestamp\"]\n",
    "    diff_timestamp_4 = timestamp - user_info[user_id][\"fourth_timestamp\"]\n",
    "    diff_timestamp_5 = timestamp - user_info[user_id][\"fifth_timestamp\"]\n",
    "\n",
    "    user_info[user_id][\"fifth_timestamp\"] = user_info[user_id][\"fourth_timestamp\"]\n",
    "    user_info[user_id][\"fourth_timestamp\"] = user_info[user_id][\"third_timestamp\"]\n",
    "    user_info[user_id][\"third_timestamp\"] = user_info[user_id][\"second_timestamp\"]\n",
    "    user_info[user_id][\"second_timestamp\"] = user_info[user_id][\"first_timestamp\"]\n",
    "    user_info[user_id][\"first_timestamp\"] = timestamp\n",
    "    \n",
    "    if (user_info[user_id][\"second_timestamp\"] - user_info[user_id][\"third_timestamp\"]) != 0:\n",
    "        user_info[user_id][\"ts_diff_shifted\"] = user_info[user_id][\"second_timestamp\"]*8.64e+7 - user_info[user_id][\"third_timestamp\"]*8.64e+7\n",
    "    \n",
    "    if (user_info[user_id][\"second_timestamp\"] - user_info[user_id][\"fourth_timestamp\"]) != 0:\n",
    "        user_info[user_id][\"ts_diff_shifted_2\"] = user_info[user_id][\"second_timestamp\"]*8.64e+7 - user_info[user_id][\"fourth_timestamp\"]*8.64e+7\n",
    "        \n",
    "        \n",
    "    if not isinstance(explan, pd._libs.missing.NAType) and explan == explan and not lec:\n",
    "        user_info[user_id][\"priors_5\"].append(explan) \n",
    "    \n",
    "    if diff_timestamp_1 > 0.083:\n",
    "        user_info[user_id][\"sessions\"] += 1\n",
    "        user_info[user_id][\"session_count\"] = 0\n",
    "        user_info[user_id][\"sum_pauses\"] += diff_timestamp_1\n",
    "        \n",
    "    user_info[user_id][\"session_count\"] += 1\n",
    "    \n",
    "    \n",
    "    return  diff_timestamp_1, diff_timestamp_2, diff_timestamp_3, diff_timestamp_4, diff_timestamp_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:07.323730Z",
     "iopub.status.busy": "2021-01-08T05:50:07.322890Z",
     "iopub.status.idle": "2021-01-08T05:50:07.325695Z",
     "shell.execute_reply": "2021-01-08T05:50:07.325261Z"
    },
    "papermill": {
     "duration": 0.043704,
     "end_time": "2021-01-08T05:50:07.325790",
     "exception": false,
     "start_time": "2021-01-08T05:50:07.282086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_data(prior_group_answers_correct):  #Update data retrospectively\n",
    "    \n",
    "    global tmp_data\n",
    "    \n",
    "    arr = np.array(ast.literal_eval(prior_group_answers_correct))\n",
    "\n",
    "    for i, line in enumerate(tmp_data): #Loop through users with correct answers\n",
    "        \n",
    "        user_id = line[cols['user_id']]\n",
    "        content_type_id = line[cols['content_type_id']]\n",
    "        content_id = line[cols['content_id']]\n",
    "        timestamp = line[cols['timestamp']]\n",
    "        task_container_id = line[cols['task_container_id']]\n",
    "        \n",
    "        explan = line[cols['prior_question_had_explanation']]\n",
    "        if isinstance(explan, pd._libs.missing.NAType) or explan == explan:\n",
    "            explan = 0\n",
    "            \n",
    "        elapsed = line[cols['prior_question_elapsed_time']]\n",
    "        if isinstance(elapsed, pd._libs.missing.NAType) or elapsed == elapsed:\n",
    "            elapsed = 0\n",
    "            \n",
    "        answered_correctly = arr[i]\n",
    "        \n",
    "        \n",
    "        user_arr = groups[user_id]\n",
    "        user_arr.insert(len(user_arr), answered_correctly)\n",
    "        groups[user_id] = user_arr\n",
    "\n",
    "        if content_type_id == False: #If question\n",
    "\n",
    "            if user_info.get(user_id, -1) == -1: #If user is new\n",
    "                add_user(user_id)\n",
    "                \n",
    "            update_user(user_id, explan, elapsed, content_id, answered_correctly, timestamp)\n",
    "\n",
    "     \n",
    "    tmp_data = []  #Flush tmp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:07.385355Z",
     "iopub.status.busy": "2021-01-08T05:50:07.383632Z",
     "iopub.status.idle": "2021-01-08T05:50:07.385941Z",
     "shell.execute_reply": "2021-01-08T05:50:07.386352Z"
    },
    "papermill": {
     "duration": 0.033695,
     "end_time": "2021-01-08T05:50:07.386470",
     "exception": false,
     "start_time": "2021-01-08T05:50:07.352775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:07.470898Z",
     "iopub.status.busy": "2021-01-08T05:50:07.448730Z",
     "iopub.status.idle": "2021-01-08T05:50:07.498896Z",
     "shell.execute_reply": "2021-01-08T05:50:07.498439Z"
    },
    "papermill": {
     "duration": 0.085588,
     "end_time": "2021-01-08T05:50:07.498996",
     "exception": false,
     "start_time": "2021-01-08T05:50:07.413408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_1(chunk): #Optimized preprocess function for small batches, for batches > 2500 preprocess is better. Includes adding lecture\n",
    "    \n",
    "    \n",
    "    data_1 = chunk.values\n",
    "    out = np.zeros((data_1.shape[0], len(features)))\n",
    "    \n",
    "    batch_counts = data_1[:, [cols[\"user_id\"],cols[\"content_type_id\"]]]\n",
    "    batch_counts = Counter(batch_counts[batch_counts[:, 1] == False][:, 0])\n",
    "\n",
    "    global tmp_data\n",
    "\n",
    "\n",
    "    for i in range(data_1.shape[0]):\n",
    "\n",
    "        user_id, content_type_id, content_id ,prior_group_answers_correct, timestamp, task_container_id, explan, elapsed = get_meta_data(data_1[i])\n",
    "        part = parts.get(content_id, -1)\n",
    "        task_count = batch_counts[user_id]\n",
    "        \n",
    "        #When users answers are provided, update user states\n",
    "        if prior_group_answers_correct == prior_group_answers_correct and prior_group_answers_correct != '[]': #If user's previous responses are there\n",
    "            update_data(prior_group_answers_correct)\n",
    "\n",
    "        tmp_data.append(data_1[i].tolist()) #Append incoming data\n",
    "        \n",
    "        \n",
    "\n",
    "        if content_type_id: #If lecture, update the lecture state\n",
    "            update_lec_data(user_id, content_id)\n",
    "\n",
    "            \n",
    "        #Timestamp difference\n",
    "        if user_info.get(user_id, -1) == -1: #If user is new\n",
    "            add_user(user_id)        \n",
    "            \n",
    "        \n",
    "        user_info[user_id][\"interaction_n\"] += 1\n",
    "\n",
    "\n",
    "        _ = non_lag_update(user_id, timestamp, elapsed, explan, content_type_id)\n",
    "        diff_timestamp_1, diff_timestamp_2, diff_timestamp_3, diff_timestamp_4, diff_timestamp_5 = _\n",
    "\n",
    "        if not content_type_id:\n",
    "            if not isinstance(elapsed, pd._libs.missing.NAType) and elapsed == elapsed:\n",
    "                user_info[user_id][\"el_sum\"] += elapsed\n",
    "            else:\n",
    "                elapsed = 0\n",
    "\n",
    "            if isinstance(explan, pd._libs.missing.NAType) or explan != explan:\n",
    "                explan = 0 \n",
    "\n",
    "            if user_info[user_id][\"count_2\"] != 0:\n",
    "                last_part = user_info[user_id][\"last_part\"]\n",
    "                user_info[user_id][\"part_et\"][last_part-1] += elapsed\n",
    "                user_info[user_id][\"part_explan\"][last_part-1] += explan\n",
    "            \n",
    "            user_info[user_id][\"last_part\"] = part\n",
    "            \n",
    "            user_info[user_id][\"part_count_2\"][part-1] += 1\n",
    "            user_info[user_id][\"count_2\"] += 1\n",
    "\n",
    "            #-----Filling the array -------------------------------------------->\n",
    "\n",
    "            out[i, features_dict['content_id']] = content_id\n",
    "            out[i, features_dict['task_container_id']] = data_1[i, cols['task_container_id']]\n",
    "\n",
    "\n",
    "            #prior_question_elapsed_time, and prior_question_elapsed_time\n",
    "            out[i, features_dict['prior_question_elapsed_time']] = elapsed\n",
    "            out[i, features_dict['el_avg']] = (user_info[user_id][\"el_sum\"]/(user_info[user_id][\"count_2\"]))/1000\n",
    "            out[i, features_dict['prior_question_elapsed_time_u_part_avg']] = (user_info[user_id][\"part_et\"][part-1])/(user_info[user_id][\"part_count_2\"][part-1])\n",
    "\n",
    "            out[i, features_dict['prior_question_had_explanation']] = explan\n",
    "            out[i, features_dict['prior_question_had_explanation_u_part_avg']] = user_info[user_id][\"part_explan\"][part-1]/(user_info[user_id][\"part_count_2\"][part-1]+1)\n",
    "\n",
    "\n",
    "            #out[i, features_dict[\"content_type_id\"]] = content_type_id\n",
    "            #Fill all question features\n",
    "            out[i, features_dict['bundle_id']:] = np.array(list(questions.get(content_id).values())) #No unkown question expected\n",
    "\n",
    "            #que_count_user, question_repeated, user_mean, correct_count\n",
    "            out[i, features_dict['que_count_user']] = user_info.get(user_id, {}).get('count',0)\n",
    "            out[i, features_dict['question_repeated']] = repeated_que_count.get(user_id, {}).get(content_id, 0) + 1\n",
    "\n",
    "\n",
    "            m = user_info.get(user_id, {}).get('mean_acc', 0)\n",
    "            if m == 0: #Usually when user is new, mean drops to zero\n",
    "                out[i, features_dict['user_mean']] = 0.55\n",
    "            else:\n",
    "                out[i, features_dict['user_mean']] = m\n",
    "\n",
    "            out[i, features_dict['opp_mean']] = 1 - out[i, features_dict['user_mean']]\n",
    "\n",
    "            #out[i, features_dict['correct_count']] = user_info.get(user_id, {}).get('correct_count',0)\n",
    "            out[i, features_dict['last_lecture']] = user_info.get(user_id, {}).get('last_lec',0)\n",
    "\n",
    "\n",
    "            #Time gap\n",
    "            out[i, features_dict['time_diff']] = diff_timestamp_1\n",
    "            out[i, features_dict['time_diff1']] = diff_timestamp_2\n",
    "            out[i, features_dict['time_diff2']] = diff_timestamp_3\n",
    "            out[i, features_dict['time_diff3']] = diff_timestamp_4\n",
    "            out[i, features_dict['time_diff4']] = diff_timestamp_5\n",
    "\n",
    "            out[i, features_dict['timestamp']] = timestamp/8.64e+7\n",
    "            out[i, features_dict['correct_recency']] = (timestamp - user_info[user_id][\"recent_corr\"])/8.64e+7\n",
    "\n",
    "            kk = user_info[user_id][\"priors_5\"][-5:]\n",
    "            if len(kk) != 0: #If array not empty\n",
    "                #print(str(len(kk)) + \" \" + str(np.array(kk).mean()))\n",
    "                out[i, features_dict['rolling_mean_5_prior_question']] = np.array(kk).mean()\n",
    "\n",
    "            out[i, features_dict['ts_diff_shifted']] = user_info[user_id][\"ts_diff_shifted\"]\n",
    "            out[i, features_dict['ts_diff_shifted_2']] = user_info[user_id][\"ts_diff_shifted_2\"]\n",
    "\n",
    "            #Container mean\n",
    "            out[i, features_dict['container_mean']] = containers_mean[task_container_id]\n",
    "            out[i, features_dict['lecs_per']] = user_info[user_id][\"lecs_n\"]/user_info[user_id][\"interaction_n\"]*100\n",
    "            out[i, features_dict['sessions']] = user_info[user_id][\"sessions\"]\n",
    "            out[i, features_dict['session_count']] = user_info[user_id][\"session_count\"]\n",
    "\n",
    "            if user_info[user_id][\"count\"] != 0:\n",
    "                out[i, features_dict['prior_question_had_explanation_ratio']] = user_info[user_id][\"had_exp\"]/user_info[user_id][\"count\"]\n",
    "\n",
    "            if user_info[user_id][\"sessions\"] != 0:\n",
    "                out[i, features_dict['mean_pause']] = user_info[user_id][\"sum_pauses\"]/user_info[user_id][\"sessions\"]\n",
    "\n",
    "\n",
    "            #Easy ratio\n",
    "            nn = user_info.get(user_id, {}).get('easy_ct',0)\n",
    "            if nn != 0:\n",
    "                out[i, features_dict['easy_ratio']] = user_info.get(user_id, {}).get('easy_cr',0)/nn\n",
    "\n",
    "            out[i, features_dict['easy_ratio_opp']] = 1 - out[i, features_dict['easy_ratio']]\n",
    "\n",
    "\n",
    "            #Hard ratio\n",
    "            nn = user_info.get(user_id, {}).get('hard_ct',0)\n",
    "            if nn != 0:\n",
    "                out[i, features_dict['hard_ratio']] = user_info.get(user_id, {}).get('hard_cr',0)/nn\n",
    "\n",
    "            out[i, features_dict['hard_ratio_opp']] = 1 - out[i, features_dict['hard_ratio']]\n",
    "\n",
    "\n",
    "\n",
    "            nn = user_info.get(user_id, {}).get('part_count',[])\n",
    "            if nn != []: #User exists\n",
    "                part = int(out[i, features_dict['part']] - 1)\n",
    "                ct = nn[part]\n",
    "                cr = user_info[user_id]['part_corr'][part]\n",
    "\n",
    "                if ct != 0:\n",
    "                    out[i, features_dict['part_mean']] = cr/ct\n",
    "\n",
    "\n",
    "            #Rolling mean\n",
    "            if groups.get(user_id, -1) != -1 and groups[user_id] != []:\n",
    "\n",
    "\n",
    "                last_arr = np.array(groups[user_id][-40:])\n",
    "\n",
    "                out[i, features_dict['rolling_mean']] = numpy_ewma_vectorized_v2(np.array(last_arr[-30:]),5)[-1]  #Limit the length of groups array\n",
    "                \n",
    "                \n",
    "                last_arr = last_arr[last_arr != -1]  #Remove lecture data\n",
    "                out[i, features_dict['ewm_mean_10']] = numpy_ewma_vectorized_v2(np.array(last_arr[-30:]),5)[-1]\n",
    "\n",
    "                out[i, features_dict['rolling_mean_10']] = np.array(last_arr[-10:]).mean()\n",
    "                out[i, features_dict['rolling_mean_15']] = np.array(last_arr[-15:]).mean()\n",
    "                out[i, features_dict['rolling_mean_5']] = np.array(last_arr[-5:]).mean()\n",
    "\n",
    "\n",
    "            out[i, features_dict['prior_part_mean']] = prior_part_mean_dict[int(out[i, features_dict['part']])]\n",
    "\n",
    "            k = que_2_k[content_id]\n",
    "            out[i, features_dict['k']] = k\n",
    "            if user_info[user_id][\"k_count\"][k] != 0:\n",
    "                out[i, features_dict['k_acc']] = user_info[user_id][\"k_corr\"][k]/user_info[user_id][\"k_count\"][k]\n",
    "\n",
    "            \n",
    "            out[i, features_dict['wut']] = user_info[user_id][\"ts_diff_shifted\"] - elapsed*task_count\n",
    "            if out[i, features_dict['wut']] < 0:\n",
    "                out[i, features_dict['wut']] = 0\n",
    "                \n",
    "             \n",
    "            #Watched_n\n",
    "            usr = watched_tags.get(str(user_id), -1)\n",
    "            if usr != -1:\n",
    "\n",
    "                nn = 0\n",
    "\n",
    "                for k in range(6):\n",
    "                    nn += usr.get(str(int(questions1[content_id][\"tags\"+str(k+1)])), 0) \n",
    "\n",
    "                out[i, features_dict['watched']] = nn\n",
    "\n",
    "\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:07.559210Z",
     "iopub.status.busy": "2021-01-08T05:50:07.558606Z",
     "iopub.status.idle": "2021-01-08T05:50:10.363582Z",
     "shell.execute_reply": "2021-01-08T05:50:10.362394Z"
    },
    "papermill": {
     "duration": 2.836754,
     "end_time": "2021-01-08T05:50:10.363700",
     "exception": false,
     "start_time": "2021-01-08T05:50:07.526946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "model_lgbm = lgb.Booster(model_file='../input/lgbm-test/lgb_classifier.txt')\n",
    "stack_lgbm = lgb.Booster(model_file='../input/lgbm-test/lgb_stack.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:10.426073Z",
     "iopub.status.busy": "2021-01-08T05:50:10.424202Z",
     "iopub.status.idle": "2021-01-08T05:50:10.426788Z",
     "shell.execute_reply": "2021-01-08T05:50:10.427207Z"
    },
    "papermill": {
     "duration": 0.035244,
     "end_time": "2021-01-08T05:50:10.427321",
     "exception": false,
     "start_time": "2021-01-08T05:50:10.392077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vals = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:10.495617Z",
     "iopub.status.busy": "2021-01-08T05:50:10.494959Z",
     "iopub.status.idle": "2021-01-08T05:50:11.486756Z",
     "shell.execute_reply": "2021-01-08T05:50:11.488519Z"
    },
    "papermill": {
     "duration": 1.032858,
     "end_time": "2021-01-08T05:50:11.488698",
     "exception": false,
     "start_time": "2021-01-08T05:50:10.455840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 401 ms, sys: 88.2 ms, total: 489 ms\n",
      "Wall time: 988 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for (test_data,sample_prediction_df) in iter_test:\n",
    "    \n",
    "    if not isinstance(vals, int): #First value case\n",
    "        \n",
    "        #vals = vals[vals[:,2] != 1] #Remove lectures from old vals\n",
    "        \n",
    "        if test_data.iloc[0].prior_group_answers_correct == test_data.iloc[0].prior_group_answers_correct:\n",
    "            past_vals = np.array(ast.literal_eval(test_data.iloc[0].prior_group_answers_correct)) \n",
    "            past_answers = np.array(ast.literal_eval(test_data.iloc[0].prior_group_responses))\n",
    "            \n",
    "            past_vals = np.concatenate((vals, past_vals.reshape(len(past_vals),1)), axis=1)\n",
    "            past_vals = np.concatenate((past_vals, past_answers.reshape(len(past_answers),1)), axis=1)\n",
    "\n",
    "            update_group_var(past_vals)  #Update database with the vals of the last batch\n",
    "    \n",
    "    vals = test_data[[\"user_id\",\"content_id\", \"content_type_id\", \"task_container_id\",\"timestamp\",\"prior_question_elapsed_time\"]].values\n",
    "\n",
    "    test_transform = preprocess_1(test_data)\n",
    "    \n",
    "    \n",
    "    lgbm_predic = model_lgbm.predict(test_transform)\n",
    "    st_predic = np.array(split_preds(vals))\n",
    "    \n",
    "    \n",
    "    #test_transform = np.insert(test_transform, lgb_preds_idx, lgbm_predic, axis=1)\n",
    "    #test_transform = np.insert(test_transform, st_preds_idx, st_predic, axis=1)\n",
    "    \n",
    "    test_data['answered_correctly'] = (lgbm_predic*0.85 + 1.15*st_predic)/2 #stack_lgbm.predict(test_transform)\n",
    "    env.predict(test_data.loc[test_data['content_type_id'] == 0, ['row_id', 'answered_correctly']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T05:50:11.551488Z",
     "iopub.status.busy": "2021-01-08T05:50:11.550766Z",
     "iopub.status.idle": "2021-01-08T05:50:12.421601Z",
     "shell.execute_reply": "2021-01-08T05:50:12.421102Z"
    },
    "papermill": {
     "duration": 0.903975,
     "end_time": "2021-01-08T05:50:12.421709",
     "exception": false,
     "start_time": "2021-01-08T05:50:11.517734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id,answered_correctly\r\n",
      "0,0.3758300324146362\r\n",
      "1,0.8822475281339635\r\n",
      "2,0.3542358928615694\r\n",
      "3,0.860720291355968\r\n",
      "4,0.39390342440980325\r\n",
      "5,0.6541060538012191\r\n",
      "6,0.3642214811055272\r\n",
      "7,0.7486421731556245\r\n",
      "8,0.8322334797178218\r\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.028558,
     "end_time": "2021-01-08T05:50:12.478964",
     "exception": false,
     "start_time": "2021-01-08T05:50:12.450406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 61.788725,
   "end_time": "2021-01-08T05:50:14.298393",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-08T05:49:12.509668",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
