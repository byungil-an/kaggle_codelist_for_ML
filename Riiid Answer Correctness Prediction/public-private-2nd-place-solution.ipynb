{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-08T02:40:16.458224Z",
     "iopub.status.busy": "2021-01-08T02:40:16.457462Z",
     "iopub.status.idle": "2021-01-08T02:40:16.502245Z",
     "shell.execute_reply": "2021-01-08T02:40:16.502817Z"
    },
    "papermill": {
     "duration": 0.076879,
     "end_time": "2021-01-08T02:40:16.502971",
     "exception": false,
     "start_time": "2021-01-08T02:40:16.426092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/riiid-test-answer-prediction/example_sample_submission.csv\n",
      "/kaggle/input/riiid-test-answer-prediction/example_test.csv\n",
      "/kaggle/input/riiid-test-answer-prediction/questions.csv\n",
      "/kaggle/input/riiid-test-answer-prediction/train.csv\n",
      "/kaggle/input/riiid-test-answer-prediction/lectures.csv\n",
      "/kaggle/input/riiid-test-answer-prediction/riiideducation/competition.cpython-37m-x86_64-linux-gnu.so\n",
      "/kaggle/input/riiid-test-answer-prediction/riiideducation/__init__.py\n",
      "/kaggle/input/mamastan-gpu-v27/models/my_encoder_exp184_best.pth\n",
      "/kaggle/input/mamastan-gpu-v27/models/my_encoder_exp218_best.pth\n",
      "/kaggle/input/mamastan-gpu-v27/models/my_encoder_exp233_best.pth\n",
      "/kaggle/input/mamastan-gpu-v27/models/my_encoder_exp166_best.pth\n",
      "/kaggle/input/mamastan-gpu-v27/models/my_encoder_exp224_best.pth\n",
      "/kaggle/input/mamastan-gpu-v27/models/my_encoder_exp219_best.pth\n",
      "/kaggle/input/mamastan-gpu-v27/models/my_encoder_exp248_best.pth\n",
      "/kaggle/input/mamastan-gpu-v27/models/my_encoder_exp162_best.pth\n",
      "/kaggle/input/mamastan-gpu-v27/models/my_encoder_exp222_best.pth\n",
      "/kaggle/input/mamastan-gpu-v27/models/my_encoder_exp249_best.pth\n",
      "/kaggle/input/mamastan-gpu-v27/models/my_encoder_exp221_best.pth\n",
      "/kaggle/input/mamastan-gpu-v27/data/questions.csv\n",
      "/kaggle/input/mamastan-gpu-v27/data/lectures.csv\n",
      "/kaggle/input/mamastan-gpu-v27/references/lec_tags_rolling_count_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/n_samples_rolling_mean_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/task_container_id_diff_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/nn_user_answer_history_length_400_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/timestamp_diff_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/nn_content_id_history_length_400_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/nn_normed_log_timestamp_history_length_400_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/nn_normed_modified_timedelta_history_length_400_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/lec_part_rolling_count_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/positive_rolling_mean_for_prior_question_had_explanation_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/rolling_mean_sum_count_for_content_id_darkness_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/rolling_sum_for_prior_question_isnull_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/negative_rolling_mean_for_prior_question_had_explanation_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/content_type_id_diff_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/negative_rolling_mean_for_prior_question_elapsed_time_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/rolling_mean_for_prior_question_elapsed_time_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/rolling_mean_sum_count_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/rolling_mean_for_prior_question_had_explanation_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/nn_question_had_explanation_history_length_400_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/lec_type_of_rolling_count_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/nn_normed_elapsed_history_length_400_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/nn_task_container_id_history_length_400_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/norm_rolling_count_and_cut_for_user_answer_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/nn_content_type_id_diff_history_length_400_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/nn_correctness_history_length_400_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/rolling_mean_sum_count_for_6_tags_and_whole_tag_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/nn_task_container_id_diff_history_length_400_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/positive_rolling_mean_for_prior_question_elapsed_time_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/lec_rolling_count_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/references/rolling_mean_sum_count_for_part_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/others/que_proc_int.npy\n",
      "/kaggle/input/mamastan-gpu-v27/others/que_proc.pkl\n",
      "/kaggle/input/mamastan-gpu-v27/others/user_map_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/others/que_onehot.pkl\n",
      "/kaggle/input/mamastan-gpu-v27/others/que_part_onehot.pkl\n",
      "/kaggle/input/mamastan-gpu-v27/others/n_users_train.npy\n",
      "/kaggle/input/mamastan-gpu-v27/others/lec_map.npy\n",
      "/kaggle/input/mamastan-gpu-v27/others/lec_proc.pkl\n",
      "/kaggle/input/mamastan-gpu-v27/others/que_proc_2.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:40:16.565771Z",
     "iopub.status.busy": "2021-01-08T02:40:16.565011Z",
     "iopub.status.idle": "2021-01-08T02:40:19.498081Z",
     "shell.execute_reply": "2021-01-08T02:40:19.496840Z"
    },
    "papermill": {
     "duration": 2.971953,
     "end_time": "2021-01-08T02:40:19.498216",
     "exception": false,
     "start_time": "2021-01-08T02:40:16.526263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#libs\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import types\n",
    "import ast\n",
    "import gc\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        # module imports\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield name, val\n",
    "        # functions / callables\n",
    "        if hasattr(val, '__call__'):\n",
    "            yield name, val\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "noglobal = lambda fn: types.FunctionType(fn.__code__, dict(imports()))\n",
    "import collections\n",
    "from scipy.sparse import lil_matrix\n",
    "import scipy.sparse\n",
    "%load_ext Cython\n",
    "from itertools import chain\n",
    "from IPython.display import display, HTML\n",
    "import lightgbm as lgb\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'Mean of empty slice')\n",
    "\n",
    "#for GPU\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as torchdata\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:40:19.583370Z",
     "iopub.status.busy": "2021-01-08T02:40:19.581359Z",
     "iopub.status.idle": "2021-01-08T02:40:19.584049Z",
     "shell.execute_reply": "2021-01-08T02:40:19.584516Z"
    },
    "papermill": {
     "duration": 0.061718,
     "end_time": "2021-01-08T02:40:19.584635",
     "exception": false,
     "start_time": "2021-01-08T02:40:19.522917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#utils\n",
    "class RiiidEnv:\n",
    "    def __init__(self, sn, iterate_wo_predict = False):\n",
    "        self.sn = sn\n",
    "        self.sub = sn.loc[sn['content_type_id'] == 0][['row_id']].copy()\n",
    "        self.sub['answered_correctly'] = 0.5\n",
    "        self.can_yield = True\n",
    "        self.iterate_wo_predict = iterate_wo_predict\n",
    "        self.num_groups = self.sn.index.max() + 1\n",
    "        \n",
    "    def iter_test(self):\n",
    "        for i in range(self.num_groups):\n",
    "            self.i = i\n",
    "            assert(self.can_yield)\n",
    "            if not self.iterate_wo_predict:\n",
    "                self.can_yield = False\n",
    "\n",
    "            if i in self.sub.index:\n",
    "                yield self.sn.loc[[i]], self.sub.loc[[i]]\n",
    "            elif i not in self.sub.index:\n",
    "                yield self.sn.loc[[i]], None\n",
    "                    \n",
    "    def predict(self, my_sub):\n",
    "        assert(my_sub['row_id'].dtype == 'int64')\n",
    "        assert(my_sub['answered_correctly'].dtype == 'float64')\n",
    "        assert(my_sub.index.name == 'group_num')\n",
    "        assert(my_sub.index.dtype == 'int64')\n",
    "        \n",
    "        if self.i in self.sub.index:\n",
    "            assert(np.all(my_sub['answered_correctly'] >= 0))\n",
    "            assert(np.all(my_sub['answered_correctly'] <= 1))\n",
    "            assert(np.all(my_sub.index == self.i))\n",
    "            assert(np.all(my_sub['row_id'] == self.sub.loc[[self.i]]['row_id']))\n",
    "            self.sub.loc[[self.i]] = my_sub\n",
    "            self.can_yield = True\n",
    "            \n",
    "        elif self.i not in self.sub.index:\n",
    "            assert(my_sub.shape[0] == 0)\n",
    "            self.can_yield = True\n",
    "\n",
    "@noglobal\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, mode='wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "@noglobal\n",
    "def load_pickle(path):\n",
    "    with open(path, mode='rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "@noglobal\n",
    "def encode(train, column_name):\n",
    "    encoded = pd.merge(train[[column_name]], pd.DataFrame(train[column_name].unique(), columns=[column_name])\\\n",
    "                       .reset_index().dropna(), how='left', on=column_name)[['index']].rename(\n",
    "        columns={'index': column_name})\n",
    "    return encoded\n",
    "\n",
    "@noglobal\n",
    "def update_user_map(tes, user_map_ref, n_users_ref):\n",
    "    #new_users = tes[tes['timestamp'] == 0]['user_id'].unique()\n",
    "    users = tes['user_id'].unique()\n",
    "    keys = user_map_ref.keys()\n",
    "    new_users = users[np.array([user not in keys for user in users])]\n",
    "    n_new_users = new_users.shape[0]\n",
    "    if n_new_users > 0:\n",
    "        user_map_ref[new_users] = np.arange(n_users_ref, n_users_ref + n_new_users)\n",
    "    return user_map_ref, n_users_ref + n_new_users\n",
    "    \n",
    "@noglobal\n",
    "def write_to_ref_map(path, ref_name, f_names):\n",
    "    ref_map = load_pickle(path)\n",
    "    ref_map[ref_name] = f_names\n",
    "    save_pickle(ref_map, path)\n",
    "    \n",
    "class VectorizedDict():\n",
    "    def __init__(self):\n",
    "        self.tr_dict = dict()\n",
    "        self.set_value = np.vectorize(self.tr_dict.__setitem__)\n",
    "        self.get_value = np.vectorize(self.tr_dict.__getitem__)\n",
    "        \n",
    "    def keys(self):\n",
    "        return self.tr_dict.keys()\n",
    "        \n",
    "    def __setitem__(self, indices, values):\n",
    "        self.set_value(indices, values)\n",
    "    \n",
    "    def __getitem__(self, indices):\n",
    "        if indices.shape[0] == 0:\n",
    "            return np.array([], dtype=np.int32)\n",
    "        return self.get_value(indices)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:40:19.634817Z",
     "iopub.status.busy": "2021-01-08T02:40:19.633965Z",
     "iopub.status.idle": "2021-01-08T02:40:21.839266Z",
     "shell.execute_reply": "2021-01-08T02:40:21.840206Z"
    },
    "papermill": {
     "duration": 2.233087,
     "end_time": "2021-01-08T02:40:21.840449",
     "exception": false,
     "start_time": "2021-01-08T02:40:19.607362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%cython \n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef np.ndarray[int] cget_memory_indices(np.ndarray task):\n",
    "    \n",
    "    cdef Py_ssize_t n = task.shape[1]\n",
    "    cdef np.ndarray[int, ndim = 2] res = np.zeros_like(task, dtype = np.int32)\n",
    "    cdef np.ndarray[int] tmp_counter = np.full(task.shape[0], -1, dtype = np.int32)\n",
    "    cdef np.ndarray[int] u_counter = np.full(task.shape[0], task.shape[1] - 1, dtype = np.int32)\n",
    "    \n",
    "    for i in range(n):\n",
    "        res[:, i] = u_counter\n",
    "        tmp_counter += 1\n",
    "        if i != n - 1:\n",
    "            mask = (task[:, i] != task[:, i + 1])\n",
    "            u_counter[mask] = tmp_counter[mask]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:40:21.946509Z",
     "iopub.status.busy": "2021-01-08T02:40:21.924158Z",
     "iopub.status.idle": "2021-01-08T02:40:23.662124Z",
     "shell.execute_reply": "2021-01-08T02:40:23.660822Z"
    },
    "papermill": {
     "duration": 1.786921,
     "end_time": "2021-01-08T02:40:23.662243",
     "exception": false,
     "start_time": "2021-01-08T02:40:21.875322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#func_gpu\n",
    "@noglobal\n",
    "def nn_online_get_content_id_history(r_ref, tes, user_map_ref):\n",
    "    n_sample = r_ref.shape[1]\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    users = spc_tes['user_id'].values\n",
    "    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int16)\n",
    "    res[:, :n_sample] = r_ref[user_map_ref[users]]\n",
    "    res[:, n_sample] = spc_tes['content_id'].values\n",
    "    return pd.DataFrame(res, columns = ['nn_content_id_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n",
    "\n",
    "@noglobal\n",
    "def nn_update_reference_get_content_id_history(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n",
    "    content_ids = spc_prev_tes_cp['content_id'].values\n",
    "    for user, content_id in zip(enc_users, content_ids):\n",
    "        r_ref[user, :-1] = r_ref[user, 1:]\n",
    "        r_ref[user, -1] = content_id\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def nn_online_get_normed_log_timestamp_history(r_ref, tes, user_map_ref):\n",
    "    n_sample = r_ref.shape[1]\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    spc_tes['log_timestamp'] = np.log1p(spc_tes['timestamp'].values.astype(np.float32))\n",
    "    std = 3.3530\n",
    "    mean = 20.863\n",
    "    spc_tes['normed_log_timestamp'] = (spc_tes['log_timestamp'].values - mean)/std\n",
    "    users = spc_tes['user_id'].values\n",
    "    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n",
    "    res[:, :n_sample] = r_ref[user_map_ref[users]]\n",
    "    res[:, n_sample] = spc_tes['normed_log_timestamp'].values\n",
    "    return pd.DataFrame(res, columns = ['nn_normed_log_timestamp_history_' + str(i) + '_length_' + str(n_sample) \\\n",
    "                                        for i in range(n_sample + 1)])\n",
    "\n",
    "@noglobal\n",
    "def nn_update_reference_get_normed_log_timestamp_history(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n",
    "    timestamp = spc_prev_tes_cp['timestamp'].values.astype(np.float32)\n",
    "    std = 3.3530\n",
    "    mean = 20.863\n",
    "    normed_log_timestamps = (np.log1p(timestamp) - mean)/std\n",
    "    for user, normed_log_timestamp in zip(enc_users, normed_log_timestamps):\n",
    "        r_ref[user, :-1] = r_ref[user, 1:]\n",
    "        r_ref[user, -1] = normed_log_timestamp\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def nn_online_get_correctness_history(r_ref, tes, user_map_ref):\n",
    "    n_sample = r_ref.shape[1]\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    users = spc_tes['user_id'].values\n",
    "    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\n",
    "    res[:, :n_sample] = r_ref[user_map_ref[users]]\n",
    "    res[:, n_sample] = 2\n",
    "    return pd.DataFrame(res, columns = ['nn_correctness_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n",
    "\n",
    "@noglobal\n",
    "def nn_update_reference_get_correctness_history(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n",
    "    targets = spc_prev_tes_cp['answered_correctly'].values\n",
    "    for user, target in zip(enc_users, targets):\n",
    "        r_ref[user, :-1] = r_ref[user, 1:]\n",
    "        r_ref[user, -1] = target\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def nn_online_get_question_had_explanation_history(r_ref, tes, user_map_ref):\n",
    "    n_sample = r_ref.shape[1]\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    users = spc_tes['user_id'].values\n",
    "    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\n",
    "    res[:, :n_sample] = r_ref[user_map_ref[users]]\n",
    "    res[:, n_sample] = 2\n",
    "    return pd.DataFrame(res, columns = ['nn_question_had_explanation_history_' + str(i) + '_length_' + str(n_sample) \\\n",
    "                                        for i in range(n_sample + 1)])\n",
    "\n",
    "@noglobal\n",
    "def nn_early_update_reference_get_question_had_explanation_history(r_ref, tes, user_map_ref):\n",
    "    spc_tes_cp = tes[(tes['content_type_id'] == 0) \\\n",
    "                    & (~tes['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n",
    "    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n",
    "    explanations = spc_tes_cp['prior_question_had_explanation'].values.astype('int')\n",
    "    for explanation, idx in zip(explanations, row_idx):\n",
    "        r_ref[idx, r_ref[idx] == 2] = explanation\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def nn_update_reference_get_question_had_explanation_history(r_ref, prev_tes, tes, user_map_ref):\n",
    "    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 0].copy()\n",
    "    row_idx = user_map_ref[spc_prev_tes['user_id'].values]\n",
    "    for user in row_idx:\n",
    "        r_ref[user, :-1] = r_ref[user, 1:]\n",
    "        r_ref[user, -1] = 2\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def nn_online_get_normed_elapsed_history(r_ref, tes, user_map_ref):\n",
    "    n_sample = r_ref.shape[1]\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    users = spc_tes['user_id'].values\n",
    "    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n",
    "    res[:, :n_sample] = r_ref[user_map_ref[users]]\n",
    "    res[:, n_sample] = -999\n",
    "    return pd.DataFrame(res, columns = ['nn_normed_elapsed_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n",
    "\n",
    "@noglobal\n",
    "def nn_early_update_reference_get_normed_elapsed_history(r_ref, tes, user_map_ref):\n",
    "    spc_tes_cp = tes[(tes['content_type_id'] == 0) \\\n",
    "                    & (~tes['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n",
    "    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n",
    "    elapsed = spc_tes_cp['prior_question_elapsed_time'].values\n",
    "    clipped = np.clip(elapsed, 0, 1000 * 300).astype(np.float32)\n",
    "    mean = 25953\n",
    "    std = 20418\n",
    "    normed_elapsed = (clipped - mean)/std\n",
    "    for el, idx in zip(normed_elapsed, row_idx):\n",
    "        r_ref[idx, r_ref[idx] == -999] = el\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def nn_update_reference_get_normed_elapsed_history(r_ref, prev_tes, tes, user_map_ref):\n",
    "    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 0].copy()\n",
    "    row_idx = user_map_ref[spc_prev_tes['user_id'].values]\n",
    "    for user in row_idx:\n",
    "        r_ref[user, :-1] = r_ref[user, 1:]\n",
    "        r_ref[user, -1] = -999\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_modified_timedelta(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\n",
    "    spc_tes['count'] = np.bincount(enc)[enc]\n",
    "    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\n",
    "    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_timestamp_diff(r_ref, prev_tes, tes, user_map_ref):\n",
    "    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['timestamp'].values\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def nn_online_get_normed_modified_timedelta_history(r_ref, tes, f_tes_delta, user_map_ref):\n",
    "    n_sample = r_ref.shape[1]\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "\n",
    "    clipped = np.clip(f_tes_delta['modified_timedelta'].values, 0, 1000 * 800)\n",
    "    mean = 126568\n",
    "    std = 218000\n",
    "    clipped = (clipped - mean)/std\n",
    "    clipped[np.isnan(clipped)] = 0\n",
    "    spc_tes['normed_modified_timedelta'] = clipped.astype(np.float32)\n",
    "\n",
    "    users = spc_tes['user_id'].values\n",
    "    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n",
    "    res[:, :n_sample] = r_ref[user_map_ref[users]]\n",
    "    res[:, n_sample] = spc_tes['normed_modified_timedelta'].values\n",
    "    return pd.DataFrame(res, columns = ['nn_normed_modified_timedelta_history_' + str(i) + '_length_' + str(n_sample) \\\n",
    "                                        for i in range(n_sample + 1)])\n",
    "\n",
    "@noglobal\n",
    "def nn_update_reference_get_normed_modified_timedelta_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    \n",
    "    clipped = np.clip(f_tes_delta['modified_timedelta'].values, 0, 1000 * 800)\n",
    "    mean = 126568\n",
    "    std = 218000\n",
    "    clipped = (clipped - mean)/std\n",
    "    clipped[np.isnan(clipped)] = 0\n",
    "    spc_prev_tes_cp['normed_modified_timedelta'] = clipped.astype(np.float32)\n",
    "\n",
    "    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n",
    "    normed_modified_timedeltas = spc_prev_tes_cp['normed_modified_timedelta'].values\n",
    "    for user, normed_modified_timedelta in zip(enc_users, normed_modified_timedeltas):\n",
    "        r_ref[user, :-1] = r_ref[user, 1:]\n",
    "        r_ref[user, -1] = normed_modified_timedelta\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def nn_online_get_user_answer_history(r_ref, tes, user_map_ref):\n",
    "    n_sample = r_ref.shape[1]\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    users = spc_tes['user_id'].values\n",
    "    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\n",
    "    res[:, :n_sample] = r_ref[user_map_ref[users]]\n",
    "    res[:, n_sample] = 4\n",
    "    return pd.DataFrame(res, columns = ['nn_user_answer_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n",
    "\n",
    "@noglobal\n",
    "def nn_update_reference_get_user_answer_history(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n",
    "    targets = spc_prev_tes_cp['user_answer'].values\n",
    "    for user, target in zip(enc_users, targets):\n",
    "        r_ref[user, :-1] = r_ref[user, 1:]\n",
    "        r_ref[user, -1] = target\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_task_container_id_diff(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    res = spc_tes['task_container_id'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    return pd.DataFrame(res, columns = ['task_container_id_diff']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_task_container_id_diff(r_ref, prev_tes, tes, user_map_ref):\n",
    "    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['task_container_id'].values\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def nn_online_get_task_container_id_diff_history(r_ref, tes, f_tes_delta, user_map_ref):\n",
    "    n_sample = r_ref.shape[1]\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    value = f_tes_delta['task_container_id_diff'].values\n",
    "    value[np.isnan(value)] = 0\n",
    "    spc_tes['task_container_id_diff'] = value\n",
    "    \n",
    "    users = spc_tes['user_id'].values\n",
    "    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n",
    "    res[:, :n_sample] = r_ref[user_map_ref[users]]\n",
    "    res[:, n_sample] = spc_tes['task_container_id_diff'].values\n",
    "    return pd.DataFrame(res, columns = ['nn_task_container_id_diff_history_' + str(i) + '_length_' + str(n_sample) \\\n",
    "                                        for i in range(n_sample + 1)])\n",
    "\n",
    "@noglobal\n",
    "def nn_update_reference_get_task_container_id_diff_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    \n",
    "    value = f_tes_delta['task_container_id_diff'].values\n",
    "    value[np.isnan(value)] = 0\n",
    "    spc_prev_tes_cp['task_container_id_diff'] = value\n",
    "\n",
    "    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n",
    "    task_container_id_diffs = spc_prev_tes_cp['task_container_id_diff'].values\n",
    "    for user, task_container_id_diff in zip(enc_users, task_container_id_diffs):\n",
    "        r_ref[user, :-1] = r_ref[user, 1:]\n",
    "        r_ref[user, -1] = task_container_id_diff\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_content_type_id_diff(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    res = spc_tes['content_type_id'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    return pd.DataFrame(res, columns = ['content_type_id_diff']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_content_type_id_diff(r_ref, prev_tes, tes, user_map_ref):\n",
    "    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['content_type_id'].values\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def nn_online_get_content_type_id_diff_history(r_ref, tes, f_tes_delta, user_map_ref):\n",
    "    n_sample = r_ref.shape[1]\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    value = f_tes_delta['content_type_id_diff'].values\n",
    "    value[np.isnan(value)] = 0\n",
    "    spc_tes['content_type_id_diff'] = value\n",
    "    \n",
    "    users = spc_tes['user_id'].values\n",
    "    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n",
    "    res[:, :n_sample] = r_ref[user_map_ref[users]]\n",
    "    res[:, n_sample] = spc_tes['content_type_id_diff'].values\n",
    "    return pd.DataFrame(res, columns = ['nn_content_type_id_diff_history_' + str(i) + '_length_' + str(n_sample) \\\n",
    "                                        for i in range(n_sample + 1)])\n",
    "\n",
    "@noglobal\n",
    "def nn_update_reference_get_content_type_id_diff_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    \n",
    "    value = f_tes_delta['content_type_id_diff'].values\n",
    "    value[np.isnan(value)] = 0\n",
    "    spc_prev_tes_cp['content_type_id_diff'] = value\n",
    "\n",
    "    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n",
    "    content_type_id_diffs = spc_prev_tes_cp['content_type_id_diff'].values\n",
    "    for user, content_type_id_diff in zip(enc_users, content_type_id_diffs):\n",
    "        r_ref[user, :-1] = r_ref[user, 1:]\n",
    "        r_ref[user, -1] = content_type_id_diff\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def nn_online_get_task_container_id_history(r_ref, tes, user_map_ref):\n",
    "    n_sample = r_ref.shape[1]\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    users = spc_tes['user_id'].values\n",
    "    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int16)\n",
    "    res[:, :n_sample] = r_ref[user_map_ref[users]]\n",
    "    res[:, n_sample] = spc_tes['task_container_id'].values\n",
    "    return pd.DataFrame(res, columns = ['nn_task_container_id_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n",
    "\n",
    "@noglobal\n",
    "def nn_update_reference_get_task_container_id_history(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n",
    "    task_container_ids = spc_prev_tes_cp['task_container_id'].values\n",
    "    for user, task_container_id in zip(enc_users, task_container_ids):\n",
    "        r_ref[user, :-1] = r_ref[user, 1:]\n",
    "        r_ref[user, -1] = task_container_id\n",
    "    return r_ref\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, n_proj, n_dims):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.n_proj = n_proj\n",
    "        self.n_dims = n_dims\n",
    "        self.embed = nn.Embedding(n_proj, n_dims)\n",
    "\n",
    "    def forward(self, indices):\n",
    "        z = self.embed(indices)\n",
    "        return z\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MyEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(MyEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "\n",
    "    def forward(self, q, k, v, src_mask, src_key_padding_mask):\n",
    "        src2 = self.self_attn(q, k, v, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = q + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class MyEncoderExp162(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MyEncoderExp162, self).__init__()\n",
    "        #query, key and value\n",
    "        self.embedder_content_id = Embedder(13523, 512)\n",
    "        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n",
    "        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.norm1 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #key and value\n",
    "        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #MyEncoder\n",
    "        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 4 * args.n_conv, args.emb_dim * 4)\n",
    "        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n",
    "        \n",
    "        #conv\n",
    "        self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\n",
    "        self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\n",
    "        self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\n",
    "\n",
    "    def forward(self, batch, args):\n",
    "        #input\n",
    "        inputs_int = batch['inputs_int']\n",
    "        inputs_float = batch['inputs_float']\n",
    "        inputs_add = batch['inputs_add'].transpose(0, 1)\n",
    "        inputs_add_2 = batch['inputs_add_2']\n",
    "        N = inputs_int.shape[0]\n",
    "        n_length = int(inputs_int.shape[1]/2)\n",
    "\n",
    "        #for query, key and value\n",
    "        content_id = inputs_int[:, :, 0]\n",
    "        part = (inputs_int[:, :, 1] - 1)\n",
    "        tags = inputs_int[:, :, 2:8]\n",
    "        tag_mask = batch['tag_mask']\n",
    "        #digit_timedelta = inputs_int[:, :, 8]\n",
    "        normed_timedelta = inputs_float[:, :, 2]\n",
    "        normed_log_timestamp = inputs_float[:, :, 1]\n",
    "        correct_answer = inputs_int[:, :, 13]\n",
    "        task_container_id_diff = inputs_add_2[:, :, 0]\n",
    "        content_type_id_diff = inputs_add_2[:, :, 1]\n",
    "\n",
    "        #for key and value\n",
    "        explanation = inputs_int[:, :, 9]\n",
    "        correctness = inputs_int[:, :, 10]\n",
    "        normed_elapsed = inputs_float[:, :, 0]\n",
    "        user_answer = inputs_int[:, :, 11]\n",
    "        end_pos_idx = batch['end_pos_idx']\n",
    "\n",
    "        #mask\n",
    "        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n",
    "        padding_mask = batch['padding_mask']\n",
    "\n",
    "        #target, loss_mask\n",
    "        target = batch['target']\n",
    "        if 'loss_mask' in batch.keys():\n",
    "            loss_mask = batch['loss_mask']\n",
    "        else:\n",
    "            loss_mask = batch['cut_mask']\n",
    "            \n",
    "        # relative positional encoding\n",
    "        position = batch['position']\n",
    "            \n",
    "        #query, key and value\n",
    "        emb_content_id = self.embedder_content_id(content_id)\n",
    "        ohe_part = F.one_hot(part, num_classes = 7)\n",
    "        ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "        ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "        #key and value\n",
    "        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "        \n",
    "        #query process\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "        \n",
    "        #key and value process\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "\n",
    "        #transpose\n",
    "        query = query.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "        \n",
    "        #MyEncoder\n",
    "        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n",
    "        \n",
    "        #conv process\n",
    "        conv_inputs_int = batch['conv_inputs_int']\n",
    "        conv_inputs_float = batch['conv_inputs_float']\n",
    "        conv_inputs_const = batch['conv_inputs_const']\n",
    "\n",
    "        conv_explanation = conv_inputs_int[:, :, :, 0]\n",
    "        conv_correctness = conv_inputs_int[:, :, :, 1]\n",
    "        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n",
    "        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n",
    "        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n",
    "        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n",
    "        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n",
    "    \n",
    "        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n",
    "        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n",
    "        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n",
    "        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n",
    "\n",
    "        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n",
    "        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n",
    "        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n",
    "        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n",
    "        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n",
    "        for_conv = for_conv.contiguous().transpose(2, 3).view(N * n_length * 2, -1, args.n_conv)\n",
    "        out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_length * 2, N, -1)\n",
    "        \n",
    "        #concat \n",
    "        out = torch.cat([out, out_conv, inputs_add], dim = 2)\n",
    "        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 4 * args.n_conv)))))\\\n",
    "        .reshape(-1, N).transpose(0, 1)\n",
    "        score = torch.sigmoid(out)\n",
    "        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n",
    "        return err, score\n",
    "    \n",
    "    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n",
    "        torch.set_grad_enabled(False)\n",
    "        N = inputs['content_id'].shape[0]\n",
    "        scores = torch.zeros(N).to(args.device)\n",
    "        for i in range(0, N, pred_bs):\n",
    "            content_id = inputs['content_id'][i: i + pred_bs]\n",
    "            n_batch = content_id.shape[0]\n",
    "            n_sample = content_id.shape[1] - 1\n",
    "\n",
    "            padding_mask = (content_id == -1)[:, :n_sample]\n",
    "            token_idx = padding_mask[:, -1].copy()\n",
    "            padding_mask[:, -1] = False\n",
    "\n",
    "            part = que['part'].values[content_id] - 1\n",
    "            tags = que_proc_int[content_id]\n",
    "            tag_mask = (tags != 188)\n",
    "            correct_answer = que['correct_answer'].values[content_id]\n",
    "\n",
    "            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n",
    "            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n",
    "            explanation = inputs['explanation'][i: i + pred_bs]\n",
    "            correctness = inputs['correctness'][i: i + pred_bs]\n",
    "            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n",
    "            user_answer = inputs['user_answer'][i: i + pred_bs]\n",
    "            \n",
    "            #diff features\n",
    "            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n",
    "            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n",
    "            \n",
    "            #preprop\n",
    "            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n",
    "            content_id[content_id == -1] = 1\n",
    "            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n",
    "            token_idx = torch.from_numpy(token_idx).to(args.device)\n",
    "            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n",
    "            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n",
    "            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n",
    "            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n",
    "\n",
    "            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n",
    "            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n",
    "            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n",
    "            explanation[explanation == -2] = 1\n",
    "            explanation[explanation == 2] = 1\n",
    "            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n",
    "            correctness[correctness == -1] = 1\n",
    "            correctness[correctness == 2] = 1\n",
    "            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n",
    "            normed_elapsed[normed_elapsed == -999] = 0\n",
    "            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n",
    "            user_answer[user_answer == -1] = 1\n",
    "            user_answer[user_answer == 4] = 1\n",
    "            \n",
    "            #diff features\n",
    "            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n",
    "            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n",
    "\n",
    "            #generate token\n",
    "            explanation[token_idx, n_sample - 1] = 2\n",
    "            correctness[token_idx, n_sample - 1] = 2\n",
    "            normed_elapsed[token_idx, n_sample - 1] = 0\n",
    "            user_answer[token_idx, n_sample - 1] = 4\n",
    "\n",
    "            #for conv process\n",
    "            explanation[:, :-1][padding_mask] = 2\n",
    "            correctness[:, :-1][padding_mask] = 2\n",
    "            normed_elapsed[:, :-1][padding_mask] = 0\n",
    "            normed_timedelta[:, :-1][padding_mask] = 0\n",
    "            normed_log_timestamp[:, :-1][padding_mask] = 0\n",
    "\n",
    "            #query, key and value\n",
    "            emb_content_id = self.embedder_content_id(content_id)\n",
    "            ohe_part = F.one_hot(part, num_classes = 7)\n",
    "            ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "            ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "            #key and value\n",
    "            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "            # relative positional encoding\n",
    "            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n",
    "            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n",
    "\n",
    "            #query process\n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "            #key and value process\n",
    "            query_clone = query[:, :n_sample].clone()\n",
    "            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \n",
    "                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\n",
    "            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "\n",
    "            #transpose\n",
    "            query = query.transpose(0, 1)\n",
    "            memory = memory.transpose(0, 1)\n",
    "            spc_query = query[-1:]\n",
    "\n",
    "            #MyEncoder\n",
    "            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n",
    "            \n",
    "            #conv process\n",
    "            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n",
    "            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n",
    "            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n",
    "            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n",
    "            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n",
    "            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n",
    "            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n",
    "            \n",
    "            \n",
    "            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n",
    "            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n",
    "\n",
    "            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n",
    "            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n",
    "\n",
    "            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n",
    "            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n",
    "            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n",
    "            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n",
    "            const_normed_timedelta.unsqueeze(2)], dim = 2).transpose(1, 2)\n",
    "\n",
    "            out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_batch, - 1)\n",
    "\n",
    "            #features\n",
    "            features = inputs['features'][i: i + pred_bs].copy()\n",
    "            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n",
    "            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n",
    "            features[np.isnan(features)] = 0\n",
    "            features = torch.from_numpy(features).to(args.device)\n",
    "\n",
    "            #concat and last fc\n",
    "            out_cat = torch.cat([out, out_conv, features], dim = 1)\n",
    "            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n",
    "            score = torch.sigmoid(out_cat[:, 0])\n",
    "            scores[i: i + pred_bs] = score\n",
    "        return scores.cpu().numpy()\n",
    "    \n",
    "class MyEncoderExp166(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MyEncoderExp166, self).__init__()\n",
    "        #query, key and value\n",
    "        self.embedder_content_id = Embedder(13523, 512)\n",
    "        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n",
    "        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.norm1 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #key and value\n",
    "        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #MyEncoder\n",
    "        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 4 * args.n_conv, args.emb_dim * 4)\n",
    "        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n",
    "        \n",
    "        #conv\n",
    "        self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\n",
    "        self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\n",
    "        self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\n",
    "\n",
    "    def forward(self, batch, args):\n",
    "        #input\n",
    "        inputs_int = batch['inputs_int']\n",
    "        inputs_float = batch['inputs_float']\n",
    "        inputs_add = batch['inputs_add'].transpose(0, 1)\n",
    "        inputs_add_2 = batch['inputs_add_2']\n",
    "        N = inputs_int.shape[0]\n",
    "        n_length = int(inputs_int.shape[1]/2)\n",
    "\n",
    "        #for query, key and value\n",
    "        content_id = inputs_int[:, :, 0]\n",
    "        part = (inputs_int[:, :, 1] - 1)\n",
    "        tags = inputs_int[:, :, 2:8]\n",
    "        tag_mask = batch['tag_mask']\n",
    "        #digit_timedelta = inputs_int[:, :, 8]\n",
    "        normed_timedelta = inputs_float[:, :, 2]\n",
    "        normed_log_timestamp = inputs_float[:, :, 1]\n",
    "        correct_answer = inputs_int[:, :, 13]\n",
    "        task_container_id_diff = inputs_add_2[:, :, 0]\n",
    "        content_type_id_diff = inputs_add_2[:, :, 1]\n",
    "\n",
    "        #for key and value\n",
    "        explanation = inputs_int[:, :, 9]\n",
    "        correctness = inputs_int[:, :, 10]\n",
    "        normed_elapsed = inputs_float[:, :, 0]\n",
    "        user_answer = inputs_int[:, :, 11]\n",
    "        end_pos_idx = batch['end_pos_idx']\n",
    "\n",
    "        #mask\n",
    "        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n",
    "        padding_mask = batch['padding_mask']\n",
    "\n",
    "        #target, loss_mask\n",
    "        target = batch['target']\n",
    "        if 'loss_mask' in batch.keys():\n",
    "            loss_mask = batch['loss_mask']\n",
    "        else:\n",
    "            loss_mask = batch['cut_mask']\n",
    "            \n",
    "        # relative positional encoding\n",
    "        position = batch['position']\n",
    "            \n",
    "        #query, key and value\n",
    "        emb_content_id = self.embedder_content_id(content_id)\n",
    "        ohe_part = F.one_hot(part, num_classes = 7)\n",
    "        ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "        ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "        #key and value\n",
    "        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "        \n",
    "        #query process\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "        \n",
    "        #key and value process\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "\n",
    "        #transpose\n",
    "        query = query.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "        \n",
    "        #MyEncoder\n",
    "        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n",
    "        \n",
    "        #conv process\n",
    "        conv_inputs_int = batch['conv_inputs_int']\n",
    "        conv_inputs_float = batch['conv_inputs_float']\n",
    "        conv_inputs_const = batch['conv_inputs_const']\n",
    "\n",
    "        conv_explanation = conv_inputs_int[:, :, :, 0]\n",
    "        conv_correctness = conv_inputs_int[:, :, :, 1]\n",
    "        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n",
    "        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n",
    "        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n",
    "        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n",
    "        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n",
    "    \n",
    "        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n",
    "        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n",
    "        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n",
    "        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n",
    "\n",
    "        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n",
    "        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n",
    "        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n",
    "        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n",
    "        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n",
    "        for_conv = for_conv.contiguous().transpose(2, 3).view(N * n_length * 2, -1, args.n_conv)\n",
    "        out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_length * 2, N, -1)\n",
    "        \n",
    "        #concat \n",
    "        out = torch.cat([out, out_conv, inputs_add], dim = 2)\n",
    "        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 4 * args.n_conv)))))\\\n",
    "        .reshape(-1, N).transpose(0, 1)\n",
    "        score = torch.sigmoid(out)\n",
    "        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n",
    "        return err, score\n",
    "    \n",
    "    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n",
    "        torch.set_grad_enabled(False)\n",
    "        N = inputs['content_id'].shape[0]\n",
    "        scores = torch.zeros(N).to(args.device)\n",
    "        for i in range(0, N, pred_bs):\n",
    "            content_id = inputs['content_id'][i: i + pred_bs]\n",
    "            n_batch = content_id.shape[0]\n",
    "            n_sample = content_id.shape[1] - 1\n",
    "\n",
    "            padding_mask = (content_id == -1)[:, :n_sample]\n",
    "            token_idx = padding_mask[:, -1].copy()\n",
    "            padding_mask[:, -1] = False\n",
    "\n",
    "            part = que['part'].values[content_id] - 1\n",
    "            tags = que_proc_int[content_id]\n",
    "            tag_mask = (tags != 188)\n",
    "            correct_answer = que['correct_answer'].values[content_id]\n",
    "\n",
    "            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n",
    "            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n",
    "            explanation = inputs['explanation'][i: i + pred_bs]\n",
    "            correctness = inputs['correctness'][i: i + pred_bs]\n",
    "            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n",
    "            user_answer = inputs['user_answer'][i: i + pred_bs]\n",
    "            \n",
    "            #diff features\n",
    "            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n",
    "            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n",
    "            \n",
    "            #preprop\n",
    "            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n",
    "            content_id[content_id == -1] = 1\n",
    "            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n",
    "            token_idx = torch.from_numpy(token_idx).to(args.device)\n",
    "            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n",
    "            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n",
    "            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n",
    "            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n",
    "\n",
    "            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n",
    "            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n",
    "            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n",
    "            explanation[explanation == -2] = 1\n",
    "            explanation[explanation == 2] = 1\n",
    "            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n",
    "            correctness[correctness == -1] = 1\n",
    "            correctness[correctness == 2] = 1\n",
    "            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n",
    "            normed_elapsed[normed_elapsed == -999] = 0\n",
    "            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n",
    "            user_answer[user_answer == -1] = 1\n",
    "            user_answer[user_answer == 4] = 1\n",
    "            \n",
    "            #diff features\n",
    "            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n",
    "            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n",
    "\n",
    "            #generate token\n",
    "            explanation[token_idx, n_sample - 1] = 2\n",
    "            correctness[token_idx, n_sample - 1] = 2\n",
    "            normed_elapsed[token_idx, n_sample - 1] = 0\n",
    "            user_answer[token_idx, n_sample - 1] = 4\n",
    "\n",
    "            #for conv process\n",
    "            explanation[:, :-1][padding_mask] = 2\n",
    "            correctness[:, :-1][padding_mask] = 2\n",
    "            normed_elapsed[:, :-1][padding_mask] = 0\n",
    "            normed_timedelta[:, :-1][padding_mask] = 0\n",
    "            normed_log_timestamp[:, :-1][padding_mask] = 0\n",
    "\n",
    "            #query, key and value\n",
    "            emb_content_id = self.embedder_content_id(content_id)\n",
    "            ohe_part = F.one_hot(part, num_classes = 7)\n",
    "            ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "            ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "            #key and value\n",
    "            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "            # relative positional encoding\n",
    "            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n",
    "            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n",
    "\n",
    "            #query process\n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "            #key and value process\n",
    "            query_clone = query[:, :n_sample].clone()\n",
    "            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \n",
    "                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\n",
    "            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "\n",
    "            #transpose\n",
    "            query = query.transpose(0, 1)\n",
    "            memory = memory.transpose(0, 1)\n",
    "            spc_query = query[-1:]\n",
    "\n",
    "            #MyEncoder\n",
    "            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n",
    "            \n",
    "            #conv process\n",
    "            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n",
    "            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n",
    "            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n",
    "            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n",
    "            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n",
    "            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n",
    "            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n",
    "            \n",
    "            \n",
    "            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n",
    "            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n",
    "\n",
    "            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n",
    "            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n",
    "\n",
    "            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n",
    "            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n",
    "            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n",
    "            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n",
    "            const_normed_timedelta.unsqueeze(2)], dim = 2).transpose(1, 2)\n",
    "\n",
    "            out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_batch, - 1)\n",
    "\n",
    "            #features\n",
    "            features = inputs['features'][i: i + pred_bs].copy()\n",
    "            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n",
    "            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n",
    "            features[np.isnan(features)] = 0\n",
    "            features = torch.from_numpy(features).to(args.device)\n",
    "\n",
    "            #concat and last fc\n",
    "            out_cat = torch.cat([out, out_conv, features], dim = 1)\n",
    "            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n",
    "            score = torch.sigmoid(out_cat[:, 0])\n",
    "            scores[i: i + pred_bs] = score\n",
    "        return scores.cpu().numpy()\n",
    "    \n",
    "class MyEncoderExp184(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MyEncoderExp184, self).__init__()\n",
    "        #query, key and value\n",
    "        self.embedder_content_id = Embedder(13523, 512)\n",
    "        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n",
    "        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.norm1 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #key and value\n",
    "        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #MyEncoder\n",
    "        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 16 * 8, args.emb_dim * 4)\n",
    "        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n",
    "        \n",
    "        #conv\n",
    "#         self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\n",
    "#         self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\n",
    "#         self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\n",
    "        \n",
    "        #rnn\n",
    "        self.rnn = nn.GRU(13, 16, 8, batch_first = True)\n",
    "\n",
    "    def forward(self, batch, args):\n",
    "        #input\n",
    "        inputs_int = batch['inputs_int']\n",
    "        inputs_float = batch['inputs_float']\n",
    "        inputs_add = batch['inputs_add'].transpose(0, 1)\n",
    "        inputs_add_2 = batch['inputs_add_2']\n",
    "        N = inputs_int.shape[0]\n",
    "        n_length = int(inputs_int.shape[1]/2)\n",
    "\n",
    "        #for query, key and value\n",
    "        content_id = inputs_int[:, :, 0]\n",
    "        part = (inputs_int[:, :, 1] - 1)\n",
    "        tags = inputs_int[:, :, 2:8]\n",
    "        tag_mask = batch['tag_mask']\n",
    "        #digit_timedelta = inputs_int[:, :, 8]\n",
    "        normed_timedelta = inputs_float[:, :, 2]\n",
    "        normed_log_timestamp = inputs_float[:, :, 1]\n",
    "        correct_answer = inputs_int[:, :, 13]\n",
    "        task_container_id_diff = inputs_add_2[:, :, 0]\n",
    "        content_type_id_diff = inputs_add_2[:, :, 1]\n",
    "\n",
    "        #for key and value\n",
    "        explanation = inputs_int[:, :, 9]\n",
    "        correctness = inputs_int[:, :, 10]\n",
    "        normed_elapsed = inputs_float[:, :, 0]\n",
    "        user_answer = inputs_int[:, :, 11]\n",
    "        end_pos_idx = batch['end_pos_idx']\n",
    "\n",
    "        #mask\n",
    "        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n",
    "        padding_mask = batch['padding_mask']\n",
    "\n",
    "        #target, loss_mask\n",
    "        target = batch['target']\n",
    "        if 'loss_mask' in batch.keys():\n",
    "            loss_mask = batch['loss_mask']\n",
    "        else:\n",
    "            loss_mask = batch['cut_mask']\n",
    "            \n",
    "        # relative positional encoding\n",
    "        position = batch['position']\n",
    "            \n",
    "        #query, key and value\n",
    "        emb_content_id = self.embedder_content_id(content_id)\n",
    "        ohe_part = F.one_hot(part, num_classes = 7)\n",
    "        ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "        ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "        #key and value\n",
    "        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "        \n",
    "        #query process\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "        \n",
    "        #key and value process\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "\n",
    "        #transpose\n",
    "        query = query.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "        \n",
    "        #MyEncoder\n",
    "        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n",
    "        \n",
    "        #conv process\n",
    "        conv_inputs_int = batch['conv_inputs_int']\n",
    "        conv_inputs_float = batch['conv_inputs_float']\n",
    "        conv_inputs_const = batch['conv_inputs_const']\n",
    "\n",
    "        conv_explanation = conv_inputs_int[:, :, :, 0]\n",
    "        conv_correctness = conv_inputs_int[:, :, :, 1]\n",
    "        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n",
    "        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n",
    "        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n",
    "        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n",
    "        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n",
    "    \n",
    "        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n",
    "        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n",
    "        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n",
    "        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n",
    "\n",
    "        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n",
    "        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n",
    "        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n",
    "        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n",
    "        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n",
    "        \n",
    "        #rnn\n",
    "        for_rnn = for_conv.contiguous().view(N * n_length * 2, args.n_conv, -1)\n",
    "        _, out_rnn = self.rnn(for_rnn)\n",
    "        out_rnn = out_rnn.transpose(0, 1).contiguous().view(n_length *2, N, -1)\n",
    "\n",
    "        #cat\n",
    "        out = torch.cat([out, out_rnn, inputs_add], dim = 2)\n",
    "        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 16 * 8)))))\\\n",
    "        .reshape(-1, N).transpose(0, 1)\n",
    "        score = torch.sigmoid(out)\n",
    "        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n",
    "        return err, score\n",
    "    \n",
    "    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n",
    "        torch.set_grad_enabled(False)\n",
    "        N = inputs['content_id'].shape[0]\n",
    "        scores = torch.zeros(N).to(args.device)\n",
    "        for i in range(0, N, pred_bs):\n",
    "            content_id = inputs['content_id'][i: i + pred_bs]\n",
    "            n_batch = content_id.shape[0]\n",
    "            n_sample = content_id.shape[1] - 1\n",
    "\n",
    "            padding_mask = (content_id == -1)[:, :n_sample]\n",
    "            token_idx = padding_mask[:, -1].copy()\n",
    "            padding_mask[:, -1] = False\n",
    "\n",
    "            part = que['part'].values[content_id] - 1\n",
    "            tags = que_proc_int[content_id]\n",
    "            tag_mask = (tags != 188)\n",
    "            correct_answer = que['correct_answer'].values[content_id]\n",
    "\n",
    "            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n",
    "            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n",
    "            explanation = inputs['explanation'][i: i + pred_bs]\n",
    "            correctness = inputs['correctness'][i: i + pred_bs]\n",
    "            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n",
    "            user_answer = inputs['user_answer'][i: i + pred_bs]\n",
    "            \n",
    "            #diff features\n",
    "            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n",
    "            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n",
    "            \n",
    "            #preprop\n",
    "            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n",
    "            content_id[content_id == -1] = 1\n",
    "            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n",
    "            token_idx = torch.from_numpy(token_idx).to(args.device)\n",
    "            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n",
    "            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n",
    "            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n",
    "            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n",
    "\n",
    "            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n",
    "            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n",
    "            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n",
    "            explanation[explanation == -2] = 1\n",
    "            explanation[explanation == 2] = 1\n",
    "            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n",
    "            correctness[correctness == -1] = 1\n",
    "            correctness[correctness == 2] = 1\n",
    "            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n",
    "            normed_elapsed[normed_elapsed == -999] = 0\n",
    "            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n",
    "            user_answer[user_answer == -1] = 1\n",
    "            user_answer[user_answer == 4] = 1\n",
    "            \n",
    "            #diff features\n",
    "            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n",
    "            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n",
    "\n",
    "            #generate token\n",
    "            explanation[token_idx, n_sample - 1] = 2\n",
    "            correctness[token_idx, n_sample - 1] = 2\n",
    "            normed_elapsed[token_idx, n_sample - 1] = 0\n",
    "            user_answer[token_idx, n_sample - 1] = 4\n",
    "\n",
    "            #for conv process\n",
    "            explanation[:, :-1][padding_mask] = 2\n",
    "            correctness[:, :-1][padding_mask] = 2\n",
    "            normed_elapsed[:, :-1][padding_mask] = 0\n",
    "            normed_timedelta[:, :-1][padding_mask] = 0\n",
    "            normed_log_timestamp[:, :-1][padding_mask] = 0\n",
    "\n",
    "            #query, key and value\n",
    "            emb_content_id = self.embedder_content_id(content_id)\n",
    "            ohe_part = F.one_hot(part, num_classes = 7)\n",
    "            ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "            ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "            #key and value\n",
    "            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "            # relative positional encoding\n",
    "            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n",
    "            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n",
    "\n",
    "            #query process\n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "            #key and value process\n",
    "            query_clone = query[:, :n_sample].clone()\n",
    "            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \n",
    "                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\n",
    "            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "\n",
    "            #transpose\n",
    "            query = query.transpose(0, 1)\n",
    "            memory = memory.transpose(0, 1)\n",
    "            spc_query = query[-1:]\n",
    "\n",
    "            #MyEncoder\n",
    "            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n",
    "            \n",
    "            #conv process\n",
    "            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n",
    "            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n",
    "            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n",
    "            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n",
    "            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n",
    "            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n",
    "            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n",
    "            \n",
    "            \n",
    "            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n",
    "            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n",
    "\n",
    "            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n",
    "            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n",
    "\n",
    "            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n",
    "            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n",
    "            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n",
    "            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n",
    "            const_normed_timedelta.unsqueeze(2)], dim = 2)\n",
    "            \n",
    "            #rnn\n",
    "            for_rnn = for_conv\n",
    "            _, out_rnn = self.rnn(for_rnn)\n",
    "            out_rnn = out_rnn.transpose(0, 1).contiguous().view(n_batch, -1)\n",
    "\n",
    "            #features\n",
    "            features = inputs['features'][i: i + pred_bs].copy()\n",
    "            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n",
    "            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n",
    "            features[np.isnan(features)] = 0\n",
    "            features = torch.from_numpy(features).to(args.device)\n",
    "\n",
    "            #concat and last fc\n",
    "            out_cat = torch.cat([out, out_rnn, features], dim = 1)\n",
    "            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n",
    "            score = torch.sigmoid(out_cat[:, 0])\n",
    "            scores[i: i + pred_bs] = score\n",
    "        return scores.cpu().numpy()\n",
    "    \n",
    "class MyEncoderExp218(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MyEncoderExp218, self).__init__()\n",
    "        #query, key and value\n",
    "        self.embedder_content_id = Embedder(13523, 512)\n",
    "        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n",
    "        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.norm1 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #key and value\n",
    "        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #for rnn\n",
    "        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n",
    "        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.norm3 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm4 = nn.LayerNorm(args.emb_dim)\n",
    "        self.rnn = nn.GRU(512, 256, 4, batch_first = True, dropout = args.dropout)\n",
    "\n",
    "        #MyEncoder\n",
    "        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features, args.emb_dim * 4)\n",
    "        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n",
    "\n",
    "    def forward(self, batch, args):\n",
    "        #inputs\n",
    "        inputs_int = batch['inputs_int']\n",
    "        inputs_float = batch['inputs_float']\n",
    "        inputs_add = batch['inputs_add'].transpose(0, 1)\n",
    "        inputs_add_2 = batch['inputs_add_2']\n",
    "        N = inputs_int.shape[0]\n",
    "        n_length = int(inputs_int.shape[1]/2)\n",
    "\n",
    "        #for query, key and value\n",
    "        content_id = inputs_int[:, :, 0]\n",
    "        part = (inputs_int[:, :, 1] - 1)\n",
    "        tags = inputs_int[:, :, 2:8]\n",
    "        tag_mask = batch['tag_mask']\n",
    "        #digit_timedelta = inputs_int[:, :, 8]\n",
    "        normed_timedelta = inputs_float[:, :, 2]\n",
    "        normed_log_timestamp = inputs_float[:, :, 1]\n",
    "        correct_answer = inputs_int[:, :, 13]\n",
    "        task_container_id_diff = inputs_add_2[:, :, 0]\n",
    "        content_type_id_diff = inputs_add_2[:, :, 1]\n",
    "\n",
    "        #for key and value\n",
    "        explanation = inputs_int[:, :, 9]\n",
    "        correctness = inputs_int[:, :, 10]\n",
    "        normed_elapsed = inputs_float[:, :, 0]\n",
    "        user_answer = inputs_int[:, :, 11]\n",
    "        end_pos_idx = batch['end_pos_idx']\n",
    "\n",
    "        #mask\n",
    "        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n",
    "        padding_mask = batch['padding_mask']\n",
    "\n",
    "        #target, loss_mask\n",
    "        target = batch['target']\n",
    "        if 'loss_mask' in batch.keys():\n",
    "            loss_mask = batch['loss_mask']\n",
    "        else:\n",
    "            loss_mask = batch['cut_mask']\n",
    "\n",
    "        # relative positional encoding\n",
    "        position = batch['position']\n",
    "\n",
    "        #query, key and value\n",
    "        emb_content_id = self.embedder_content_id(content_id)\n",
    "        ohe_part = F.one_hot(part, num_classes = 7)\n",
    "        ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "        ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "        #initial query and memory \n",
    "        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "\n",
    "        #for rnn process\n",
    "        memory[padding_mask] = memory[padding_mask] * 0\n",
    "\n",
    "        tmp, _ = self.rnn(memory)\n",
    "        out_rnn = tmp.clone()\n",
    "        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n",
    "\n",
    "        memory_idx = batch['memory_idx']\n",
    "        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n",
    "        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n",
    "        memory_rnn = out_rnn[memory_idx_]\n",
    "\n",
    "        #new query and memory\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n",
    "        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n",
    "\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n",
    "\n",
    "        #transpose\n",
    "        query = query.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "\n",
    "        #MyEncoder\n",
    "        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n",
    "\n",
    "        #cat\n",
    "        out = torch.cat([out, inputs_add], dim = 2)\n",
    "        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features)))))\\\n",
    "        .reshape(-1, N).transpose(0, 1)\n",
    "        score = torch.sigmoid(out)\n",
    "        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n",
    "        return err, score\n",
    "    \n",
    "    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n",
    "        torch.set_grad_enabled(False)\n",
    "        N = inputs['content_id'].shape[0]\n",
    "        scores = torch.zeros(N).to(args.device)\n",
    "        for i in range(0, N, pred_bs):\n",
    "            content_id = inputs['content_id'][i: i + pred_bs]\n",
    "            n_batch = content_id.shape[0]\n",
    "            n_sample = content_id.shape[1] - 1\n",
    "\n",
    "            padding_mask = (content_id == -1)[:, :n_sample]\n",
    "            token_idx = padding_mask[:, -1].copy()\n",
    "            padding_mask[:, -1] = False\n",
    "\n",
    "            part = que['part'].values[content_id] - 1\n",
    "            tags = que_proc_int[content_id]\n",
    "            tag_mask = (tags != 188)\n",
    "            correct_answer = que['correct_answer'].values[content_id]\n",
    "\n",
    "            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n",
    "            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n",
    "            explanation = inputs['explanation'][i: i + pred_bs]\n",
    "            correctness = inputs['correctness'][i: i + pred_bs]\n",
    "            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n",
    "            user_answer = inputs['user_answer'][i: i + pred_bs]\n",
    "\n",
    "            #diff features\n",
    "            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n",
    "            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n",
    "\n",
    "            #preprop\n",
    "            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n",
    "            content_id[content_id == -1] = 1\n",
    "            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n",
    "            token_idx = torch.from_numpy(token_idx).to(args.device)\n",
    "            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n",
    "            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n",
    "            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n",
    "            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n",
    "\n",
    "            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n",
    "            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n",
    "            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n",
    "            explanation[explanation == -2] = 1\n",
    "            explanation[explanation == 2] = 1\n",
    "            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n",
    "            correctness[correctness == -1] = 1\n",
    "            correctness[correctness == 2] = 1\n",
    "            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n",
    "            normed_elapsed[normed_elapsed == -999] = 0\n",
    "            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n",
    "            user_answer[user_answer == -1] = 1\n",
    "            user_answer[user_answer == 4] = 1\n",
    "\n",
    "            #diff features\n",
    "            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n",
    "            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n",
    "\n",
    "            #generate token\n",
    "            explanation[token_idx, n_sample - 1] = 2\n",
    "            correctness[token_idx, n_sample - 1] = 2\n",
    "            normed_elapsed[token_idx, n_sample - 1] = 0\n",
    "            user_answer[token_idx, n_sample - 1] = 4\n",
    "\n",
    "            #for conv process\n",
    "            explanation[:, :-1][padding_mask] = 2\n",
    "            correctness[:, :-1][padding_mask] = 2\n",
    "            normed_elapsed[:, :-1][padding_mask] = 0\n",
    "            normed_timedelta[:, :-1][padding_mask] = 0\n",
    "            normed_log_timestamp[:, :-1][padding_mask] = 0\n",
    "\n",
    "            #query, key and value\n",
    "            emb_content_id = self.embedder_content_id(content_id)\n",
    "            ohe_part = F.one_hot(part, num_classes = 7)\n",
    "            ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "            ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "            #key and value\n",
    "            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "            # relative positional encoding\n",
    "            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n",
    "            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n",
    "\n",
    "            #query process\n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "            #key and value process\n",
    "            query_clone = query.clone()\n",
    "            query_clone[token_idx, -2] = 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n",
    "                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "            memory[:, :-1][padding_mask] = 0\n",
    "            memory[:, -1] = 0\n",
    "            out_rnn, _ = self.rnn(memory)\n",
    "            out_rnn[:, -1] = 0\n",
    "\n",
    "            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n",
    "            memory_idx = cget_memory_indices(task_container_id)\n",
    "            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n",
    "            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n",
    "            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n",
    "\n",
    "            #query process 2 \n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n",
    "            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n",
    "\n",
    "            #key and value process 2\n",
    "            query_clone = query.clone()\n",
    "            query_clone[token_idx, -2] = 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n",
    "                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n",
    "            memory = memory[:, :-1]\n",
    "\n",
    "            #transpose\n",
    "            query = query.transpose(0, 1)\n",
    "            memory = memory.transpose(0, 1)\n",
    "            spc_query = query[-1:]\n",
    "\n",
    "            #MyEncoder\n",
    "            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n",
    "\n",
    "            #features\n",
    "            features = inputs['features'][i: i + pred_bs].copy()\n",
    "            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n",
    "            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n",
    "            features[np.isnan(features)] = 0\n",
    "            features = torch.from_numpy(features).to(args.device)\n",
    "\n",
    "            #concat and last fc\n",
    "            out_cat = torch.cat([out, features], dim = 1)\n",
    "            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n",
    "            score = torch.sigmoid(out_cat[:, 0])\n",
    "            scores[i: i + pred_bs] = score\n",
    "        return scores.cpu().numpy()\n",
    "    \n",
    "class MyEncoderExp224(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MyEncoderExp224, self).__init__()\n",
    "        #query, key and value\n",
    "        self.embedder_content_id = Embedder(13523, 512)\n",
    "        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n",
    "        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.norm1 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #key and value\n",
    "        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #for rnn\n",
    "        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n",
    "        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.norm3 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm4 = nn.LayerNorm(args.emb_dim)\n",
    "        self.rnn = nn.LSTM(512, 256, 4, batch_first = True, dropout = args.dropout)\n",
    "\n",
    "        #MyEncoder\n",
    "        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256 + 16 * 8, args.emb_dim * 4)\n",
    "        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n",
    "        \n",
    "        self.fixlen_rnn = nn.GRU(13, 16, 8, batch_first = True)\n",
    "\n",
    "    def forward(self, batch, args):\n",
    "        #inputs\n",
    "        inputs_int = batch['inputs_int']\n",
    "        inputs_float = batch['inputs_float']\n",
    "        inputs_add = batch['inputs_add'].transpose(0, 1)\n",
    "        inputs_add_2 = batch['inputs_add_2']\n",
    "        N = inputs_int.shape[0]\n",
    "        n_length = int(inputs_int.shape[1]/2)\n",
    "\n",
    "        #for query, key and value\n",
    "        content_id = inputs_int[:, :, 0]\n",
    "        part = (inputs_int[:, :, 1] - 1)\n",
    "        tags = inputs_int[:, :, 2:8]\n",
    "        tag_mask = batch['tag_mask']\n",
    "        #digit_timedelta = inputs_int[:, :, 8]\n",
    "        normed_timedelta = inputs_float[:, :, 2]\n",
    "        normed_log_timestamp = inputs_float[:, :, 1]\n",
    "        correct_answer = inputs_int[:, :, 13]\n",
    "        task_container_id_diff = inputs_add_2[:, :, 0]\n",
    "        content_type_id_diff = inputs_add_2[:, :, 1]\n",
    "\n",
    "        #for key and value\n",
    "        explanation = inputs_int[:, :, 9]\n",
    "        correctness = inputs_int[:, :, 10]\n",
    "        normed_elapsed = inputs_float[:, :, 0]\n",
    "        user_answer = inputs_int[:, :, 11]\n",
    "        end_pos_idx = batch['end_pos_idx']\n",
    "\n",
    "        #mask\n",
    "        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n",
    "        padding_mask = batch['padding_mask']\n",
    "\n",
    "        #target, loss_mask\n",
    "        target = batch['target']\n",
    "        if 'loss_mask' in batch.keys():\n",
    "            loss_mask = batch['loss_mask']\n",
    "        else:\n",
    "            loss_mask = batch['cut_mask']\n",
    "\n",
    "        # relative positional encoding\n",
    "        position = batch['position']\n",
    "\n",
    "        #query, key and value\n",
    "        emb_content_id = self.embedder_content_id(content_id)\n",
    "        ohe_part = F.one_hot(part, num_classes = 7)\n",
    "        ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "        ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "        #initial query and memory \n",
    "        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "\n",
    "        #for rnn process\n",
    "        memory[padding_mask] = memory[padding_mask] * 0\n",
    "\n",
    "        tmp, _ = self.rnn(memory)\n",
    "        out_rnn = tmp.clone()\n",
    "        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n",
    "\n",
    "        memory_idx = batch['memory_idx']\n",
    "        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n",
    "        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n",
    "        memory_rnn = out_rnn[memory_idx_]\n",
    "\n",
    "        #new query and memory\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n",
    "        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n",
    "\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n",
    "\n",
    "        #transpose\n",
    "        query = query.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "\n",
    "        #MyEncoder\n",
    "        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n",
    "        \n",
    "        #conv process\n",
    "        conv_inputs_int = batch['conv_inputs_int']\n",
    "        conv_inputs_float = batch['conv_inputs_float']\n",
    "        conv_inputs_const = batch['conv_inputs_const']\n",
    "\n",
    "        conv_explanation = conv_inputs_int[:, :, :, 0]\n",
    "        conv_correctness = conv_inputs_int[:, :, :, 1]\n",
    "        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n",
    "        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n",
    "        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n",
    "        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n",
    "        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n",
    "    \n",
    "        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n",
    "        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n",
    "        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n",
    "        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n",
    "\n",
    "        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n",
    "        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n",
    "        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n",
    "        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n",
    "        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n",
    "        \n",
    "        #rnn\n",
    "        for_rnn = for_conv.contiguous().view(N * n_length * 2, args.n_conv, -1)\n",
    "        _, out_fixlen_rnn = self.fixlen_rnn(for_rnn)\n",
    "        out_fixlen_rnn = out_fixlen_rnn.transpose(0, 1).contiguous().view(n_length *2, N, -1)\n",
    "\n",
    "        #cat\n",
    "        out = torch.cat([out, out_fixlen_rnn, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\n",
    "        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256 + 16 * 8)))))\\\n",
    "        .reshape(-1, N).transpose(0, 1)\n",
    "        score = torch.sigmoid(out)\n",
    "        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n",
    "        return err, score\n",
    "    \n",
    "    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n",
    "        torch.set_grad_enabled(False)\n",
    "        N = inputs['content_id'].shape[0]\n",
    "        scores = torch.zeros(N).to(args.device)\n",
    "        for i in range(0, N, pred_bs):\n",
    "            content_id = inputs['content_id'][i: i + pred_bs]\n",
    "            n_batch = content_id.shape[0]\n",
    "            n_sample = content_id.shape[1] - 1\n",
    "\n",
    "            padding_mask = (content_id == -1)[:, :n_sample]\n",
    "            token_idx = padding_mask[:, -1].copy()\n",
    "            padding_mask[:, -1] = False\n",
    "\n",
    "            part = que['part'].values[content_id] - 1\n",
    "            tags = que_proc_int[content_id]\n",
    "            tag_mask = (tags != 188)\n",
    "            correct_answer = que['correct_answer'].values[content_id]\n",
    "\n",
    "            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n",
    "            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n",
    "            explanation = inputs['explanation'][i: i + pred_bs]\n",
    "            correctness = inputs['correctness'][i: i + pred_bs]\n",
    "            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n",
    "            user_answer = inputs['user_answer'][i: i + pred_bs]\n",
    "\n",
    "            #diff features\n",
    "            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n",
    "            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n",
    "\n",
    "            #preprop\n",
    "            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n",
    "            content_id[content_id == -1] = 1\n",
    "            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n",
    "            token_idx = torch.from_numpy(token_idx).to(args.device)\n",
    "            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n",
    "            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n",
    "            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n",
    "            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n",
    "\n",
    "            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n",
    "            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n",
    "            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n",
    "            explanation[explanation == -2] = 1\n",
    "            explanation[explanation == 2] = 1\n",
    "            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n",
    "            correctness[correctness == -1] = 1\n",
    "            correctness[correctness == 2] = 1\n",
    "            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n",
    "            normed_elapsed[normed_elapsed == -999] = 0\n",
    "            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n",
    "            user_answer[user_answer == -1] = 1\n",
    "            user_answer[user_answer == 4] = 1\n",
    "\n",
    "            #diff features\n",
    "            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n",
    "            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n",
    "\n",
    "            #generate token\n",
    "            explanation[token_idx, n_sample - 1] = 2\n",
    "            correctness[token_idx, n_sample - 1] = 2\n",
    "            normed_elapsed[token_idx, n_sample - 1] = 0\n",
    "            user_answer[token_idx, n_sample - 1] = 4\n",
    "\n",
    "            #for conv process\n",
    "            explanation[:, :-1][padding_mask] = 2\n",
    "            correctness[:, :-1][padding_mask] = 2\n",
    "            normed_elapsed[:, :-1][padding_mask] = 0\n",
    "            normed_timedelta[:, :-1][padding_mask] = 0\n",
    "            normed_log_timestamp[:, :-1][padding_mask] = 0\n",
    "\n",
    "            #query, key and value\n",
    "            emb_content_id = self.embedder_content_id(content_id)\n",
    "            ohe_part = F.one_hot(part, num_classes = 7)\n",
    "            ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "            ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "            #key and value\n",
    "            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "            # relative positional encoding\n",
    "            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n",
    "            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n",
    "\n",
    "            #query process\n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "            #key and value process\n",
    "            query_clone = query.clone()\n",
    "            query_clone[token_idx, -2] = 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n",
    "                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "            memory[:, :-1][padding_mask] = 0\n",
    "            memory[:, -1] = 0\n",
    "            out_rnn, _ = self.rnn(memory)\n",
    "            out_rnn[:, -1] = 0\n",
    "\n",
    "            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n",
    "            memory_idx = cget_memory_indices(task_container_id)\n",
    "            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n",
    "            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n",
    "            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n",
    "\n",
    "            #query process 2 \n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n",
    "            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n",
    "\n",
    "            #key and value process 2\n",
    "            query_clone = query.clone()\n",
    "            query_clone[token_idx, -2] = 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n",
    "                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n",
    "            memory = memory[:, :-1]\n",
    "\n",
    "            #transpose\n",
    "            query = query.transpose(0, 1)\n",
    "            memory = memory.transpose(0, 1)\n",
    "            spc_query = query[-1:]\n",
    "\n",
    "            #MyEncoder\n",
    "            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n",
    "            \n",
    "            #conv process\n",
    "            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n",
    "            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n",
    "            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n",
    "            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n",
    "            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n",
    "            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n",
    "            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n",
    "            \n",
    "            \n",
    "            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n",
    "            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n",
    "\n",
    "            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n",
    "            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n",
    "\n",
    "            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n",
    "            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n",
    "            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n",
    "            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n",
    "            const_normed_timedelta.unsqueeze(2)], dim = 2)\n",
    "            \n",
    "            #rnn\n",
    "            for_rnn = for_conv\n",
    "            _, out_fixlen_rnn = self.fixlen_rnn(for_rnn)\n",
    "            out_fixlen_rnn = out_fixlen_rnn.transpose(0, 1).contiguous().view(n_batch, -1)\n",
    "\n",
    "            #features\n",
    "            features = inputs['features'][i: i + pred_bs].copy()\n",
    "            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n",
    "            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n",
    "            features[np.isnan(features)] = 0\n",
    "            features = torch.from_numpy(features).to(args.device)\n",
    "\n",
    "            #concat and last fc\n",
    "            out_cat = torch.cat([out, out_fixlen_rnn, features, memory_rnn[:, -1]], dim = 1)\n",
    "            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n",
    "            score = torch.sigmoid(out_cat[:, 0])\n",
    "            scores[i: i + pred_bs] = score\n",
    "        return scores.cpu().numpy()\n",
    "    \n",
    "class MyEncoderExp219(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MyEncoderExp219, self).__init__()\n",
    "        #query, key and value\n",
    "        self.embedder_content_id = Embedder(13523, 512)\n",
    "        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n",
    "        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.norm1 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #key and value\n",
    "        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #for rnn\n",
    "        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n",
    "        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.norm3 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm4 = nn.LayerNorm(args.emb_dim)\n",
    "        self.rnn = nn.LSTM(512, 256, 4, batch_first = True)\n",
    "\n",
    "        #MyEncoder\n",
    "        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features, args.emb_dim * 4)\n",
    "        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n",
    "\n",
    "    def forward(self, batch, args):\n",
    "        #inputs\n",
    "        inputs_int = batch['inputs_int']\n",
    "        inputs_float = batch['inputs_float']\n",
    "        inputs_add = batch['inputs_add'].transpose(0, 1)\n",
    "        inputs_add_2 = batch['inputs_add_2']\n",
    "        N = inputs_int.shape[0]\n",
    "        n_length = int(inputs_int.shape[1]/2)\n",
    "\n",
    "        #for query, key and value\n",
    "        content_id = inputs_int[:, :, 0]\n",
    "        part = (inputs_int[:, :, 1] - 1)\n",
    "        tags = inputs_int[:, :, 2:8]\n",
    "        tag_mask = batch['tag_mask']\n",
    "        #digit_timedelta = inputs_int[:, :, 8]\n",
    "        normed_timedelta = inputs_float[:, :, 2]\n",
    "        normed_log_timestamp = inputs_float[:, :, 1]\n",
    "        correct_answer = inputs_int[:, :, 13]\n",
    "        task_container_id_diff = inputs_add_2[:, :, 0]\n",
    "        content_type_id_diff = inputs_add_2[:, :, 1]\n",
    "\n",
    "        #for key and value\n",
    "        explanation = inputs_int[:, :, 9]\n",
    "        correctness = inputs_int[:, :, 10]\n",
    "        normed_elapsed = inputs_float[:, :, 0]\n",
    "        user_answer = inputs_int[:, :, 11]\n",
    "        end_pos_idx = batch['end_pos_idx']\n",
    "\n",
    "        #mask\n",
    "        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n",
    "        padding_mask = batch['padding_mask']\n",
    "\n",
    "        #target, loss_mask\n",
    "        target = batch['target']\n",
    "        if 'loss_mask' in batch.keys():\n",
    "            loss_mask = batch['loss_mask']\n",
    "        else:\n",
    "            loss_mask = batch['cut_mask']\n",
    "\n",
    "        # relative positional encoding\n",
    "        position = batch['position']\n",
    "\n",
    "        #query, key and value\n",
    "        emb_content_id = self.embedder_content_id(content_id)\n",
    "        ohe_part = F.one_hot(part, num_classes = 7)\n",
    "        ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "        ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "        #initial query and memory \n",
    "        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "\n",
    "        #for rnn process\n",
    "        memory[padding_mask] = memory[padding_mask] * 0\n",
    "\n",
    "        tmp, _ = self.rnn(memory)\n",
    "        out_rnn = tmp.clone()\n",
    "        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n",
    "\n",
    "        memory_idx = batch['memory_idx']\n",
    "        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n",
    "        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n",
    "        memory_rnn = out_rnn[memory_idx_]\n",
    "\n",
    "        #new query and memory\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n",
    "        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n",
    "\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n",
    "\n",
    "        #transpose\n",
    "        query = query.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "\n",
    "        #MyEncoder\n",
    "        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n",
    "\n",
    "        #cat\n",
    "        out = torch.cat([out, inputs_add], dim = 2)\n",
    "        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features)))))\\\n",
    "        .reshape(-1, N).transpose(0, 1)\n",
    "        score = torch.sigmoid(out)\n",
    "        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n",
    "        return err, score\n",
    "    \n",
    "    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n",
    "        torch.set_grad_enabled(False)\n",
    "        N = inputs['content_id'].shape[0]\n",
    "        scores = torch.zeros(N).to(args.device)\n",
    "        for i in range(0, N, pred_bs):\n",
    "            content_id = inputs['content_id'][i: i + pred_bs]\n",
    "            n_batch = content_id.shape[0]\n",
    "            n_sample = content_id.shape[1] - 1\n",
    "\n",
    "            padding_mask = (content_id == -1)[:, :n_sample]\n",
    "            token_idx = padding_mask[:, -1].copy()\n",
    "            padding_mask[:, -1] = False\n",
    "\n",
    "            part = que['part'].values[content_id] - 1\n",
    "            tags = que_proc_int[content_id]\n",
    "            tag_mask = (tags != 188)\n",
    "            correct_answer = que['correct_answer'].values[content_id]\n",
    "\n",
    "            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n",
    "            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n",
    "            explanation = inputs['explanation'][i: i + pred_bs]\n",
    "            correctness = inputs['correctness'][i: i + pred_bs]\n",
    "            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n",
    "            user_answer = inputs['user_answer'][i: i + pred_bs]\n",
    "\n",
    "            #diff features\n",
    "            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n",
    "            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n",
    "\n",
    "            #preprop\n",
    "            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n",
    "            content_id[content_id == -1] = 1\n",
    "            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n",
    "            token_idx = torch.from_numpy(token_idx).to(args.device)\n",
    "            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n",
    "            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n",
    "            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n",
    "            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n",
    "\n",
    "            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n",
    "            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n",
    "            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n",
    "            explanation[explanation == -2] = 1\n",
    "            explanation[explanation == 2] = 1\n",
    "            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n",
    "            correctness[correctness == -1] = 1\n",
    "            correctness[correctness == 2] = 1\n",
    "            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n",
    "            normed_elapsed[normed_elapsed == -999] = 0\n",
    "            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n",
    "            user_answer[user_answer == -1] = 1\n",
    "            user_answer[user_answer == 4] = 1\n",
    "\n",
    "            #diff features\n",
    "            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n",
    "            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n",
    "\n",
    "            #generate token\n",
    "            explanation[token_idx, n_sample - 1] = 2\n",
    "            correctness[token_idx, n_sample - 1] = 2\n",
    "            normed_elapsed[token_idx, n_sample - 1] = 0\n",
    "            user_answer[token_idx, n_sample - 1] = 4\n",
    "\n",
    "            #for conv process\n",
    "            explanation[:, :-1][padding_mask] = 2\n",
    "            correctness[:, :-1][padding_mask] = 2\n",
    "            normed_elapsed[:, :-1][padding_mask] = 0\n",
    "            normed_timedelta[:, :-1][padding_mask] = 0\n",
    "            normed_log_timestamp[:, :-1][padding_mask] = 0\n",
    "\n",
    "            #query, key and value\n",
    "            emb_content_id = self.embedder_content_id(content_id)\n",
    "            ohe_part = F.one_hot(part, num_classes = 7)\n",
    "            ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "            ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "            #key and value\n",
    "            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "            # relative positional encoding\n",
    "            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n",
    "            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n",
    "\n",
    "            #query process\n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "            #key and value process\n",
    "            query_clone = query.clone()\n",
    "            query_clone[token_idx, -2] = 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n",
    "                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "            memory[:, :-1][padding_mask] = 0\n",
    "            memory[:, -1] = 0\n",
    "            out_rnn, _ = self.rnn(memory)\n",
    "            out_rnn[:, -1] = 0\n",
    "\n",
    "            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n",
    "            memory_idx = cget_memory_indices(task_container_id)\n",
    "            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n",
    "            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n",
    "            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n",
    "\n",
    "            #query process 2 \n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n",
    "            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n",
    "\n",
    "            #key and value process 2\n",
    "            query_clone = query.clone()\n",
    "            query_clone[token_idx, -2] = 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n",
    "                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n",
    "            memory = memory[:, :-1]\n",
    "\n",
    "            #transpose\n",
    "            query = query.transpose(0, 1)\n",
    "            memory = memory.transpose(0, 1)\n",
    "            spc_query = query[-1:]\n",
    "\n",
    "            #MyEncoder\n",
    "            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n",
    "\n",
    "            #features\n",
    "            features = inputs['features'][i: i + pred_bs].copy()\n",
    "            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n",
    "            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n",
    "            features[np.isnan(features)] = 0\n",
    "            features = torch.from_numpy(features).to(args.device)\n",
    "\n",
    "            #concat and last fc\n",
    "            out_cat = torch.cat([out, features], dim = 1)\n",
    "            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n",
    "            score = torch.sigmoid(out_cat[:, 0])\n",
    "            scores[i: i + pred_bs] = score\n",
    "        return scores.cpu().numpy()\n",
    "    \n",
    "class MyEncoderExp221(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MyEncoderExp221, self).__init__()\n",
    "        #query, key and value\n",
    "        self.embedder_content_id = Embedder(13523, 512)\n",
    "        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n",
    "        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.norm1 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #key and value\n",
    "        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #for rnn\n",
    "        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n",
    "        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.norm3 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm4 = nn.LayerNorm(args.emb_dim)\n",
    "        self.rnn = nn.GRU(512, 256, 4, batch_first = True)\n",
    "\n",
    "        #MyEncoder\n",
    "        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256, args.emb_dim * 4)\n",
    "        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n",
    "\n",
    "    def forward(self, batch, args):\n",
    "        #inputs\n",
    "        inputs_int = batch['inputs_int']\n",
    "        inputs_float = batch['inputs_float']\n",
    "        inputs_add = batch['inputs_add'].transpose(0, 1)\n",
    "        inputs_add_2 = batch['inputs_add_2']\n",
    "        N = inputs_int.shape[0]\n",
    "        n_length = int(inputs_int.shape[1]/2)\n",
    "\n",
    "        #for query, key and value\n",
    "        content_id = inputs_int[:, :, 0]\n",
    "        part = (inputs_int[:, :, 1] - 1)\n",
    "        tags = inputs_int[:, :, 2:8]\n",
    "        tag_mask = batch['tag_mask']\n",
    "        #digit_timedelta = inputs_int[:, :, 8]\n",
    "        normed_timedelta = inputs_float[:, :, 2]\n",
    "        normed_log_timestamp = inputs_float[:, :, 1]\n",
    "        correct_answer = inputs_int[:, :, 13]\n",
    "        task_container_id_diff = inputs_add_2[:, :, 0]\n",
    "        content_type_id_diff = inputs_add_2[:, :, 1]\n",
    "\n",
    "        #for key and value\n",
    "        explanation = inputs_int[:, :, 9]\n",
    "        correctness = inputs_int[:, :, 10]\n",
    "        normed_elapsed = inputs_float[:, :, 0]\n",
    "        user_answer = inputs_int[:, :, 11]\n",
    "        end_pos_idx = batch['end_pos_idx']\n",
    "\n",
    "        #mask\n",
    "        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n",
    "        padding_mask = batch['padding_mask']\n",
    "\n",
    "        #target, loss_mask\n",
    "        target = batch['target']\n",
    "        if 'loss_mask' in batch.keys():\n",
    "            loss_mask = batch['loss_mask']\n",
    "        else:\n",
    "            loss_mask = batch['cut_mask']\n",
    "\n",
    "        # relative positional encoding\n",
    "        position = batch['position']\n",
    "\n",
    "        #query, key and value\n",
    "        emb_content_id = self.embedder_content_id(content_id)\n",
    "        ohe_part = F.one_hot(part, num_classes = 7)\n",
    "        ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "        ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "        #initial query and memory \n",
    "        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "\n",
    "        #for rnn process\n",
    "        memory[padding_mask] = memory[padding_mask] * 0\n",
    "\n",
    "        tmp, _ = self.rnn(memory)\n",
    "        out_rnn = tmp.clone()\n",
    "        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n",
    "\n",
    "        memory_idx = batch['memory_idx']\n",
    "        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n",
    "        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n",
    "        memory_rnn = out_rnn[memory_idx_]\n",
    "\n",
    "        #new query and memory\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n",
    "        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n",
    "\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n",
    "\n",
    "        #transpose\n",
    "        query = query.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "\n",
    "        #MyEncoder\n",
    "        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n",
    "\n",
    "        #cat\n",
    "        out = torch.cat([out, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\n",
    "        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256)))))\\\n",
    "        .reshape(-1, N).transpose(0, 1)\n",
    "        score = torch.sigmoid(out)\n",
    "        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n",
    "        return err, score\n",
    "    \n",
    "    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n",
    "        torch.set_grad_enabled(False)\n",
    "        N = inputs['content_id'].shape[0]\n",
    "        scores = torch.zeros(N).to(args.device)\n",
    "        for i in range(0, N, pred_bs):\n",
    "            content_id = inputs['content_id'][i: i + pred_bs]\n",
    "            n_batch = content_id.shape[0]\n",
    "            n_sample = content_id.shape[1] - 1\n",
    "\n",
    "            padding_mask = (content_id == -1)[:, :n_sample]\n",
    "            token_idx = padding_mask[:, -1].copy()\n",
    "            padding_mask[:, -1] = False\n",
    "\n",
    "            part = que['part'].values[content_id] - 1\n",
    "            tags = que_proc_int[content_id]\n",
    "            tag_mask = (tags != 188)\n",
    "            correct_answer = que['correct_answer'].values[content_id]\n",
    "\n",
    "            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n",
    "            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n",
    "            explanation = inputs['explanation'][i: i + pred_bs]\n",
    "            correctness = inputs['correctness'][i: i + pred_bs]\n",
    "            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n",
    "            user_answer = inputs['user_answer'][i: i + pred_bs]\n",
    "\n",
    "            #diff features\n",
    "            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n",
    "            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n",
    "\n",
    "            #preprop\n",
    "            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n",
    "            content_id[content_id == -1] = 1\n",
    "            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n",
    "            token_idx = torch.from_numpy(token_idx).to(args.device)\n",
    "            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n",
    "            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n",
    "            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n",
    "            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n",
    "\n",
    "            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n",
    "            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n",
    "            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n",
    "            explanation[explanation == -2] = 1\n",
    "            explanation[explanation == 2] = 1\n",
    "            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n",
    "            correctness[correctness == -1] = 1\n",
    "            correctness[correctness == 2] = 1\n",
    "            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n",
    "            normed_elapsed[normed_elapsed == -999] = 0\n",
    "            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n",
    "            user_answer[user_answer == -1] = 1\n",
    "            user_answer[user_answer == 4] = 1\n",
    "\n",
    "            #diff features\n",
    "            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n",
    "            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n",
    "\n",
    "            #generate token\n",
    "            explanation[token_idx, n_sample - 1] = 2\n",
    "            correctness[token_idx, n_sample - 1] = 2\n",
    "            normed_elapsed[token_idx, n_sample - 1] = 0\n",
    "            user_answer[token_idx, n_sample - 1] = 4\n",
    "\n",
    "            #for conv process\n",
    "            explanation[:, :-1][padding_mask] = 2\n",
    "            correctness[:, :-1][padding_mask] = 2\n",
    "            normed_elapsed[:, :-1][padding_mask] = 0\n",
    "            normed_timedelta[:, :-1][padding_mask] = 0\n",
    "            normed_log_timestamp[:, :-1][padding_mask] = 0\n",
    "\n",
    "            #query, key and value\n",
    "            emb_content_id = self.embedder_content_id(content_id)\n",
    "            ohe_part = F.one_hot(part, num_classes = 7)\n",
    "            ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "            ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "            #key and value\n",
    "            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "            # relative positional encoding\n",
    "            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n",
    "            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n",
    "\n",
    "            #query process\n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "            #key and value process\n",
    "            query_clone = query.clone()\n",
    "            query_clone[token_idx, -2] = 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n",
    "                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "            memory[:, :-1][padding_mask] = 0\n",
    "            memory[:, -1] = 0\n",
    "            out_rnn, _ = self.rnn(memory)\n",
    "            out_rnn[:, -1] = 0\n",
    "\n",
    "            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n",
    "            memory_idx = cget_memory_indices(task_container_id)\n",
    "            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n",
    "            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n",
    "            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n",
    "\n",
    "            #query process 2 \n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n",
    "            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n",
    "\n",
    "            #key and value process 2\n",
    "            query_clone = query.clone()\n",
    "            query_clone[token_idx, -2] = 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n",
    "                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n",
    "            memory = memory[:, :-1]\n",
    "\n",
    "            #transpose\n",
    "            query = query.transpose(0, 1)\n",
    "            memory = memory.transpose(0, 1)\n",
    "            spc_query = query[-1:]\n",
    "\n",
    "            #MyEncoder\n",
    "            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n",
    "\n",
    "            #features\n",
    "            features = inputs['features'][i: i + pred_bs].copy()\n",
    "            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n",
    "            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n",
    "            features[np.isnan(features)] = 0\n",
    "            features = torch.from_numpy(features).to(args.device)\n",
    "\n",
    "            #concat and last fc\n",
    "            out_cat = torch.cat([out, features, memory_rnn[:, -1]], dim = 1)\n",
    "            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n",
    "            score = torch.sigmoid(out_cat[:, 0])\n",
    "            scores[i: i + pred_bs] = score\n",
    "        return scores.cpu().numpy()\n",
    "    \n",
    "class MyEncoderExp222(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MyEncoderExp222, self).__init__()\n",
    "        #query, key and value\n",
    "        self.embedder_content_id = Embedder(13523, 512)\n",
    "        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n",
    "        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.norm1 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #key and value\n",
    "        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        #for rnn\n",
    "        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n",
    "        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n",
    "        self.norm3 = nn.LayerNorm(args.emb_dim)\n",
    "\n",
    "        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n",
    "        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.norm4 = nn.LayerNorm(args.emb_dim)\n",
    "        self.rnn = nn.LSTM(512, 256, 4, batch_first = True, dropout = args.dropout)\n",
    "\n",
    "        #MyEncoder\n",
    "        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n",
    "                                                dropout = args.dropout)\n",
    "        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256, args.emb_dim * 4)\n",
    "        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n",
    "\n",
    "    def forward(self, batch, args):\n",
    "        #inputs\n",
    "        inputs_int = batch['inputs_int']\n",
    "        inputs_float = batch['inputs_float']\n",
    "        inputs_add = batch['inputs_add'].transpose(0, 1)\n",
    "        inputs_add_2 = batch['inputs_add_2']\n",
    "        N = inputs_int.shape[0]\n",
    "        n_length = int(inputs_int.shape[1]/2)\n",
    "\n",
    "        #for query, key and value\n",
    "        content_id = inputs_int[:, :, 0]\n",
    "        part = (inputs_int[:, :, 1] - 1)\n",
    "        tags = inputs_int[:, :, 2:8]\n",
    "        tag_mask = batch['tag_mask']\n",
    "        #digit_timedelta = inputs_int[:, :, 8]\n",
    "        normed_timedelta = inputs_float[:, :, 2]\n",
    "        normed_log_timestamp = inputs_float[:, :, 1]\n",
    "        correct_answer = inputs_int[:, :, 13]\n",
    "        task_container_id_diff = inputs_add_2[:, :, 0]\n",
    "        content_type_id_diff = inputs_add_2[:, :, 1]\n",
    "\n",
    "        #for key and value\n",
    "        explanation = inputs_int[:, :, 9]\n",
    "        correctness = inputs_int[:, :, 10]\n",
    "        normed_elapsed = inputs_float[:, :, 0]\n",
    "        user_answer = inputs_int[:, :, 11]\n",
    "        end_pos_idx = batch['end_pos_idx']\n",
    "\n",
    "        #mask\n",
    "        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n",
    "        padding_mask = batch['padding_mask']\n",
    "\n",
    "        #target, loss_mask\n",
    "        target = batch['target']\n",
    "        if 'loss_mask' in batch.keys():\n",
    "            loss_mask = batch['loss_mask']\n",
    "        else:\n",
    "            loss_mask = batch['cut_mask']\n",
    "\n",
    "        # relative positional encoding\n",
    "        position = batch['position']\n",
    "\n",
    "        #query, key and value\n",
    "        emb_content_id = self.embedder_content_id(content_id)\n",
    "        ohe_part = F.one_hot(part, num_classes = 7)\n",
    "        ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "        ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "        #initial query and memory \n",
    "        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "\n",
    "        #for rnn process\n",
    "        memory[padding_mask] = memory[padding_mask] * 0\n",
    "\n",
    "        tmp, _ = self.rnn(memory)\n",
    "        out_rnn = tmp.clone()\n",
    "        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n",
    "\n",
    "        memory_idx = batch['memory_idx']\n",
    "        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n",
    "        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n",
    "        memory_rnn = out_rnn[memory_idx_]\n",
    "\n",
    "        #new query and memory\n",
    "        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n",
    "        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n",
    "\n",
    "        query_clone = query.clone()\n",
    "        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n",
    "        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n",
    "\n",
    "        #transpose\n",
    "        query = query.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "\n",
    "        #MyEncoder\n",
    "        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n",
    "        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n",
    "\n",
    "        #cat\n",
    "        out = torch.cat([out, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\n",
    "        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256)))))\\\n",
    "        .reshape(-1, N).transpose(0, 1)\n",
    "        score = torch.sigmoid(out)\n",
    "        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n",
    "        return err, score\n",
    "    \n",
    "    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n",
    "        torch.set_grad_enabled(False)\n",
    "        N = inputs['content_id'].shape[0]\n",
    "        scores = torch.zeros(N).to(args.device)\n",
    "        for i in range(0, N, pred_bs):\n",
    "            content_id = inputs['content_id'][i: i + pred_bs]\n",
    "            n_batch = content_id.shape[0]\n",
    "            n_sample = content_id.shape[1] - 1\n",
    "\n",
    "            padding_mask = (content_id == -1)[:, :n_sample]\n",
    "            token_idx = padding_mask[:, -1].copy()\n",
    "            padding_mask[:, -1] = False\n",
    "\n",
    "            part = que['part'].values[content_id] - 1\n",
    "            tags = que_proc_int[content_id]\n",
    "            tag_mask = (tags != 188)\n",
    "            correct_answer = que['correct_answer'].values[content_id]\n",
    "\n",
    "            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n",
    "            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n",
    "            explanation = inputs['explanation'][i: i + pred_bs]\n",
    "            correctness = inputs['correctness'][i: i + pred_bs]\n",
    "            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n",
    "            user_answer = inputs['user_answer'][i: i + pred_bs]\n",
    "\n",
    "            #diff features\n",
    "            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n",
    "            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n",
    "\n",
    "            #preprop\n",
    "            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n",
    "            content_id[content_id == -1] = 1\n",
    "            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n",
    "            token_idx = torch.from_numpy(token_idx).to(args.device)\n",
    "            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n",
    "            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n",
    "            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n",
    "            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n",
    "\n",
    "            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n",
    "            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n",
    "            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n",
    "            explanation[explanation == -2] = 1\n",
    "            explanation[explanation == 2] = 1\n",
    "            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n",
    "            correctness[correctness == -1] = 1\n",
    "            correctness[correctness == 2] = 1\n",
    "            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n",
    "            normed_elapsed[normed_elapsed == -999] = 0\n",
    "            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n",
    "            user_answer[user_answer == -1] = 1\n",
    "            user_answer[user_answer == 4] = 1\n",
    "\n",
    "            #diff features\n",
    "            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n",
    "            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n",
    "\n",
    "            #generate token\n",
    "            explanation[token_idx, n_sample - 1] = 2\n",
    "            correctness[token_idx, n_sample - 1] = 2\n",
    "            normed_elapsed[token_idx, n_sample - 1] = 0\n",
    "            user_answer[token_idx, n_sample - 1] = 4\n",
    "\n",
    "            #for conv process\n",
    "            explanation[:, :-1][padding_mask] = 2\n",
    "            correctness[:, :-1][padding_mask] = 2\n",
    "            normed_elapsed[:, :-1][padding_mask] = 0\n",
    "            normed_timedelta[:, :-1][padding_mask] = 0\n",
    "            normed_log_timestamp[:, :-1][padding_mask] = 0\n",
    "\n",
    "            #query, key and value\n",
    "            emb_content_id = self.embedder_content_id(content_id)\n",
    "            ohe_part = F.one_hot(part, num_classes = 7)\n",
    "            ohe_tags = F.one_hot(tags, num_classes = 189)\n",
    "            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n",
    "            ohe_tags_sum = ohe_tags.sum(dim = 2)\n",
    "            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n",
    "\n",
    "            #key and value\n",
    "            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n",
    "            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n",
    "            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n",
    "\n",
    "            # relative positional encoding\n",
    "            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n",
    "            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n",
    "\n",
    "            #query process\n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n",
    "            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n",
    "\n",
    "            #key and value process\n",
    "            query_clone = query.clone()\n",
    "            query_clone[token_idx, -2] = 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n",
    "                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n",
    "            memory[:, :-1][padding_mask] = 0\n",
    "            memory[:, -1] = 0\n",
    "            out_rnn, _ = self.rnn(memory)\n",
    "            out_rnn[:, -1] = 0\n",
    "\n",
    "            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n",
    "            memory_idx = cget_memory_indices(task_container_id)\n",
    "            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n",
    "            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n",
    "            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n",
    "\n",
    "            #query process 2 \n",
    "            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n",
    "                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n",
    "                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n",
    "                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n",
    "            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n",
    "\n",
    "            #key and value process 2\n",
    "            query_clone = query.clone()\n",
    "            query_clone[token_idx, -2] = 0\n",
    "            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n",
    "                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n",
    "            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n",
    "            memory = memory[:, :-1]\n",
    "\n",
    "            #transpose\n",
    "            query = query.transpose(0, 1)\n",
    "            memory = memory.transpose(0, 1)\n",
    "            spc_query = query[-1:]\n",
    "\n",
    "            #MyEncoder\n",
    "            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n",
    "            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n",
    "\n",
    "            #features\n",
    "            features = inputs['features'][i: i + pred_bs].copy()\n",
    "            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n",
    "            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n",
    "            features[np.isnan(features)] = 0\n",
    "            features = torch.from_numpy(features).to(args.device)\n",
    "\n",
    "            #concat and last fc\n",
    "            out_cat = torch.cat([out, features, memory_rnn[:, -1]], dim = 1)\n",
    "            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n",
    "            score = torch.sigmoid(out_cat[:, 0])\n",
    "            scores[i: i + pred_bs] = score\n",
    "        return scores.cpu().numpy()\n",
    "\n",
    "#func_cpu\n",
    "@noglobal\n",
    "def online_get_user_id_content_id_task_container_id(tes):\n",
    "    f_tes = tes[tes['content_type_id'] == 0][['user_id', \n",
    "                                           'content_id', 'task_container_id']].astype('float32').reset_index(drop = True)\n",
    "    return f_tes\n",
    "\n",
    "@noglobal\n",
    "def online_get_timestamp_and_prior_question_elapsed_time_and_prior_question_had_explanation(tes):\n",
    "    f_tes = tes[tes['content_type_id'] == 0][['timestamp', \n",
    "                                              'prior_question_elapsed_time', 'prior_question_had_explanation']].astype('float32').reset_index(drop = True)\n",
    "    return f_tes\n",
    "\n",
    "@noglobal\n",
    "def online_get_part(tes, que):\n",
    "    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    f_tes = pd.DataFrame(que['part'].values[spc_tes_cp['content_id'].values], columns = ['part']).astype(np.float32)\n",
    "    return f_tes\n",
    "\n",
    "@noglobal\n",
    "def online_get_correct_answer(tes, que):\n",
    "    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    f_tes = pd.DataFrame(que['correct_answer'].values[spc_tes_cp['content_id'].values], columns = ['correct_answer']).astype(np.float32)\n",
    "    return f_tes\n",
    "\n",
    "@noglobal\n",
    "def online_get_modified_timedelta(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\n",
    "    spc_tes['count'] = np.bincount(enc)[enc]\n",
    "    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\n",
    "    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def online_get_tags(tes, que_proc_2):\n",
    "    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    return pd.DataFrame(que_proc_2.values[spc_tes_cp['content_id'].values], columns = ['tags_' + str(i) for i in range(6)])\n",
    "\n",
    "@noglobal\n",
    "def online_get_hell_rolling_mean_for_tags(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    sum_count = r_ref[:, :-1][user_map_ref[spc_tes['user_id'].values]]\n",
    "    f_tes = pd.DataFrame(sum_count[:, :, 0]/sum_count[:, :, 1], columns = ['hell_rolling_mean_for_tag_' + str(i) for i in range(188)])\n",
    "    return f_tes\n",
    "\n",
    "@noglobal\n",
    "def online_get_rolling_mean_sum_count(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    mean = sum_count[:, 0]/sum_count[:, 1]\n",
    "    res = np.zeros((spc_tes.shape[0], 3), dtype = np.float32)\n",
    "    res[:, 0] = mean\n",
    "    res[:, 1:] = sum_count\n",
    "    res = np.zeros((spc_tes.shape[0], 3), dtype = np.float32)\n",
    "    res[:, 0] = mean\n",
    "    res[:, 1:] = sum_count\n",
    "    return pd.DataFrame(res, columns = ['target_full_mean', 'target_full_sum', 'target_count'])\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_rolling_mean_sum_count(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    cts = spc_prev_tes_cp['user_id'].value_counts()\n",
    "    pos_cts = spc_prev_tes_cp['user_id'][spc_prev_tes_cp['answered_correctly'] == 1].value_counts()\n",
    "    r_ref[user_map_ref[pos_cts.index], 0] += pos_cts.values\n",
    "    r_ref[user_map_ref[cts.index], 1] += cts.values\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref, tes, que_proc, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    col_idx = que_proc.values[spc_tes['content_id'].values]\n",
    "    users = user_map_ref[spc_tes['user_id'].values]\n",
    "    row_idx = np.repeat(users[:, None], 6, axis = 1)\n",
    "\n",
    "    res = np.zeros((spc_tes.shape[0], 7, 3), dtype = np.float32)\n",
    "    sliced = r_ref[row_idx, col_idx]\n",
    "    res[:, :6, 1:] = sliced\n",
    "    res[:, 6, 1:] = np.nansum(sliced, axis = 1)\n",
    "    res[:, :, 0] = res[:, :, 1]/res[:, :, 2]\n",
    "    res = res.reshape(-1, 3 * 7)\n",
    "    f_names = sum([['tags_order_mean_' + str(i), 'tags_order_sum_' + str(i), 'tags_order_count_' + str(i)] for i in range(6)], [])\n",
    "    f_names += ['whole_tags_order_mean', 'whole_tags_order_sum', 'whole_tags_order_count']\n",
    "    return pd.DataFrame(res, columns = f_names)\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref, prev_tes, tes, que_onehot, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "\n",
    "    for_sum = que_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\n",
    "    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values[spc_prev_tes_cp['answered_correctly'] == 1], return_inverse = True)\n",
    "    sum_cts = np.array([np.bincount(enc, weights = for_sum[:, i]) for i in range(188)]).T\n",
    "    r_ref[user_map_ref[uniq], :-1, 0] += sum_cts\n",
    "\n",
    "    for_count = que_onehot.values[spc_prev_tes_cp['content_id']]\n",
    "    uniq, enc = np.unique(spc_prev_tes_cp['user_id'], return_inverse = True)\n",
    "    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(188)]).T\n",
    "    r_ref[user_map_ref[uniq], :-1, 1] += cts\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_rolling_mean_sum_count_for_part(r_ref, tes, que, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    spc_tes['part'] = que['part'].values[spc_tes['content_id'].values]\n",
    "    res = np.zeros((spc_tes.shape[0], 8, 3), dtype = np.float32)\n",
    "    res[:, :7, 1:] = r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    res[:, 7, 1:] = r_ref[user_map_ref[spc_tes['user_id'].values], spc_tes['part'] - 1]\n",
    "    res[:, :, 0] = res[:, :, 1]/res[:, :, 2]\n",
    "    res = res.reshape(-1, 3 * 8)\n",
    "    f_names = sum([['part_' + str(i) + '_mean', 'part_' + str(i) + '_sum', 'part_' + str(i) + '_count'] for i in range(1, 8)], []) + \\\n",
    "    ['part_cut_mean', 'part_cut_sum', 'part_cut_count']\n",
    "    return pd.DataFrame(res, columns = f_names)\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_rolling_mean_sum_count_for_part(r_ref, prev_tes, tes, que_part_onehot, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    \n",
    "    for_sum = que_part_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\n",
    "    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values[spc_prev_tes_cp['answered_correctly'] == 1], return_inverse = True)\n",
    "    sum_cts = np.array([np.bincount(enc, weights = for_sum[:, i]) for i in range(7)]).T\n",
    "    r_ref[user_map_ref[uniq], :, 0] += sum_cts\n",
    "    \n",
    "    for_count = que_part_onehot.values[spc_prev_tes_cp['content_id']]\n",
    "    uniq, enc = np.unique(spc_prev_tes_cp['user_id'], return_inverse = True)\n",
    "    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(7)]).T\n",
    "    r_ref[user_map_ref[uniq], :, 1] += cts\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_lec_rolling_count(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    return pd.DataFrame(r_ref[user_map_ref[spc_tes['user_id'].values]], columns = ['lec_rolling_count'])\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_lec_rolling_count(r_ref, prev_tes, tes, user_map_ref):\n",
    "    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\n",
    "    r_ref[user_map_ref[spc_prev_tes['user_id'].values]] += 1\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_lec_part_rolling_count(r_ref, tes, user_map_ref, que):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    res = np.zeros((spc_tes.shape[0], 8), dtype = np.float32)\n",
    "    part_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    res[:, :7] = part_count\n",
    "    res[:, 7] = part_count[np.arange(spc_tes.shape[0]), que['part'].values[spc_tes['content_id'].values] - 1]\n",
    "    return pd.DataFrame(res, columns = ['lec_part_' + str(i + 1) for i in range(7)] + ['lec_part_cut'])\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_lec_part_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec, lec_map):\n",
    "    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\n",
    "    row_idx = user_map_ref[spc_prev_tes['user_id']]\n",
    "    col_idx = lec['part'].values[lec_map[spc_prev_tes['content_id'].values]] - 1\n",
    "    r_ref[row_idx, col_idx] += 1\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_lec_type_of_rolling_count(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    res = r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    return pd.DataFrame(res, columns = ['lec_type_of_' + str(i) for i in range(4)]).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_lec_type_of_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec_proc, lec_map):\n",
    "    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\n",
    "    row_idx = user_map_ref[spc_prev_tes['user_id']]\n",
    "    col_idx = lec_proc['type_of'].values[lec_map[spc_prev_tes['content_id'].values]]\n",
    "    r_ref[row_idx, col_idx] += 1\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_lec_tags_rolling_count(r_ref, tes, user_map_ref, que_proc):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    res = np.zeros((spc_tes.shape[0], 7), dtype = np.float32)\n",
    "    users = user_map_ref[spc_tes['user_id'].values]\n",
    "    row_idx = np.repeat(users[:, None], 6, axis = 1)\n",
    "    col_idx = que_proc.values[spc_tes['content_id'].values]\n",
    "    \n",
    "    sliced = r_ref[row_idx, col_idx]\n",
    "    res[:, :6] = sliced\n",
    "    res[:, 6] = np.nansum(sliced, axis = 1)\n",
    "    return pd.DataFrame(res, columns = sum([['lec_tags_order_count_' + str(i)] for i in range(6)], []) + ['lec_whole_tags_order_count'])\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_lec_tags_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec, lec_map):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    #prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 1].copy().reset_index(drop = True)\n",
    "    col_idx = lec['tag'].values[lec_map[spc_prev_tes_cp['content_id'].values]]\n",
    "    row_idx = user_map_ref[spc_prev_tes_cp['user_id'].values]\n",
    "    r_ref[row_idx, col_idx] += 1\n",
    "    return r_ref\n",
    "\n",
    "@noglobal \n",
    "def online_get_rolling_mean_sum_count_for_content_id(r_ref_sum, r_ref_count, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    row_idx = user_map_ref[spc_tes['user_id'].values]\n",
    "    col_idx = spc_tes['content_id'].values\n",
    "    f_sum = r_ref_sum[row_idx, col_idx].toarray()[0]\n",
    "    f_count = r_ref_count[row_idx, col_idx].toarray()[0]\n",
    "    f_mean = f_sum/f_count\n",
    "    f_tr = pd.DataFrame()\n",
    "    f_tr['content_id_cut_mean'] = f_mean.astype(np.float32)\n",
    "    f_tr['content_id_cut_sum'] = f_sum.astype(np.float32)\n",
    "    f_tr['content_id_cut_count'] = f_count.astype(np.float32)\n",
    "    return f_tr\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_rolling_mean_sum_count_for_content_id(r_ref_sum, r_ref_count, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n",
    "    contents = spc_prev_tes_cp['content_id'].values\n",
    "    targets = spc_prev_tes_cp['answered_correctly'].values\n",
    "    for user, content, target in zip(users, contents, targets):\n",
    "        r_ref_sum[user, content] = r_ref_sum[user, content] + target\n",
    "        r_ref_count[user, content] = r_ref_count[user, content] + 1\n",
    "    return r_ref_sum, r_ref_count\n",
    "\n",
    "@noglobal\n",
    "def online_get_timestamp_diff(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    res = spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    return pd.DataFrame(res, columns = ['timestamp_diff']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_timestamp_diff(r_ref, prev_tes, tes, user_map_ref):\n",
    "    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['timestamp'].values\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_whole_oof_target_encoding_content_id(r_ref, tes):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    sum_count = r_ref[spc_tes['content_id'].values]\n",
    "    res = (sum_count[:, 0]/sum_count[:, 1]).astype(np.float32)\n",
    "    return pd.DataFrame(res, columns = ['whole_oof_target_encoding_content_id'])\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_whole_oof_target_encoding_content_id(r_ref, prev_tes, tes):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    cts = spc_prev_tes_cp['content_id'].value_counts()\n",
    "    pos_cts = spc_prev_tes_cp['content_id'][spc_prev_tes_cp['answered_correctly'] == 1].value_counts()\n",
    "    r_ref[pos_cts.index.values, 0] += pos_cts.values\n",
    "    r_ref[cts.index.values, 1] += cts.values\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_whole_oof_target_encoding_tags_order_and_whole_tags(r_ref, tes, que_proc):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    sum_count = r_ref[que_proc.values[spc_tes['content_id'].values]]\n",
    "    mean = sum_count[:, :, 0]/sum_count[:, :, 1]\n",
    "    nansum = np.nansum(sum_count, axis = 1)\n",
    "    whole_mean = nansum[:, 0]/nansum[:, 1]\n",
    "    res = np.zeros((spc_tes.shape[0], 7), dtype = np.float32)\n",
    "    res[:, :6] = mean\n",
    "    res[:, 6] = whole_mean\n",
    "    f_names = ['whole_oof_target_encoding_tags_order_' + str(i) for i in range(6)] + ['whole_oof_target_encoding_whole_tags']\n",
    "    return pd.DataFrame(res, columns = f_names)\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_whole_oof_target_encoding_tags_order_and_whole_tags(r_ref, prev_tes, tes, que_onehot):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    for_sum = que_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\n",
    "    sum_cts = for_sum.sum(axis = 0)\n",
    "    r_ref[:-1, 0] += sum_cts\n",
    "\n",
    "    for_count = que_onehot.values[spc_prev_tes_cp['content_id'].values]\n",
    "    cts = for_count.sum(axis = 0)\n",
    "    r_ref[:-1, 1] += cts\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_correct_answer(tes, que):\n",
    "    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    f_tes = pd.DataFrame(que['correct_answer'].values[spc_tes_cp['content_id'].values], columns = ['correct_answer']).astype(np.float32)\n",
    "    return f_tes\n",
    "\n",
    "@noglobal\n",
    "def online_get_modified_timedelta(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\n",
    "    spc_tes['count'] = np.bincount(enc)[enc]\n",
    "    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\n",
    "    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_norm_rolling_count_and_cut_for_user_answer(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "\n",
    "    for_count = np.zeros((spc_prev_tes_cp.shape[0], 4))\n",
    "    for_count[np.arange(for_count.shape[0]), spc_prev_tes_cp['user_answer'].values] = 1\n",
    "    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n",
    "    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(4)]).T\n",
    "    r_ref[user_map_ref[uniq]] += cts\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_norm_rolling_count_and_cut_for_user_answer(r_ref, tes, que, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    res = np.zeros((spc_tes.shape[0], 5), dtype = np.float32)\n",
    "    cts = r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    norm_cts = cts/cts.sum(axis = 1)[:, None]\n",
    "    res[:, :4] = norm_cts\n",
    "    correct_answer = que['correct_answer'].values[spc_tes['content_id'].values]\n",
    "    res[:, 4] = norm_cts[np.arange(norm_cts.shape[0]), correct_answer]\n",
    "    f_names = ['norm_rolling_count_user_answer_' + str(i) for i in range(4)] + ['cut_norm_rolling_count_user_answer']\n",
    "    return pd.DataFrame(res, columns = f_names)\n",
    "\n",
    "@noglobal\n",
    "def online_get_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    res = sum_count[:, 0]/sum_count[:, 1]\n",
    "    return pd.DataFrame(res, columns = ['rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def early_update_reference_get_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n",
    "    tes_cp = tes.copy()\n",
    "    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n",
    "                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n",
    "    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n",
    "    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\n",
    "    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n",
    "    uniq, enc = np.unique(tes_cp[tes_cp['content_type_id'] == 0]['user_id'].values, return_inverse = True)\n",
    "    r_ref[user_map_ref[uniq], 2] = np.bincount(enc)\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    res = sum_count[:, 0]/sum_count[:, 1]\n",
    "    return pd.DataFrame(res, columns = ['rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def early_update_reference_get_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n",
    "    tes_cp = tes.copy()\n",
    "    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n",
    "                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n",
    "    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n",
    "    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values.astype('int') * r_ref[row_idx, 2]\n",
    "    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n",
    "    uniq, enc = np.unique(tes_cp[tes_cp['content_type_id'] == 0]['user_id'].values, return_inverse = True)\n",
    "    r_ref[user_map_ref[uniq], 2] = np.bincount(enc)\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_rolling_sum_for_prior_question_isnull(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    res = r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    return pd.DataFrame(res, columns = ['rolling_sum_for_prior_question_isnull']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_rolling_sum_for_prior_question_isnull(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes[(prev_tes['content_type_id'] == 0) & (prev_tes['prior_question_elapsed_time'].isnull())].copy()\n",
    "    r_ref[user_map_ref[prev_tes_cp['user_id'].values]] += prev_tes_cp['prior_question_elapsed_time'].isnull().astype('int').values\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    res = sum_count[:, 0]/sum_count[:, 1]\n",
    "    return pd.DataFrame(res, columns = ['positive_rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def early_update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n",
    "    tes_cp = tes.copy()\n",
    "    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n",
    "                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n",
    "    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n",
    "    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\n",
    "    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n",
    "    sum_cts = np.bincount(enc, weights = spc_prev_tes_cp['answered_correctly'].values)\n",
    "    r_ref[user_map_ref[uniq], 2] = sum_cts\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    res = sum_count[:, 0]/sum_count[:, 1]\n",
    "    return pd.DataFrame(res, columns = ['negative_rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def early_update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n",
    "    tes_cp = tes.copy()\n",
    "    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n",
    "                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n",
    "    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n",
    "    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\n",
    "    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n",
    "    sum_cts = np.bincount(enc, weights = 1 - spc_prev_tes_cp['answered_correctly'].values)\n",
    "    r_ref[user_map_ref[uniq], 2] = sum_cts\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    res = sum_count[:, 0]/sum_count[:, 1]\n",
    "    return pd.DataFrame(res, columns = ['positive_rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def early_update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n",
    "    tes_cp = tes.copy()\n",
    "    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n",
    "                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n",
    "    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n",
    "    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values * r_ref[row_idx, 2]\n",
    "    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n",
    "    sum_cts = np.bincount(enc, weights = spc_prev_tes_cp['answered_correctly'].values)\n",
    "    r_ref[user_map_ref[uniq], 2] = sum_cts\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n",
    "    res = sum_count[:, 0]/sum_count[:, 1]\n",
    "    return pd.DataFrame(res, columns = ['negative_rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\n",
    "\n",
    "@noglobal\n",
    "def early_update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n",
    "    tes_cp = tes.copy()\n",
    "    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n",
    "                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n",
    "    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n",
    "    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values * r_ref[row_idx, 2]\n",
    "    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n",
    "    sum_cts = np.bincount(enc, weights = 1 - spc_prev_tes_cp['answered_correctly'].values)\n",
    "    r_ref[user_map_ref[uniq], 2] = sum_cts\n",
    "    return r_ref\n",
    "\n",
    "@noglobal\n",
    "def online_get_diff_modified_timedelta_and_prior_elapsed_time_rolling_mean(f_tes_base, f_tes_pn, f_tes_p, f_tes_n):\n",
    "    f_names = ['rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta', \n",
    "                                   'positive_rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta', \n",
    "                                   'negative_rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta']\n",
    "    f_tes = pd.DataFrame()\n",
    "    f_tes[f_names[0]] = f_tes_base.values[:, 0] - f_tes_pn.values[:, 0]\n",
    "    f_tes[f_names[1]] = f_tes_base.values[:, 0] - f_tes_p.values[:, 0]\n",
    "    f_tes[f_names[2]] = f_tes_base.values[:, 0] - f_tes_n.values[:, 0]\n",
    "    return f_tes\n",
    "\n",
    "@noglobal\n",
    "def online_get_n_samples_rolling_mean(r_ref, tes, user_map_ref):\n",
    "    n_samples = np.array([1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 200])\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    users = spc_tes['user_id'].values\n",
    "    history = r_ref[user_map_ref[users]].astype(np.float32)\n",
    "    history[history == -1] = np.nan\n",
    "    res = np.zeros((spc_tes.shape[0], len(n_samples)), dtype = np.float32)\n",
    "    for i, n_sample in enumerate(n_samples):\n",
    "        res[:, i] = np.nanmean(history[:, -n_sample:], axis = 1)\n",
    "    return pd.DataFrame(res, columns = [str(i) + '_samples_rolling_mean' for i in n_samples])\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_n_samples_rolling_mean(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n",
    "    targets = spc_prev_tes_cp['answered_correctly'].values\n",
    "    for user, target in zip(enc_users, targets):\n",
    "        r_ref[user, :-1] = r_ref[user, 1:]\n",
    "        r_ref[user, -1] = target\n",
    "    return r_ref\n",
    "\n",
    "@noglobal \n",
    "def online_get_rolling_mean_sum_count_for_content_id_darkness(r_ref, tes, user_map_ref):\n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    row_idx = user_map_ref[spc_tes['user_id'].values]\n",
    "    col_idx = spc_tes['content_id'].values\n",
    "    f_base = r_ref[row_idx, col_idx].toarray()[0]\n",
    "    f_sum = f_base % 128\n",
    "    f_count = f_base // 128\n",
    "    f_mean = f_sum/f_count\n",
    "    f_tr = pd.DataFrame()\n",
    "    f_tr['content_id_cut_mean'] = f_mean.astype(np.float32)\n",
    "    f_tr['content_id_cut_sum'] = f_sum.astype(np.float32)\n",
    "    f_tr['content_id_cut_count'] = f_count.astype(np.float32)\n",
    "    return f_tr\n",
    "\n",
    "@noglobal\n",
    "def update_reference_get_rolling_mean_sum_count_for_content_id_darkness(r_ref, prev_tes, tes, user_map_ref):\n",
    "    prev_tes_cp = prev_tes.copy()\n",
    "    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n",
    "    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n",
    "    users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n",
    "    contents = spc_prev_tes_cp['content_id'].values\n",
    "    targets = spc_prev_tes_cp['answered_correctly'].values\n",
    "    for user, content, target in zip(users, contents, targets):\n",
    "        r_ref[user, content] = r_ref[user, content] + target + 128\n",
    "    return r_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:40:23.712871Z",
     "iopub.status.busy": "2021-01-08T02:40:23.712147Z",
     "iopub.status.idle": "2021-01-08T02:40:23.816001Z",
     "shell.execute_reply": "2021-01-08T02:40:23.816554Z"
    },
    "papermill": {
     "duration": 0.131055,
     "end_time": "2021-01-08T02:40:23.816696",
     "exception": false,
     "start_time": "2021-01-08T02:40:23.685641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialization\n",
    "#comment outで入れ替える\n",
    "#prefix = '..'\n",
    "prefix = '/kaggle/input/mamastan-gpu-v27'\n",
    "#sn = pd.read_pickle('../others/ex_tes.pkl'); env = RiiidEnv(sn, iterate_wo_predict = False)\n",
    "import riiideducation; env = riiideducation.make_env()\n",
    "iter_test = env.iter_test()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:40:23.892356Z",
     "iopub.status.busy": "2021-01-08T02:40:23.870818Z",
     "iopub.status.idle": "2021-01-08T02:40:24.692362Z",
     "shell.execute_reply": "2021-01-08T02:40:24.692848Z"
    },
    "papermill": {
     "duration": 0.852463,
     "end_time": "2021-01-08T02:40:24.692994",
     "exception": false,
     "start_time": "2021-01-08T02:40:23.840531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#params162\n",
    "args = OrderedDict()\n",
    "args.n_length = 200\n",
    "args.emb_dim = 512\n",
    "args.dim_feedforward = 1028\n",
    "args.nhead = 4\n",
    "args.dropout = 0.2\n",
    "args.dropout_pe = 0\n",
    "args.num_features = 90\n",
    "args.n_conv = 30\n",
    "args.pred_bs = 256\n",
    "args.device = torch.device('cuda')\n",
    "\n",
    "f_names = ['target_full_mean',\n",
    " 'target_count',\n",
    " 'tags_order_mean_0',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_mean_1',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_mean_2',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_mean_3',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_mean_4',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_mean_5',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_mean',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_mean',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_mean',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_mean',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_mean',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_mean',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_mean',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_mean',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_mean',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_mean',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'norm_rolling_count_user_answer_0',\n",
    " 'norm_rolling_count_user_answer_1',\n",
    " 'norm_rolling_count_user_answer_2',\n",
    " 'norm_rolling_count_user_answer_3',\n",
    " 'cut_norm_rolling_count_user_answer',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'rolling_mean_for_prior_question_had_explanation',\n",
    " 'rolling_sum_for_prior_question_isnull',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_had_explanation',\n",
    " 'negative_rolling_mean_for_prior_question_had_explanation',\n",
    " '1_samples_rolling_mean',\n",
    " '2_samples_rolling_mean',\n",
    " '3_samples_rolling_mean',\n",
    " '4_samples_rolling_mean',\n",
    " '5_samples_rolling_mean',\n",
    " '10_samples_rolling_mean',\n",
    " '20_samples_rolling_mean',\n",
    " '30_samples_rolling_mean',\n",
    " '40_samples_rolling_mean',\n",
    " '50_samples_rolling_mean',\n",
    " '100_samples_rolling_mean',\n",
    " '200_samples_rolling_mean']\n",
    "log_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "stdize_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "stdize_params = {'target_count': (0, 5.929629),\n",
    " 'tags_order_sum_0': (0, 2.034838),\n",
    " 'tags_order_count_0': (0, 2.360298),\n",
    " 'tags_order_sum_1': (0, 2.352074),\n",
    " 'tags_order_count_1': (0, 2.646724),\n",
    " 'tags_order_sum_2': (0, 3.368914),\n",
    " 'tags_order_count_2': (0, 3.698544),\n",
    " 'tags_order_sum_3': (0, 3.515896),\n",
    " 'tags_order_count_3': (0, 3.854377),\n",
    " 'tags_order_sum_4': (0, 3.355033),\n",
    " 'tags_order_count_4': (0, 3.719298),\n",
    " 'tags_order_sum_5': (0, 3.871832),\n",
    " 'tags_order_count_5': (0, 4.218161),\n",
    " 'whole_tags_order_sum': (0, 3.005077),\n",
    " 'whole_tags_order_count': (0, 3.356816),\n",
    " 'part_1_sum': (0, 2.838132),\n",
    " 'part_1_count': (0, 3.105394),\n",
    " 'part_2_sum': (0, 3.631354),\n",
    " 'part_2_count': (0, 3.947554),\n",
    " 'part_3_sum': (0, 2.574938),\n",
    " 'part_3_count': (0, 2.8732),\n",
    " 'part_4_sum': (0, 2.353698),\n",
    " 'part_4_count': (0, 2.740394),\n",
    " 'part_5_sum': (0, 4.350539),\n",
    " 'part_5_count': (0, 4.86018),\n",
    " 'part_6_sum': (0, 2.64874),\n",
    " 'part_6_count': (0, 2.971092),\n",
    " 'part_7_sum': (0, 1.864806),\n",
    " 'part_7_count': (0, 2.166899),\n",
    " 'part_cut_sum': (0, 4.154958),\n",
    " 'part_cut_count': (0, 4.582286),\n",
    " 'lec_rolling_count': (0, 2.10689),\n",
    " 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n",
    "\n",
    "args.f_names = f_names\n",
    "args.log_f_names = log_f_names\n",
    "args.stdize_f_names = stdize_f_names\n",
    "args.stdize_params = stdize_params\n",
    "args.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\n",
    "args.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\n",
    "args.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\n",
    "del f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\n",
    "args_exp162 = deepcopy(args)\n",
    "del args;\n",
    "\n",
    "#params166\n",
    "args = OrderedDict()\n",
    "args.n_length = 400\n",
    "args.emb_dim = 512\n",
    "args.dim_feedforward = 1028\n",
    "args.nhead = 4\n",
    "args.dropout = 0.2\n",
    "args.dropout_pe = 0\n",
    "args.num_features = 90\n",
    "args.n_conv = 30\n",
    "args.pred_bs = 256\n",
    "args.device = torch.device('cuda')\n",
    "\n",
    "f_names = ['target_full_mean',\n",
    " 'target_count',\n",
    " 'tags_order_mean_0',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_mean_1',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_mean_2',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_mean_3',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_mean_4',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_mean_5',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_mean',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_mean',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_mean',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_mean',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_mean',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_mean',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_mean',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_mean',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_mean',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_mean',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'norm_rolling_count_user_answer_0',\n",
    " 'norm_rolling_count_user_answer_1',\n",
    " 'norm_rolling_count_user_answer_2',\n",
    " 'norm_rolling_count_user_answer_3',\n",
    " 'cut_norm_rolling_count_user_answer',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'rolling_mean_for_prior_question_had_explanation',\n",
    " 'rolling_sum_for_prior_question_isnull',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_had_explanation',\n",
    " 'negative_rolling_mean_for_prior_question_had_explanation',\n",
    " '1_samples_rolling_mean',\n",
    " '2_samples_rolling_mean',\n",
    " '3_samples_rolling_mean',\n",
    " '4_samples_rolling_mean',\n",
    " '5_samples_rolling_mean',\n",
    " '10_samples_rolling_mean',\n",
    " '20_samples_rolling_mean',\n",
    " '30_samples_rolling_mean',\n",
    " '40_samples_rolling_mean',\n",
    " '50_samples_rolling_mean',\n",
    " '100_samples_rolling_mean',\n",
    " '200_samples_rolling_mean']\n",
    "log_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "stdize_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "stdize_params = {'target_count': (0, 5.929629),\n",
    " 'tags_order_sum_0': (0, 2.034838),\n",
    " 'tags_order_count_0': (0, 2.360298),\n",
    " 'tags_order_sum_1': (0, 2.352074),\n",
    " 'tags_order_count_1': (0, 2.646724),\n",
    " 'tags_order_sum_2': (0, 3.368914),\n",
    " 'tags_order_count_2': (0, 3.698544),\n",
    " 'tags_order_sum_3': (0, 3.515896),\n",
    " 'tags_order_count_3': (0, 3.854377),\n",
    " 'tags_order_sum_4': (0, 3.355033),\n",
    " 'tags_order_count_4': (0, 3.719298),\n",
    " 'tags_order_sum_5': (0, 3.871832),\n",
    " 'tags_order_count_5': (0, 4.218161),\n",
    " 'whole_tags_order_sum': (0, 3.005077),\n",
    " 'whole_tags_order_count': (0, 3.356816),\n",
    " 'part_1_sum': (0, 2.838132),\n",
    " 'part_1_count': (0, 3.105394),\n",
    " 'part_2_sum': (0, 3.631354),\n",
    " 'part_2_count': (0, 3.947554),\n",
    " 'part_3_sum': (0, 2.574938),\n",
    " 'part_3_count': (0, 2.8732),\n",
    " 'part_4_sum': (0, 2.353698),\n",
    " 'part_4_count': (0, 2.740394),\n",
    " 'part_5_sum': (0, 4.350539),\n",
    " 'part_5_count': (0, 4.86018),\n",
    " 'part_6_sum': (0, 2.64874),\n",
    " 'part_6_count': (0, 2.971092),\n",
    " 'part_7_sum': (0, 1.864806),\n",
    " 'part_7_count': (0, 2.166899),\n",
    " 'part_cut_sum': (0, 4.154958),\n",
    " 'part_cut_count': (0, 4.582286),\n",
    " 'lec_rolling_count': (0, 2.10689),\n",
    " 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n",
    "\n",
    "args.f_names = f_names\n",
    "args.log_f_names = log_f_names\n",
    "args.stdize_f_names = stdize_f_names\n",
    "args.stdize_params = stdize_params\n",
    "args.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\n",
    "args.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\n",
    "args.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\n",
    "del f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\n",
    "args_exp166 = deepcopy(args)\n",
    "del args;\n",
    "\n",
    "#params184\n",
    "args = OrderedDict()\n",
    "args.n_length = 400\n",
    "args.emb_dim = 512\n",
    "args.dim_feedforward = 1028\n",
    "args.nhead = 4\n",
    "args.dropout = 0.2\n",
    "args.dropout_pe = 0\n",
    "args.num_features = 90\n",
    "args.n_conv = 30\n",
    "args.device = torch.device('cuda')\n",
    "args.pred_bs = 256\n",
    "\n",
    "f_names = ['target_full_mean',\n",
    " 'target_count',\n",
    " 'tags_order_mean_0',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_mean_1',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_mean_2',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_mean_3',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_mean_4',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_mean_5',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_mean',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_mean',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_mean',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_mean',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_mean',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_mean',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_mean',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_mean',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_mean',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_mean',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'norm_rolling_count_user_answer_0',\n",
    " 'norm_rolling_count_user_answer_1',\n",
    " 'norm_rolling_count_user_answer_2',\n",
    " 'norm_rolling_count_user_answer_3',\n",
    " 'cut_norm_rolling_count_user_answer',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'rolling_mean_for_prior_question_had_explanation',\n",
    " 'rolling_sum_for_prior_question_isnull',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_had_explanation',\n",
    " 'negative_rolling_mean_for_prior_question_had_explanation',\n",
    " '1_samples_rolling_mean',\n",
    " '2_samples_rolling_mean',\n",
    " '3_samples_rolling_mean',\n",
    " '4_samples_rolling_mean',\n",
    " '5_samples_rolling_mean',\n",
    " '10_samples_rolling_mean',\n",
    " '20_samples_rolling_mean',\n",
    " '30_samples_rolling_mean',\n",
    " '40_samples_rolling_mean',\n",
    " '50_samples_rolling_mean',\n",
    " '100_samples_rolling_mean',\n",
    " '200_samples_rolling_mean']\n",
    "\n",
    "log_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "\n",
    "stdize_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "\n",
    "stdize_params = {'target_count': (0, 5.929629),\n",
    " 'tags_order_sum_0': (0, 2.034838),\n",
    " 'tags_order_count_0': (0, 2.360298),\n",
    " 'tags_order_sum_1': (0, 2.352074),\n",
    " 'tags_order_count_1': (0, 2.646724),\n",
    " 'tags_order_sum_2': (0, 3.368914),\n",
    " 'tags_order_count_2': (0, 3.698544),\n",
    " 'tags_order_sum_3': (0, 3.515896),\n",
    " 'tags_order_count_3': (0, 3.854377),\n",
    " 'tags_order_sum_4': (0, 3.355033),\n",
    " 'tags_order_count_4': (0, 3.719298),\n",
    " 'tags_order_sum_5': (0, 3.871832),\n",
    " 'tags_order_count_5': (0, 4.218161),\n",
    " 'whole_tags_order_sum': (0, 3.005077),\n",
    " 'whole_tags_order_count': (0, 3.356816),\n",
    " 'part_1_sum': (0, 2.838132),\n",
    " 'part_1_count': (0, 3.105394),\n",
    " 'part_2_sum': (0, 3.631354),\n",
    " 'part_2_count': (0, 3.947554),\n",
    " 'part_3_sum': (0, 2.574938),\n",
    " 'part_3_count': (0, 2.8732),\n",
    " 'part_4_sum': (0, 2.353698),\n",
    " 'part_4_count': (0, 2.740394),\n",
    " 'part_5_sum': (0, 4.350539),\n",
    " 'part_5_count': (0, 4.86018),\n",
    " 'part_6_sum': (0, 2.64874),\n",
    " 'part_6_count': (0, 2.971092),\n",
    " 'part_7_sum': (0, 1.864806),\n",
    " 'part_7_count': (0, 2.166899),\n",
    " 'part_cut_sum': (0, 4.154958),\n",
    " 'part_cut_count': (0, 4.582286),\n",
    " 'lec_rolling_count': (0, 2.10689),\n",
    " 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n",
    "\n",
    "args.f_names = f_names\n",
    "args.log_f_names = log_f_names\n",
    "args.stdize_f_names = stdize_f_names\n",
    "args.stdize_params = stdize_params\n",
    "\n",
    "args.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\n",
    "args.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\n",
    "args.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\n",
    "del f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\n",
    "args_exp184 = deepcopy(args)\n",
    "del args;\n",
    "\n",
    "#params218\n",
    "args = OrderedDict()\n",
    "args.n_length = 400\n",
    "args.emb_dim = 512\n",
    "args.dim_feedforward = 1028\n",
    "args.nhead = 4\n",
    "args.dropout = 0.2\n",
    "args.dropout_pe = 0\n",
    "args.num_features = 90\n",
    "args.n_conv = 30\n",
    "args.device = torch.device('cuda')\n",
    "args.pred_bs = 256\n",
    "\n",
    "f_names = ['target_full_mean',\n",
    " 'target_count',\n",
    " 'tags_order_mean_0',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_mean_1',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_mean_2',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_mean_3',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_mean_4',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_mean_5',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_mean',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_mean',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_mean',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_mean',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_mean',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_mean',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_mean',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_mean',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_mean',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_mean',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'norm_rolling_count_user_answer_0',\n",
    " 'norm_rolling_count_user_answer_1',\n",
    " 'norm_rolling_count_user_answer_2',\n",
    " 'norm_rolling_count_user_answer_3',\n",
    " 'cut_norm_rolling_count_user_answer',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'rolling_mean_for_prior_question_had_explanation',\n",
    " 'rolling_sum_for_prior_question_isnull',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_had_explanation',\n",
    " 'negative_rolling_mean_for_prior_question_had_explanation',\n",
    " '1_samples_rolling_mean',\n",
    " '2_samples_rolling_mean',\n",
    " '3_samples_rolling_mean',\n",
    " '4_samples_rolling_mean',\n",
    " '5_samples_rolling_mean',\n",
    " '10_samples_rolling_mean',\n",
    " '20_samples_rolling_mean',\n",
    " '30_samples_rolling_mean',\n",
    " '40_samples_rolling_mean',\n",
    " '50_samples_rolling_mean',\n",
    " '100_samples_rolling_mean',\n",
    " '200_samples_rolling_mean']\n",
    "\n",
    "log_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "\n",
    "stdize_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "\n",
    "stdize_params = {'target_count': (0, 5.929629),\n",
    " 'tags_order_sum_0': (0, 2.034838),\n",
    " 'tags_order_count_0': (0, 2.360298),\n",
    " 'tags_order_sum_1': (0, 2.352074),\n",
    " 'tags_order_count_1': (0, 2.646724),\n",
    " 'tags_order_sum_2': (0, 3.368914),\n",
    " 'tags_order_count_2': (0, 3.698544),\n",
    " 'tags_order_sum_3': (0, 3.515896),\n",
    " 'tags_order_count_3': (0, 3.854377),\n",
    " 'tags_order_sum_4': (0, 3.355033),\n",
    " 'tags_order_count_4': (0, 3.719298),\n",
    " 'tags_order_sum_5': (0, 3.871832),\n",
    " 'tags_order_count_5': (0, 4.218161),\n",
    " 'whole_tags_order_sum': (0, 3.005077),\n",
    " 'whole_tags_order_count': (0, 3.356816),\n",
    " 'part_1_sum': (0, 2.838132),\n",
    " 'part_1_count': (0, 3.105394),\n",
    " 'part_2_sum': (0, 3.631354),\n",
    " 'part_2_count': (0, 3.947554),\n",
    " 'part_3_sum': (0, 2.574938),\n",
    " 'part_3_count': (0, 2.8732),\n",
    " 'part_4_sum': (0, 2.353698),\n",
    " 'part_4_count': (0, 2.740394),\n",
    " 'part_5_sum': (0, 4.350539),\n",
    " 'part_5_count': (0, 4.86018),\n",
    " 'part_6_sum': (0, 2.64874),\n",
    " 'part_6_count': (0, 2.971092),\n",
    " 'part_7_sum': (0, 1.864806),\n",
    " 'part_7_count': (0, 2.166899),\n",
    " 'part_cut_sum': (0, 4.154958),\n",
    " 'part_cut_count': (0, 4.582286),\n",
    " 'lec_rolling_count': (0, 2.10689),\n",
    " 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n",
    "\n",
    "args.f_names = f_names\n",
    "args.log_f_names = log_f_names\n",
    "args.stdize_f_names = stdize_f_names\n",
    "args.stdize_params = stdize_params\n",
    "\n",
    "args.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\n",
    "args.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\n",
    "args.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\n",
    "del f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\n",
    "args_exp218 = deepcopy(args)\n",
    "del args;\n",
    "\n",
    "#params224\n",
    "args = OrderedDict()\n",
    "args.n_length = 400\n",
    "args.emb_dim = 512\n",
    "args.dim_feedforward = 1028\n",
    "args.nhead = 4\n",
    "args.dropout = 0.2\n",
    "args.dropout_pe = 0\n",
    "args.num_features = 90\n",
    "args.n_conv = 30\n",
    "args.device = torch.device('cuda')\n",
    "args.pred_bs = 256\n",
    "\n",
    "f_names = ['target_full_mean',\n",
    " 'target_count',\n",
    " 'tags_order_mean_0',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_mean_1',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_mean_2',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_mean_3',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_mean_4',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_mean_5',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_mean',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_mean',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_mean',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_mean',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_mean',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_mean',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_mean',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_mean',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_mean',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_mean',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'norm_rolling_count_user_answer_0',\n",
    " 'norm_rolling_count_user_answer_1',\n",
    " 'norm_rolling_count_user_answer_2',\n",
    " 'norm_rolling_count_user_answer_3',\n",
    " 'cut_norm_rolling_count_user_answer',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'rolling_mean_for_prior_question_had_explanation',\n",
    " 'rolling_sum_for_prior_question_isnull',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_had_explanation',\n",
    " 'negative_rolling_mean_for_prior_question_had_explanation',\n",
    " '1_samples_rolling_mean',\n",
    " '2_samples_rolling_mean',\n",
    " '3_samples_rolling_mean',\n",
    " '4_samples_rolling_mean',\n",
    " '5_samples_rolling_mean',\n",
    " '10_samples_rolling_mean',\n",
    " '20_samples_rolling_mean',\n",
    " '30_samples_rolling_mean',\n",
    " '40_samples_rolling_mean',\n",
    " '50_samples_rolling_mean',\n",
    " '100_samples_rolling_mean',\n",
    " '200_samples_rolling_mean']\n",
    "log_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "stdize_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "stdize_params = {'target_count': (0, 5.929629),\n",
    " 'tags_order_sum_0': (0, 2.034838),\n",
    " 'tags_order_count_0': (0, 2.360298),\n",
    " 'tags_order_sum_1': (0, 2.352074),\n",
    " 'tags_order_count_1': (0, 2.646724),\n",
    " 'tags_order_sum_2': (0, 3.368914),\n",
    " 'tags_order_count_2': (0, 3.698544),\n",
    " 'tags_order_sum_3': (0, 3.515896),\n",
    " 'tags_order_count_3': (0, 3.854377),\n",
    " 'tags_order_sum_4': (0, 3.355033),\n",
    " 'tags_order_count_4': (0, 3.719298),\n",
    " 'tags_order_sum_5': (0, 3.871832),\n",
    " 'tags_order_count_5': (0, 4.218161),\n",
    " 'whole_tags_order_sum': (0, 3.005077),\n",
    " 'whole_tags_order_count': (0, 3.356816),\n",
    " 'part_1_sum': (0, 2.838132),\n",
    " 'part_1_count': (0, 3.105394),\n",
    " 'part_2_sum': (0, 3.631354),\n",
    " 'part_2_count': (0, 3.947554),\n",
    " 'part_3_sum': (0, 2.574938),\n",
    " 'part_3_count': (0, 2.8732),\n",
    " 'part_4_sum': (0, 2.353698),\n",
    " 'part_4_count': (0, 2.740394),\n",
    " 'part_5_sum': (0, 4.350539),\n",
    " 'part_5_count': (0, 4.86018),\n",
    " 'part_6_sum': (0, 2.64874),\n",
    " 'part_6_count': (0, 2.971092),\n",
    " 'part_7_sum': (0, 1.864806),\n",
    " 'part_7_count': (0, 2.166899),\n",
    " 'part_cut_sum': (0, 4.154958),\n",
    " 'part_cut_count': (0, 4.582286),\n",
    " 'lec_rolling_count': (0, 2.10689),\n",
    " 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n",
    "\n",
    "args.f_names = f_names\n",
    "args.log_f_names = log_f_names\n",
    "args.stdize_f_names = stdize_f_names\n",
    "args.stdize_params = stdize_params\n",
    "\n",
    "args.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\n",
    "args.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\n",
    "args.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\n",
    "del f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\n",
    "args_exp224 = deepcopy(args)\n",
    "del args;\n",
    "\n",
    "#params219\n",
    "args = OrderedDict()\n",
    "args.n_length = 400\n",
    "args.emb_dim = 512\n",
    "args.dim_feedforward = 1028\n",
    "args.nhead = 4\n",
    "args.dropout = 0.2\n",
    "args.dropout_pe = 0\n",
    "args.lr = 0.2 * 1e-3\n",
    "args.num_features = 90\n",
    "args.device = torch.device('cuda')\n",
    "args.pred_bs = 256\n",
    "\n",
    "f_names = ['target_full_mean',\n",
    " 'target_count',\n",
    " 'tags_order_mean_0',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_mean_1',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_mean_2',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_mean_3',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_mean_4',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_mean_5',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_mean',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_mean',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_mean',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_mean',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_mean',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_mean',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_mean',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_mean',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_mean',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_mean',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'norm_rolling_count_user_answer_0',\n",
    " 'norm_rolling_count_user_answer_1',\n",
    " 'norm_rolling_count_user_answer_2',\n",
    " 'norm_rolling_count_user_answer_3',\n",
    " 'cut_norm_rolling_count_user_answer',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'rolling_mean_for_prior_question_had_explanation',\n",
    " 'rolling_sum_for_prior_question_isnull',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_had_explanation',\n",
    " 'negative_rolling_mean_for_prior_question_had_explanation',\n",
    " '1_samples_rolling_mean',\n",
    " '2_samples_rolling_mean',\n",
    " '3_samples_rolling_mean',\n",
    " '4_samples_rolling_mean',\n",
    " '5_samples_rolling_mean',\n",
    " '10_samples_rolling_mean',\n",
    " '20_samples_rolling_mean',\n",
    " '30_samples_rolling_mean',\n",
    " '40_samples_rolling_mean',\n",
    " '50_samples_rolling_mean',\n",
    " '100_samples_rolling_mean',\n",
    " '200_samples_rolling_mean']\n",
    "log_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "stdize_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "stdize_params = {'target_count': (0, 5.929629),\n",
    " 'tags_order_sum_0': (0, 2.034838),\n",
    " 'tags_order_count_0': (0, 2.360298),\n",
    " 'tags_order_sum_1': (0, 2.352074),\n",
    " 'tags_order_count_1': (0, 2.646724),\n",
    " 'tags_order_sum_2': (0, 3.368914),\n",
    " 'tags_order_count_2': (0, 3.698544),\n",
    " 'tags_order_sum_3': (0, 3.515896),\n",
    " 'tags_order_count_3': (0, 3.854377),\n",
    " 'tags_order_sum_4': (0, 3.355033),\n",
    " 'tags_order_count_4': (0, 3.719298),\n",
    " 'tags_order_sum_5': (0, 3.871832),\n",
    " 'tags_order_count_5': (0, 4.218161),\n",
    " 'whole_tags_order_sum': (0, 3.005077),\n",
    " 'whole_tags_order_count': (0, 3.356816),\n",
    " 'part_1_sum': (0, 2.838132),\n",
    " 'part_1_count': (0, 3.105394),\n",
    " 'part_2_sum': (0, 3.631354),\n",
    " 'part_2_count': (0, 3.947554),\n",
    " 'part_3_sum': (0, 2.574938),\n",
    " 'part_3_count': (0, 2.8732),\n",
    " 'part_4_sum': (0, 2.353698),\n",
    " 'part_4_count': (0, 2.740394),\n",
    " 'part_5_sum': (0, 4.350539),\n",
    " 'part_5_count': (0, 4.86018),\n",
    " 'part_6_sum': (0, 2.64874),\n",
    " 'part_6_count': (0, 2.971092),\n",
    " 'part_7_sum': (0, 1.864806),\n",
    " 'part_7_count': (0, 2.166899),\n",
    " 'part_cut_sum': (0, 4.154958),\n",
    " 'part_cut_count': (0, 4.582286),\n",
    " 'lec_rolling_count': (0, 2.10689),\n",
    " 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n",
    "\n",
    "args.f_names = f_names\n",
    "args.log_f_names = log_f_names\n",
    "args.stdize_f_names = stdize_f_names\n",
    "args.stdize_params = stdize_params\n",
    "\n",
    "args.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\n",
    "args.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\n",
    "args.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\n",
    "del f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\n",
    "args_exp219 = deepcopy(args)\n",
    "del args;\n",
    "\n",
    "#params221\n",
    "args = OrderedDict()\n",
    "args.n_length = 400\n",
    "args.emb_dim = 512\n",
    "args.dim_feedforward = 1028\n",
    "args.nhead = 4\n",
    "args.dropout = 0.2\n",
    "args.dropout_pe = 0\n",
    "args.num_features = 90\n",
    "args.device = torch.device('cuda')\n",
    "args.pred_bs = 256\n",
    "\n",
    "f_names = ['target_full_mean',\n",
    " 'target_count',\n",
    " 'tags_order_mean_0',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_mean_1',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_mean_2',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_mean_3',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_mean_4',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_mean_5',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_mean',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_mean',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_mean',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_mean',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_mean',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_mean',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_mean',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_mean',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_mean',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_mean',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'norm_rolling_count_user_answer_0',\n",
    " 'norm_rolling_count_user_answer_1',\n",
    " 'norm_rolling_count_user_answer_2',\n",
    " 'norm_rolling_count_user_answer_3',\n",
    " 'cut_norm_rolling_count_user_answer',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'rolling_mean_for_prior_question_had_explanation',\n",
    " 'rolling_sum_for_prior_question_isnull',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_had_explanation',\n",
    " 'negative_rolling_mean_for_prior_question_had_explanation',\n",
    " '1_samples_rolling_mean',\n",
    " '2_samples_rolling_mean',\n",
    " '3_samples_rolling_mean',\n",
    " '4_samples_rolling_mean',\n",
    " '5_samples_rolling_mean',\n",
    " '10_samples_rolling_mean',\n",
    " '20_samples_rolling_mean',\n",
    " '30_samples_rolling_mean',\n",
    " '40_samples_rolling_mean',\n",
    " '50_samples_rolling_mean',\n",
    " '100_samples_rolling_mean',\n",
    " '200_samples_rolling_mean']\n",
    "log_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "stdize_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "stdize_params = {'target_count': (0, 5.929629),\n",
    " 'tags_order_sum_0': (0, 2.034838),\n",
    " 'tags_order_count_0': (0, 2.360298),\n",
    " 'tags_order_sum_1': (0, 2.352074),\n",
    " 'tags_order_count_1': (0, 2.646724),\n",
    " 'tags_order_sum_2': (0, 3.368914),\n",
    " 'tags_order_count_2': (0, 3.698544),\n",
    " 'tags_order_sum_3': (0, 3.515896),\n",
    " 'tags_order_count_3': (0, 3.854377),\n",
    " 'tags_order_sum_4': (0, 3.355033),\n",
    " 'tags_order_count_4': (0, 3.719298),\n",
    " 'tags_order_sum_5': (0, 3.871832),\n",
    " 'tags_order_count_5': (0, 4.218161),\n",
    " 'whole_tags_order_sum': (0, 3.005077),\n",
    " 'whole_tags_order_count': (0, 3.356816),\n",
    " 'part_1_sum': (0, 2.838132),\n",
    " 'part_1_count': (0, 3.105394),\n",
    " 'part_2_sum': (0, 3.631354),\n",
    " 'part_2_count': (0, 3.947554),\n",
    " 'part_3_sum': (0, 2.574938),\n",
    " 'part_3_count': (0, 2.8732),\n",
    " 'part_4_sum': (0, 2.353698),\n",
    " 'part_4_count': (0, 2.740394),\n",
    " 'part_5_sum': (0, 4.350539),\n",
    " 'part_5_count': (0, 4.86018),\n",
    " 'part_6_sum': (0, 2.64874),\n",
    " 'part_6_count': (0, 2.971092),\n",
    " 'part_7_sum': (0, 1.864806),\n",
    " 'part_7_count': (0, 2.166899),\n",
    " 'part_cut_sum': (0, 4.154958),\n",
    " 'part_cut_count': (0, 4.582286),\n",
    " 'lec_rolling_count': (0, 2.10689),\n",
    " 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n",
    "\n",
    "args.f_names = f_names\n",
    "args.log_f_names = log_f_names\n",
    "args.stdize_f_names = stdize_f_names\n",
    "args.stdize_params = stdize_params\n",
    "\n",
    "args.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\n",
    "args.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\n",
    "args.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\n",
    "del f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\n",
    "args_exp221 = deepcopy(args)\n",
    "del args;\n",
    "\n",
    "#params222\n",
    "args = OrderedDict()\n",
    "args.n_length = 400\n",
    "args.emb_dim = 512\n",
    "args.dim_feedforward = 1028\n",
    "args.nhead = 4\n",
    "args.dropout = 0.2\n",
    "args.dropout_pe = 0\n",
    "args.num_features = 90\n",
    "args.device = torch.device('cuda')\n",
    "args.pred_bs = 256\n",
    "\n",
    "f_names = ['target_full_mean',\n",
    " 'target_count',\n",
    " 'tags_order_mean_0',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_mean_1',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_mean_2',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_mean_3',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_mean_4',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_mean_5',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_mean',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_mean',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_mean',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_mean',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_mean',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_mean',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_mean',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_mean',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_mean',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_mean',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'norm_rolling_count_user_answer_0',\n",
    " 'norm_rolling_count_user_answer_1',\n",
    " 'norm_rolling_count_user_answer_2',\n",
    " 'norm_rolling_count_user_answer_3',\n",
    " 'cut_norm_rolling_count_user_answer',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'rolling_mean_for_prior_question_had_explanation',\n",
    " 'rolling_sum_for_prior_question_isnull',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_had_explanation',\n",
    " 'negative_rolling_mean_for_prior_question_had_explanation',\n",
    " '1_samples_rolling_mean',\n",
    " '2_samples_rolling_mean',\n",
    " '3_samples_rolling_mean',\n",
    " '4_samples_rolling_mean',\n",
    " '5_samples_rolling_mean',\n",
    " '10_samples_rolling_mean',\n",
    " '20_samples_rolling_mean',\n",
    " '30_samples_rolling_mean',\n",
    " '40_samples_rolling_mean',\n",
    " '50_samples_rolling_mean',\n",
    " '100_samples_rolling_mean',\n",
    " '200_samples_rolling_mean']\n",
    "log_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'lec_part_1',\n",
    " 'lec_part_2',\n",
    " 'lec_part_3',\n",
    " 'lec_part_4',\n",
    " 'lec_part_5',\n",
    " 'lec_part_6',\n",
    " 'lec_part_7',\n",
    " 'lec_part_cut',\n",
    " 'lec_type_of_0',\n",
    " 'lec_type_of_1',\n",
    " 'lec_type_of_2',\n",
    " 'lec_tags_order_count_0',\n",
    " 'lec_tags_order_count_1',\n",
    " 'lec_tags_order_count_2',\n",
    " 'lec_whole_tags_order_count',\n",
    " 'content_id_cut_sum',\n",
    " 'content_id_cut_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "stdize_f_names = [\n",
    " 'target_count',\n",
    " 'tags_order_sum_0',\n",
    " 'tags_order_count_0',\n",
    " 'tags_order_sum_1',\n",
    " 'tags_order_count_1',\n",
    " 'tags_order_sum_2',\n",
    " 'tags_order_count_2',\n",
    " 'tags_order_sum_3',\n",
    " 'tags_order_count_3',\n",
    " 'tags_order_sum_4',\n",
    " 'tags_order_count_4',\n",
    " 'tags_order_sum_5',\n",
    " 'tags_order_count_5',\n",
    " 'whole_tags_order_sum',\n",
    " 'whole_tags_order_count',\n",
    " 'part_1_sum',\n",
    " 'part_1_count',\n",
    " 'part_2_sum',\n",
    " 'part_2_count',\n",
    " 'part_3_sum',\n",
    " 'part_3_count',\n",
    " 'part_4_sum',\n",
    " 'part_4_count',\n",
    " 'part_5_sum',\n",
    " 'part_5_count',\n",
    " 'part_6_sum',\n",
    " 'part_6_count',\n",
    " 'part_7_sum',\n",
    " 'part_7_count',\n",
    " 'part_cut_sum',\n",
    " 'part_cut_count',\n",
    " 'lec_rolling_count',\n",
    " 'rolling_mean_for_prior_question_elapsed_time',\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time',\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time']\n",
    "stdize_params = {'target_count': (0, 5.929629),\n",
    " 'tags_order_sum_0': (0, 2.034838),\n",
    " 'tags_order_count_0': (0, 2.360298),\n",
    " 'tags_order_sum_1': (0, 2.352074),\n",
    " 'tags_order_count_1': (0, 2.646724),\n",
    " 'tags_order_sum_2': (0, 3.368914),\n",
    " 'tags_order_count_2': (0, 3.698544),\n",
    " 'tags_order_sum_3': (0, 3.515896),\n",
    " 'tags_order_count_3': (0, 3.854377),\n",
    " 'tags_order_sum_4': (0, 3.355033),\n",
    " 'tags_order_count_4': (0, 3.719298),\n",
    " 'tags_order_sum_5': (0, 3.871832),\n",
    " 'tags_order_count_5': (0, 4.218161),\n",
    " 'whole_tags_order_sum': (0, 3.005077),\n",
    " 'whole_tags_order_count': (0, 3.356816),\n",
    " 'part_1_sum': (0, 2.838132),\n",
    " 'part_1_count': (0, 3.105394),\n",
    " 'part_2_sum': (0, 3.631354),\n",
    " 'part_2_count': (0, 3.947554),\n",
    " 'part_3_sum': (0, 2.574938),\n",
    " 'part_3_count': (0, 2.8732),\n",
    " 'part_4_sum': (0, 2.353698),\n",
    " 'part_4_count': (0, 2.740394),\n",
    " 'part_5_sum': (0, 4.350539),\n",
    " 'part_5_count': (0, 4.86018),\n",
    " 'part_6_sum': (0, 2.64874),\n",
    " 'part_6_count': (0, 2.971092),\n",
    " 'part_7_sum': (0, 1.864806),\n",
    " 'part_7_count': (0, 2.166899),\n",
    " 'part_cut_sum': (0, 4.154958),\n",
    " 'part_cut_count': (0, 4.582286),\n",
    " 'lec_rolling_count': (0, 2.10689),\n",
    " 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n",
    " 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n",
    " 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n",
    "\n",
    "args.f_names = f_names\n",
    "args.log_f_names = log_f_names\n",
    "args.stdize_f_names = stdize_f_names\n",
    "args.stdize_params = stdize_params\n",
    "\n",
    "args.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\n",
    "args.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\n",
    "args.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\n",
    "del f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\n",
    "args_exp222 = deepcopy(args)\n",
    "del args;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:40:24.776255Z",
     "iopub.status.busy": "2021-01-08T02:40:24.763741Z",
     "iopub.status.idle": "2021-01-08T02:41:32.783618Z",
     "shell.execute_reply": "2021-01-08T02:41:32.782512Z"
    },
    "papermill": {
     "duration": 68.064908,
     "end_time": "2021-01-08T02:41:32.783750",
     "exception": false,
     "start_time": "2021-01-08T02:40:24.718842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "my_encoder_exp162 = MyEncoderExp162(args_exp162)\n",
    "my_encoder_exp162.load_state_dict(torch.load(prefix + '/models/my_encoder_exp162_best.pth'))\n",
    "my_encoder_exp162.to(args_exp162.device)\n",
    "my_encoder_exp162.eval();\n",
    "\n",
    "my_encoder_exp166 = MyEncoderExp166(args_exp166)\n",
    "my_encoder_exp166.load_state_dict(torch.load(prefix + '/models/my_encoder_exp166_best.pth'))\n",
    "my_encoder_exp166.to(args_exp166.device)\n",
    "my_encoder_exp166.eval();\n",
    "\n",
    "my_encoder_exp184 = MyEncoderExp184(args_exp184)\n",
    "my_encoder_exp184.load_state_dict(torch.load(prefix + '/models/my_encoder_exp184_best.pth'))\n",
    "my_encoder_exp184.to(args_exp184.device)\n",
    "my_encoder_exp184.eval();\n",
    "\n",
    "my_encoder_exp218 = MyEncoderExp218(args_exp218)\n",
    "my_encoder_exp218.load_state_dict(torch.load(prefix + '/models/my_encoder_exp248_best.pth'))\n",
    "my_encoder_exp218.to(args_exp218.device)\n",
    "my_encoder_exp218.eval();\n",
    "\n",
    "my_encoder_exp224 = MyEncoderExp224(args_exp224)\n",
    "my_encoder_exp224.load_state_dict(torch.load(prefix + '/models/my_encoder_exp233_best.pth'))\n",
    "my_encoder_exp224.to(args_exp224.device)\n",
    "my_encoder_exp224.eval();\n",
    "\n",
    "my_encoder_exp222 = MyEncoderExp222(args_exp222)\n",
    "my_encoder_exp222.load_state_dict(torch.load(prefix + '/models/my_encoder_exp249_best.pth'))\n",
    "my_encoder_exp222.to(args_exp222.device)\n",
    "my_encoder_exp222.eval();\n",
    "\n",
    "que =  pd.read_csv(prefix + '/data/questions.csv').astype({'question_id': 'int16', 'bundle_id': 'int16', 'correct_answer': 'int8', 'part': 'int8'})\n",
    "que_proc = pd.read_pickle(prefix + '/others/que_proc.pkl')\n",
    "que_proc_2 = pd.read_pickle(prefix + '/others/que_proc_2.pkl')\n",
    "que_onehot = pd.read_pickle(prefix + '/others/que_onehot.pkl')\n",
    "que_part_onehot = pd.read_pickle(prefix + '/others/que_part_onehot.pkl')\n",
    "lec = pd.read_csv(prefix + '/data/lectures.csv').astype({'lecture_id': 'int16', 'tag': 'int16', 'part': 'int8'})\n",
    "lec_map = np.load(prefix + '/others/lec_map.npy')\n",
    "lec_proc = pd.read_pickle(prefix + '/others/lec_proc.pkl')\n",
    "que_proc_int = np.load(prefix + '/others/que_proc_int.npy')\n",
    "\n",
    "n_sample = 400\n",
    "user_map_ref = load_pickle(prefix + '/others/user_map_train.npy')\n",
    "n_users_ref = np.load(prefix + '/others/n_users_train.npy')\n",
    "#nn\n",
    "r_ref_g1 = np.load(prefix + '/references/nn_content_id_history' + '_length_' + str(n_sample) + '_train.npy')\n",
    "r_ref_g2 = np.load(prefix + '/references/nn_normed_log_timestamp_history' + '_length_' + str(n_sample) + '_train.npy')\n",
    "r_ref_g3 = np.load(prefix + '/references/nn_correctness_history' + '_length_' + str(n_sample) + '_train.npy')\n",
    "r_ref_g4 = np.load(prefix + '/references/nn_question_had_explanation_history' + '_length_' + str(n_sample) + '_train.npy')\n",
    "r_ref_g5 = np.load(prefix + '/references/nn_normed_elapsed_history' + '_length_' + str(n_sample) + '_train.npy')\n",
    "r_ref_g6 = np.load(prefix + '/references/nn_normed_modified_timedelta_history' + '_length_' + str(n_sample) + '_train.npy')\n",
    "r_ref_g7 = np.load(prefix + '/references/nn_user_answer_history' + '_length_' + str(n_sample) + '_train.npy')\n",
    "r_ref_g8 = np.load(prefix + '/references/nn_task_container_id_diff_history' + '_length_' + str(n_sample) + '_train.npy')\n",
    "r_ref_g9 = np.load(prefix + '/references/nn_content_type_id_diff_history' + '_length_' + str(n_sample) + '_train.npy')\n",
    "r_ref_g10 = np.load(prefix + '/references/nn_task_container_id_history' + '_length_' + str(n_sample) + '_train.npy')\n",
    "r_ref_delta = np.load(prefix + '/references/timestamp_diff_train.npy')\n",
    "r_ref_delta_2 = np.load(prefix + '/references/task_container_id_diff_train.npy')\n",
    "r_ref_delta_3 = np.load(prefix + '/references/content_type_id_diff_train.npy')\n",
    "#cpu\n",
    "r_ref_1 = np.load(prefix + '/references/rolling_mean_sum_count_train.npy')\n",
    "r_ref_2 = np.load(prefix + '/references/rolling_mean_sum_count_for_6_tags_and_whole_tag_train.npy')\n",
    "r_ref_3 = np.load(prefix + '/references/rolling_mean_sum_count_for_part_train.npy')\n",
    "r_ref_4 = np.load(prefix + '/references/lec_rolling_count_train.npy')\n",
    "r_ref_5 = np.load(prefix + '/references/lec_part_rolling_count_train.npy')\n",
    "r_ref_6 = np.load(prefix + '/references/lec_type_of_rolling_count_train.npy')\n",
    "r_ref_7 = np.load(prefix + '/references/lec_tags_rolling_count_train.npy')\n",
    "r_ref_8 = load_pickle(prefix + '/references/rolling_mean_sum_count_for_content_id_darkness_train.npy')\n",
    "r_ref_12 = np.load(prefix + '/references/norm_rolling_count_and_cut_for_user_answer_train.npy')\n",
    "r_ref_13 = np.load(prefix + '/references/rolling_mean_for_prior_question_elapsed_time_train.npy')\n",
    "r_ref_14 = np.load(prefix + '/references/rolling_mean_for_prior_question_had_explanation_train.npy')\n",
    "r_ref_15 = np.load(prefix + '/references/rolling_sum_for_prior_question_isnull_train.npy')\n",
    "r_ref_16 = np.load(prefix + '/references/positive_rolling_mean_for_prior_question_elapsed_time_train.npy')\n",
    "r_ref_17 = np.load(prefix + '/references/negative_rolling_mean_for_prior_question_elapsed_time_train.npy')\n",
    "r_ref_18 = np.load(prefix + '/references/positive_rolling_mean_for_prior_question_had_explanation_train.npy')\n",
    "r_ref_19 = np.load(prefix + '/references/negative_rolling_mean_for_prior_question_had_explanation_train.npy')\n",
    "r_ref_20 = np.load(prefix + '/references/n_samples_rolling_mean_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:41:32.855885Z",
     "iopub.status.busy": "2021-01-08T02:41:32.845345Z",
     "iopub.status.idle": "2021-01-08T02:41:34.736577Z",
     "shell.execute_reply": "2021-01-08T02:41:34.735969Z"
    },
    "papermill": {
     "duration": 1.929093,
     "end_time": "2021-01-08T02:41:34.736696",
     "exception": false,
     "start_time": "2021-01-08T02:41:32.807603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d1ef64ddae4e56b22f17eedd8d1a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "i = 0\n",
    "for (tes, _) in tqdm(iter_test):\n",
    "    user_map_ref, n_users_ref = update_user_map(tes, user_map_ref, n_users_ref)\n",
    "    if i != 0:\n",
    "        # update\n",
    "        r_ref_1 = update_reference_get_rolling_mean_sum_count(r_ref_1, prev_tes, tes, user_map_ref)\n",
    "        r_ref_2 = update_reference_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref_2, prev_tes, tes, que_onehot, user_map_ref)\n",
    "        r_ref_3 = update_reference_get_rolling_mean_sum_count_for_part(r_ref_3, prev_tes, tes, que_part_onehot, user_map_ref)\n",
    "        r_ref_4 = update_reference_get_lec_rolling_count(r_ref_4, prev_tes, tes, user_map_ref)\n",
    "        r_ref_5 = update_reference_get_lec_part_rolling_count(r_ref_5, prev_tes, tes, user_map_ref, lec, lec_map)\n",
    "        r_ref_6 = update_reference_get_lec_type_of_rolling_count(r_ref_6, prev_tes, tes, user_map_ref, lec_proc, lec_map)\n",
    "        r_ref_7 = update_reference_get_lec_tags_rolling_count(r_ref_7, prev_tes, tes, user_map_ref, lec, lec_map)\n",
    "        r_ref_8 = update_reference_get_rolling_mean_sum_count_for_content_id_darkness(r_ref_8, prev_tes, tes, user_map_ref)\n",
    "        \n",
    "        r_ref_12 = update_reference_get_norm_rolling_count_and_cut_for_user_answer(r_ref_12, prev_tes, tes, user_map_ref)\n",
    "        r_ref_15 = update_reference_get_rolling_sum_for_prior_question_isnull(r_ref_15, prev_tes, tes, user_map_ref)\n",
    "        r_ref_16 = update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, prev_tes, tes, user_map_ref)\n",
    "        r_ref_17 = update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, prev_tes, tes, user_map_ref)\n",
    "        r_ref_18 = update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, prev_tes, tes, user_map_ref)\n",
    "        r_ref_19 = update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, prev_tes, tes, user_map_ref)\n",
    "        r_ref_20 = update_reference_get_n_samples_rolling_mean(r_ref_20, prev_tes, tes, user_map_ref)\n",
    "\n",
    "        r_ref_delta = update_reference_get_timestamp_diff(r_ref_delta, prev_tes, tes, user_map_ref)\n",
    "        r_ref_delta_2 = update_reference_get_task_container_id_diff(r_ref_delta_2, prev_tes, tes, user_map_ref)\n",
    "        r_ref_delta_3 = update_reference_get_content_type_id_diff(r_ref_delta_3, prev_tes, tes, user_map_ref)\n",
    "        r_ref_g1 = nn_update_reference_get_content_id_history(r_ref_g1, prev_tes, tes, user_map_ref)\n",
    "        r_ref_g2 = nn_update_reference_get_normed_log_timestamp_history(r_ref_g2, prev_tes, tes, user_map_ref)\n",
    "        r_ref_g3 = nn_update_reference_get_correctness_history(r_ref_g3, prev_tes, tes, user_map_ref)\n",
    "        r_ref_g4 = nn_update_reference_get_question_had_explanation_history(r_ref_g4, prev_tes, tes, user_map_ref)\n",
    "        r_ref_g5 = nn_update_reference_get_normed_elapsed_history(r_ref_g5, prev_tes, tes, user_map_ref)\n",
    "        r_ref_g6 = nn_update_reference_get_normed_modified_timedelta_history(r_ref_g6, prev_tes, tes, f_tes_delta, user_map_ref)\n",
    "        r_ref_g7 = nn_update_reference_get_user_answer_history(r_ref_g7, prev_tes, tes, user_map_ref)\n",
    "        r_ref_g8 = nn_update_reference_get_task_container_id_diff_history(r_ref_g8, prev_tes, tes, f_tes_delta_2, user_map_ref)\n",
    "        r_ref_g9 = nn_update_reference_get_content_type_id_diff_history(r_ref_g9, prev_tes, tes, f_tes_delta_3, user_map_ref)\n",
    "        r_ref_g10 = nn_update_reference_get_task_container_id_history(r_ref_g10, prev_tes, tes, user_map_ref)\n",
    "        \n",
    "    # early update\n",
    "    r_ref_g4 = nn_early_update_reference_get_question_had_explanation_history(r_ref_g4, tes, user_map_ref)\n",
    "    r_ref_g5 = nn_early_update_reference_get_normed_elapsed_history(r_ref_g5, tes, user_map_ref)\n",
    "    r_ref_13 = early_update_reference_get_rolling_mean_for_prior_question_elapsed_time(r_ref_13, tes, user_map_ref)\n",
    "    r_ref_14 = early_update_reference_get_rolling_mean_for_prior_question_had_explanation(r_ref_14, tes, user_map_ref)\n",
    "    r_ref_16 = early_update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, tes, user_map_ref)\n",
    "    r_ref_17 = early_update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, tes, user_map_ref)\n",
    "    r_ref_18 = early_update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, tes, user_map_ref)\n",
    "    r_ref_19 = early_update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, tes, user_map_ref)\n",
    "    \n",
    "    # online function\n",
    "    f_tes_1 = online_get_rolling_mean_sum_count(r_ref_1, tes, user_map_ref)\n",
    "    f_tes_2 = online_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref_2, tes, que_proc, user_map_ref)\n",
    "    f_tes_3 = online_get_rolling_mean_sum_count_for_part(r_ref_3, tes, que, user_map_ref)\n",
    "    f_tes_4 = online_get_lec_rolling_count(r_ref_4, tes, user_map_ref)\n",
    "    f_tes_5 = online_get_lec_part_rolling_count(r_ref_5, tes, user_map_ref, que)\n",
    "    f_tes_6 = online_get_lec_type_of_rolling_count(r_ref_6, tes, user_map_ref)\n",
    "    f_tes_7 = online_get_lec_tags_rolling_count(r_ref_7, tes, user_map_ref, que_proc)\n",
    "    f_tes_8 = online_get_rolling_mean_sum_count_for_content_id_darkness(r_ref_8, tes, user_map_ref)\n",
    "    \n",
    "    f_tes_12 = online_get_norm_rolling_count_and_cut_for_user_answer(r_ref_12, tes, que, user_map_ref)\n",
    "    f_tes_13 = online_get_rolling_mean_for_prior_question_elapsed_time(r_ref_13, tes, user_map_ref)\n",
    "    f_tes_14 = online_get_rolling_mean_for_prior_question_had_explanation(r_ref_14, tes, user_map_ref)\n",
    "    f_tes_15 = online_get_rolling_sum_for_prior_question_isnull(r_ref_15, tes, user_map_ref)\n",
    "    f_tes_16 = online_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, tes, user_map_ref)\n",
    "    f_tes_17 = online_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, tes, user_map_ref)\n",
    "    f_tes_18 = online_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, tes, user_map_ref)\n",
    "    f_tes_19 = online_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, tes, user_map_ref)\n",
    "    f_tes_20 = online_get_n_samples_rolling_mean(r_ref_20, tes, user_map_ref)\n",
    "    \n",
    "    f_tes_delta = online_get_modified_timedelta(r_ref_delta, tes, user_map_ref)\n",
    "    f_tes_delta_2 = online_get_task_container_id_diff(r_ref_delta_2, tes, user_map_ref)\n",
    "    f_tes_delta_3 = online_get_content_type_id_diff(r_ref_delta_3, tes, user_map_ref)\n",
    "    \n",
    "    f_tes_g1 = nn_online_get_content_id_history(r_ref_g1, tes, user_map_ref)\n",
    "    f_tes_g2 = nn_online_get_normed_log_timestamp_history(r_ref_g2, tes, user_map_ref)\n",
    "    f_tes_g3 = nn_online_get_correctness_history(r_ref_g3, tes, user_map_ref)\n",
    "    f_tes_g4 = nn_online_get_question_had_explanation_history(r_ref_g4, tes, user_map_ref)\n",
    "    f_tes_g5 = nn_online_get_normed_elapsed_history(r_ref_g5, tes, user_map_ref)\n",
    "    f_tes_g6 = nn_online_get_normed_modified_timedelta_history(r_ref_g6, tes, f_tes_delta, user_map_ref)\n",
    "    f_tes_g7 = nn_online_get_user_answer_history(r_ref_g7, tes, user_map_ref)\n",
    "    f_tes_g8 = nn_online_get_task_container_id_diff_history(r_ref_g8, tes, f_tes_delta_2, user_map_ref)\n",
    "    f_tes_g9 = nn_online_get_content_type_id_diff_history(r_ref_g9, tes, f_tes_delta_3, user_map_ref)\n",
    "    f_tes_g10 = nn_online_get_task_container_id_history(r_ref_g10, tes, user_map_ref)\n",
    "    \n",
    "\n",
    "    # make a prediction \n",
    "    concated = pd.concat([f_tes_1, f_tes_2, f_tes_3, f_tes_4, f_tes_5, f_tes_6, f_tes_7, \n",
    "                         f_tes_8, f_tes_12, f_tes_13, f_tes_14, f_tes_15, f_tes_16, f_tes_17, f_tes_18, \n",
    "                         f_tes_19, f_tes_20], axis = 1)\n",
    "    X_tes = concated[args_exp166.f_names].values.astype(np.float32)\n",
    "    \n",
    "    inputs = {'content_id' : f_tes_g1.values, 'normed_timedelta' : f_tes_g6.values, 'normed_log_timestamp' : f_tes_g2.values,\n",
    "         'explanation': f_tes_g4.values, 'correctness': f_tes_g3.values, 'normed_elapsed': f_tes_g5.values, 'user_answer' : f_tes_g7.values,\n",
    "             'task_container_id_diff': f_tes_g8.values, 'content_type_id_diff': f_tes_g9.values, 'task_container_id': f_tes_g10.values, \n",
    "              'features': X_tes}\n",
    "    inputs_200 = {key: inputs[key][:, -201:] for key in inputs.keys()}\n",
    "    \n",
    "    \n",
    "    score_exp162 = my_encoder_exp162.predict_on_batch(inputs_200, args_exp162, args_exp162.pred_bs, que, que_proc_int) \\\n",
    "    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n",
    "    score_exp166 = my_encoder_exp166.predict_on_batch(inputs, args_exp166, args_exp166.pred_bs, que, que_proc_int) \\\n",
    "    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n",
    "    score_exp184 = my_encoder_exp184.predict_on_batch(inputs, args_exp184, args_exp184.pred_bs, que, que_proc_int) \\\n",
    "    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n",
    "    score_exp218 = my_encoder_exp218.predict_on_batch(inputs, args_exp218, args_exp218.pred_bs, que, que_proc_int) \\\n",
    "    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n",
    "    score_exp224 = my_encoder_exp224.predict_on_batch(inputs, args_exp224, args_exp224.pred_bs, que, que_proc_int) \\\n",
    "    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n",
    "    score_exp222 = my_encoder_exp222.predict_on_batch(inputs, args_exp222, args_exp222.pred_bs, que, que_proc_int) \\\n",
    "    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n",
    "    \n",
    "    preds = [score_exp162, score_exp166, score_exp184, score_exp218, score_exp222, score_exp224]\n",
    "    weights = np.array([0.42, 0.25, 0.49, 0.85, 0.84, 0.82])\n",
    "    weights[[3, 4, 5]] += 0.1\n",
    "    weights = weights/weights.sum()\n",
    "    score = 0\n",
    "    for pred, weight in zip(preds, weights):\n",
    "        score += pred * weight\n",
    "    \n",
    "    spc_tes = tes[tes['content_type_id'] == 0].copy()\n",
    "    spc_tes['answered_correctly'] = score.astype(np.float64)\n",
    "    env.predict(spc_tes[['row_id', 'answered_correctly']])\n",
    "    \n",
    "    # save previous test\n",
    "    prev_tes = tes.copy()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.02392,
     "end_time": "2021-01-08T02:41:34.785638",
     "exception": false,
     "start_time": "2021-01-08T02:41:34.761718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.024573,
     "end_time": "2021-01-08T02:41:34.834575",
     "exception": false,
     "start_time": "2021-01-08T02:41:34.810002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.028659,
     "end_time": "2021-01-08T02:41:34.886655",
     "exception": false,
     "start_time": "2021-01-08T02:41:34.857996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.024412,
     "end_time": "2021-01-08T02:41:34.935853",
     "exception": false,
     "start_time": "2021-01-08T02:41:34.911441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.023434,
     "end_time": "2021-01-08T02:41:34.983033",
     "exception": false,
     "start_time": "2021-01-08T02:41:34.959599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.02461,
     "end_time": "2021-01-08T02:41:35.031111",
     "exception": false,
     "start_time": "2021-01-08T02:41:35.006501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 83.747356,
   "end_time": "2021-01-08T02:41:35.766295",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-08T02:40:12.018939",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0622f49894ce46f2ac231eb5fef059c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_eb4eca2195af48188e6f14548a74ab14",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bed569b46ef3448b8033b2d8e05a2b31",
       "value": 1.0
      }
     },
     "1ab91deba60e4002a0529e6fbdcb6176": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "224b3185b25b4e73b33cf1b53ae52a65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7b5eb1a79d6d4aca90a7cc9ae2ece23a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1ab91deba60e4002a0529e6fbdcb6176",
       "placeholder": "​",
       "style": "IPY_MODEL_224b3185b25b4e73b33cf1b53ae52a65",
       "value": " 4/? [00:01&lt;00:00,  2.18it/s]"
      }
     },
     "add0ec7637594c3f9dba0b74550973b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bed569b46ef3448b8033b2d8e05a2b31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "c1d1ef64ddae4e56b22f17eedd8d1a63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0622f49894ce46f2ac231eb5fef059c2",
        "IPY_MODEL_7b5eb1a79d6d4aca90a7cc9ae2ece23a"
       ],
       "layout": "IPY_MODEL_add0ec7637594c3f9dba0b74550973b2"
      }
     },
     "eb4eca2195af48188e6f14548a74ab14": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
